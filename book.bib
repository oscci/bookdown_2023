@article{abbeduto2007,
  title = {Language Development in {{Down}} Syndrome: From the Prelinguistic Period to the Acquisition of Literacy},
  shorttitle = {Language Development in {{Down}} Syndrome},
  author = {Abbeduto, Leonard and Warren, Steven F. and Conners, Frances A.},
  year = {2007},
  journal = {Mental Retardation and Developmental Disabilities Research Reviews},
  volume = {13},
  number = {3},
  pages = {247--261},
  issn = {1080-4013},
  doi = {10.1002/mrdd.20158},
  abstract = {Down syndrome (DS) is associated with abnormalities in multiple organ systems and a characteristic phenotype that includes numerous behavioral features. Language, however, is among the most impaired domains of functioning in DS and, perhaps, also the greatest barrier to independent meaningful inclusion in the community. In this article, we review what is known about the extent, nature, and correlates of the language and related problems of individuals with Down syndrome. In doing so, we focus largely on the syndrome-specific features of the language phenotype, although we also consider within-syndrome variation. The review focuses on the prelinguistic foundations of language and the major components of language (i.e., vocabulary, syntax, and pragmatics). We also consider two topics in the treatment and education of individuals with DS: prelinguistic communication intervention and the acquisition of literacy skills.},
  langid = {english},
  pmid = {17910087},
  keywords = {Child,Child; Preschool,Down Syndrome,Humans,Intellectual Disability,Language,Language Development,Phenotype,Verbal Learning}
}

@article{aksayli2019,
  title = {The Cognitive and Academic Benefits of {{Cogmed}}: {{A}} Meta-Analysis},
  shorttitle = {The Cognitive and Academic Benefits of {{Cogmed}}},
  author = {Aksayli, N. Deniz and Sala, Giovanni and Gobet, Fernand},
  year = {2019},
  month = jun,
  journal = {Educational Research Review},
  volume = {27},
  pages = {229--243},
  issn = {1747-938X},
  doi = {10.1016/j.edurev.2019.04.003},
  abstract = {Cogmed Working Memory Training (CWMT) is a commercial cognitive-training program designed to foster working-memory capacity. Enhanced working-memory capacity is then supposed to increase one's overall cognitive function and academic achievement. This meta-analysis investigates the effects of CWMT on cognitive and academic outcomes. The inclusion criteria were met by 50 studies (637 effect sizes). Highly consistent near-zero effects were estimated in far-transfer measures of cognitive ability (e.g., attention and intelligence) and academic achievement (language ability and mathematics). By contrast, slightly heterogeneous small to medium effects were observed in memory tasks (i.e., near transfer). Moderator analysis showed that these effects were weaker for near-transfer measures not directly related to the trained tasks. These results highlight that, while near transfer occurs regularly, far transfer is rare or, possibly, inexistent. Transfer thus appears to be a function of the degree of overlap between trained tasks and outcome tasks.},
  langid = {english},
  keywords = {Cogmed,Meta-analysis,Transfer,Working memory training},
  file = {/Users/dorothybishop/Zotero/storage/YILZ83YJ/Aksayli et al. - 2019 - The cognitive and academic benefits of Cogmed A m.pdf;/Users/dorothybishop/Zotero/storage/R73D7Z7U/S1747938X18303658.html}
}

@article{araujo2016,
  title = {Understanding {{Variation}} in {{Sets}} of {{N-of-1 Trials}}},
  author = {Araujo, Artur and Julious, Steven and Senn, Stephen},
  year = {2016},
  month = dec,
  journal = {PLOS ONE},
  volume = {11},
  number = {12},
  pages = {e0167167},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0167167},
  abstract = {A recent paper in this journal by Chen and Chen has used computer simulations to examine a number of approaches to analysing sets of n-of-1 trials. We have examined such designs using a more theoretical approach based on considering the purpose of analysis and the structure as regards randomisation that the design uses. We show that different purposes require different analyses and that these in turn may produce quite different results. Our approach to incorporating the randomisation employed when the purpose is to test a null hypothesis of strict equality of the treatment makes use of Nelder's theory of general balance. However, where the purpose is to make inferences about the effects for individual patients, we show that a mixed model is needed. There are strong parallels to the difference between fixed and random effects meta-analyses and these are discussed.},
  langid = {english},
  keywords = {ACE inhibitor therapy,Analysis of variance,Balance and falls,Beta-adrenergic antagonist therapy,Experimental design,Metaanalysis,Nicotine replacement therapy,Simulation and modeling},
  file = {/Users/dorothybishop/Zotero/storage/N6MU7E4M/Araujo et al. - 2016 - Understanding Variation in Sets of N-of-1 Trials.pdf;/Users/dorothybishop/Zotero/storage/DGZTTMC2/article.html}
}

@article{armson1998,
  title = {Effect of {{Extended Exposure}} to {{Frequency-Altered Feedback}} on {{Stuttering During Reading}} and {{Monologue}}},
  author = {Armson, Joy and Stuart, Andrew},
  year = {1998},
  month = jun,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {41},
  number = {3},
  pages = {479--490},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/jslhr.4103.479},
  abstract = {An ABA time series design was used to examine the effect of extended, continuous exposure to frequency-altered auditory feedback (FAF) during an oral reading and monologue task on stuttering frequency and speech rate. Twelve adults who stutter participated. A statistically significant decrease in number of stuttering events, an increase in number of syllables produced, and a decrease in percent stuttering was observed during the experimental segment relative to baseline segments for the oral reading task. In the monologue task, there were no statistically significant differences for the number of stuttering events, number of syllables produced, or percent stuttering between the experimental and baseline segments. Varying individual patterns of response to FAF were evident during the experimental segment of the reading task: a large consistent reduction in stuttering, an initial reduction followed by fluctuations in amount of stuttering, and essentially no change in stuttering frequency. Ten of 12 participants showed no reduction in stuttering frequency during the experimental segment of the monologue task. These findings have ramifications both for the clinical utilization of FAF and for theoretical explanations of fluency-enhancement.},
  keywords = {extended exposure to FAF,fluency-enhancement,frequency-altered feedback,stuttering},
  file = {/Users/dorothybishop/Zotero/storage/VP4GEQC3/Armson and Stuart - 1998 - Effect of Extended Exposure to Frequency-Altered F.pdf}
}

@article{arrimada2021,
  title = {Response to {{Intervention}} in First-Grade Writing Instruction: A Large-Scale Feasibility Study},
  shorttitle = {Response to {{Intervention}} in First-Grade Writing Instruction},
  author = {Arrimada, Mar{\'i}a and Torrance, Mark and Fidalgo, Raquel},
  year = {2021},
  month = sep,
  journal = {Reading and Writing},
  issn = {1573-0905},
  doi = {10.1007/s11145-021-10211-z},
  abstract = {Early failure to learn writing skills might go unnoticed and unremedied unless teachers adopt specific strategies for identifying and supporting students who learn at a slower pace. We implemented a Response to Intervention (RTI) program for teaching narrative writing. Over 18~months from start of primary school, 161 Spanish children received instruction in strategies for planning text and training in handwriting and spelling, and completed very regular narrative writing tasks. Data from these tasks were analysed to identify students at risk of falling behind. These students then completed additional, parent-supervised training tasks. During this training the quality of these students' texts improved more rapidly than those of their peers. The resulting decrease in difference relative to peers, as measured by both regular narrative tasks and by post and follow-up measures, was sustained after additional training ceased. Interviews and questionnaires found good parent and teacher buy-in, with some caveats. Findings therefore indicate the feasibility and potential value of a RTI approach to teaching writing in single-teacher, full-range, first-grade classes.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/3R6ATUGL/Arrimada et al. - 2021 - Response to Intervention in first-grade writing in.pdf}
}

@article{baek2013,
  title = {Multilevel Models for Multiple-Baseline Data: Modeling across-Participant Variation in Autocorrelation and Residual Variance},
  shorttitle = {Multilevel Models for Multiple-Baseline Data},
  author = {Baek, Eun Kyeng and Ferron, John M.},
  year = {2013},
  month = mar,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {1},
  pages = {65--74},
  issn = {1554-3528},
  doi = {10.3758/s13428-012-0231-z},
  abstract = {Multilevel models (MLM) have been used as a method for analyzing multiple-baseline single-case data. However, some concerns can be raised because the models that have been used assume that the Level-1 error covariance matrix is the same for all participants. The purpose of this study was to extend the application of MLM of single-case data in order to accommodate across-participant variation in the Level-1 residual variance and autocorrelation. This more general model was then used in the analysis of single-case data sets to illustrate the method, to estimate the degree to which the autocorrelation and residual variances differed across participants, and to examine whether inferences about treatment effects were sensitive to whether or not the Level-1 error covariance matrix was allowed to vary across participants. The results from the analyses of five published studies showed that when the Level-1 error covariance matrix was allowed to vary across participants, some relatively large differences in autocorrelation estimates and error variance estimates emerged. The changes in modeling the variance structure did not change the conclusions about which fixed effects were statistically significant in most of the studies, but there was one exception. The fit indices did not consistently support selecting either the more complex covariance structure, which allowed the covariance parameters to vary across participants, or the simpler covariance structure. Given the uncertainty in model specification that may arise when modeling single-case data, researchers should consider conducting sensitivity analyses to examine the degree to which their conclusions are sensitive to modeling choices.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/4XJWW897/Baek and Ferron - 2013 - Multilevel models for multiple-baseline data mode.pdf}
}

@article{balduzzi2019,
  title = {How to Perform a Meta-Analysis with {{R}}: A Practical Tutorial},
  shorttitle = {How to Perform a Meta-Analysis with {{R}}},
  author = {Balduzzi, Sara and R{\"u}cker, Gerta and Schwarzer, Guido},
  year = {2019},
  month = nov,
  journal = {Evidence-Based Mental Health},
  volume = {22},
  number = {4},
  pages = {153--160},
  publisher = {{Royal College of Psychiatrists}},
  issn = {1362-0347, 1468-960X},
  doi = {10.1136/ebmental-2019-300117},
  abstract = {Objective Meta-analysis is of fundamental importance to obtain an unbiased assessment of the available evidence. In general, the use of meta-analysis has been increasing over the last three decades with mental health as a major research topic. It is then essential to well understand its methodology and interpret its results. In this publication, we describe how to perform a meta-analysis with the freely available statistical software environment R, using a working example taken from the field of mental health. Methods R package meta is used to conduct standard meta-analysis. Sensitivity analyses for missing binary outcome data and potential selection bias are conducted with R package metasens. All essential R commands are provided and clearly described to conduct and report analyses. Results The working example considers a binary outcome: we show how to conduct a fixed effect and random effects meta-analysis and subgroup analysis, produce a forest and funnel plot and to test and adjust for funnel plot asymmetry. All these steps work similar for other outcome types. Conclusions R represents a powerful and flexible tool to conduct meta-analyses. This publication gives a brief glimpse into the topic and provides directions to more advanced meta-analysis methods available in R.},
  chapter = {Statistics in practice},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2019. No commercial re-use. See rights and permissions. Published by BMJ.},
  langid = {english},
  pmid = {31563865},
  file = {/Users/dorothybishop/Zotero/storage/DWTMG92H/Balduzzi et al. - 2019 - How to perform a meta-analysis with R a practical.pdf;/Users/dorothybishop/Zotero/storage/M6FDT5A9/153.html}
}

@article{barber1968,
  title = {Fact, Fiction, and the Experimenter Bias Effect},
  author = {Barber, Theodore X. and Silver, Maurice J.},
  year = {1968},
  journal = {Psychological Bulletin},
  volume = {70},
  number = {6, Pt.2},
  pages = {1--29},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/h0026724},
  abstract = {Critically analyzes 31 studies which attempted to demonstrate that Es' expectancies and desires significantly affect the experimental outcome (E bias effect). The majority of studies do not clearly demonstrate the effect. Many of these studies are criticized for inadequacies in the analysis of results, e.g., failure to perform an overall statistical analysis to exclude the null hypothesis and failure to avoid probability pyramiding when postmortem tests are performed. 2 conclusions are drawn: (1) The E bias effect appears to be more difficult to demonstrate and less pervasive than was implied in previous reviews in this journal. (2) In those instances in which the effect was obtained, it was apparently due to one or more of the following: the student Es misjudged, misrecorded or misreported the results; they verbally reinforced their Ss for expected responses; or they intentionally or unintentionally transmitted their expectancies and desires by paralinguistic or kinesic cues. (2 p. ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Expectations,Experimentation,Literature Review,Methodology,Prejudice},
  file = {/Users/dorothybishop/Zotero/storage/I5UH93QY/1969-06146-001.html}
}

@article{beeson2006,
  title = {Evaluating {{Single-Subject Treatment Research}}: {{Lessons Learned}} from the {{Aphasia Literature}}},
  shorttitle = {Evaluating {{Single-Subject Treatment Research}}},
  author = {Beeson, P{\'e}lagie M. and Robey, Randall R.},
  year = {2006},
  month = dec,
  journal = {Neuropsychology review},
  volume = {16},
  number = {4},
  pages = {161--169},
  issn = {1040-7308},
  doi = {10.1007/s11065-006-9013-7},
  abstract = {The mandate for evidence-based practice has prompted careful consideration of the weight of the scientific evidence regarding the therapeutic value of various clinical treatments. In the field of aphasia, a large number of single-subject research studies have been conducted, providing clinical outcome data that are potentially useful for clinicians and researchers; however, it has been difficult to discern the relative potency of these treatments in a standardized manner. In this paper we describe an approach to quantify treatment outcomes for single-subject research studies using effect sizes. These values provide a means to compare treatment outcomes within and between individuals, as well as to compare the relative strength of various treatments. Effect sizes also can be aggregated in order to conduct meta-analyses of specific treatment approaches. Consideration is given to optimizing research designs and providing adequate data so that the value of treatment research is maximized.},
  pmcid = {PMC2366174},
  pmid = {17151940},
  file = {/Users/dorothybishop/Zotero/storage/HGAAV7BN/Beeson and Robey - 2006 - Evaluating Single-Subject Treatment Research Less.pdf}
}

@incollection{bem2004,
  title = {Writing the Empirical Journal Article},
  booktitle = {The Compleat Academic: {{A}} Career Guide, 2nd Ed},
  author = {Bem, Daryl J.},
  year = {2004},
  pages = {185--219},
  publisher = {{American Psychological Association}},
  address = {{Washington, DC, US}},
  abstract = {The purpose of this chapter is to enhance the chances that academic psychologists get their empirical articles published. Because I write, review, and edit primarily for journals in personality and social psychology, I have drawn most of my examples from those areas. Colleagues assure me, however, that the guidelines set forth are also pertinent for articles in experimental psychology and biopsychology. Similarly, this chapter focuses on an empirical study, but the general writing suggestions apply as well to the theoretical articles, literature reviews, and methodological contributions that also appear in psychology journals. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {978-1-59147-035-9},
  keywords = {Experimental Psychology,Scientific Communication,Written Communication},
  file = {/Users/dorothybishop/Zotero/storage/YJ67QKBB/2003-06256-010.html}
}

@article{benjamin2018,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'o}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"o}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  year = {2018},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {1},
  pages = {6--10},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
  copyright = {2017 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Human behaviour;Statistics Subject\_term\_id: human-behaviour;statistics},
  file = {/Users/dorothybishop/Zotero/storage/R9T3UD4I/Benjamin et al. - 2018 - Redefine statistical significance.pdf;/Users/dorothybishop/Zotero/storage/I54JVUF3/s41562-017-0189-z.html}
}

@article{best2013,
  title = {Aphasia Rehabilitation: {{Does}} Generalisation from Anomia Therapy Occur and Is It Predictable? {{A}} Case Series Study},
  shorttitle = {Aphasia Rehabilitation},
  author = {Best, Wendy and Greenwood, Alison and Grassly, Jennie and Herbert, Ruth and Hickin, Julie and Howard, David},
  year = {2013},
  month = oct,
  journal = {Cortex},
  volume = {49},
  number = {9},
  pages = {2345--2357},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2013.01.005},
  abstract = {Introduction The majority of adults with acquired aphasia have anomia which can respond to rehabilitation with cues. However, the literature and clinical consensus suggest change is usually limited to treated items. We investigated the effect of an experimentally controlled intervention using progressive cues in the rehabilitation of noun retrieval/production in 16 participants with chronic aphasia. Method Participants were sub-divided relative to the group according to performance on semantic tasks (spoken/written word to picture matching) and phonological output processing (presence/absence of word length effect and proportion of phonological errors in picture naming) in order to investigate outcome in relation to language profile. Cueing therapy took place weekly for 8 weeks. Results Intervention resulted in significant improvement on naming treated items for 15/16 participants, with stable performance on control tasks. Change occurred at the point of intervention and not during pre-therapy assessments. We predicted particular patterns of generalisation which were upheld. Only participants classified as having relatively less of a semantic difficulty and more of a phonological output deficit demonstrated generalisation to untreated items. Outcome did not relate to traditional aphasia classification. Conclusion A cueing hierarchy can improve word retrieval/production for adults with aphasia. In some cases generalisation to untreated items also occurs. The study demonstrates that the results of behavioural testing can be used to guide predictions of recovery with intervention.},
  langid = {english},
  keywords = {Anomia,Aphasia,Generalisation,Rehabilitation,Therapy},
  file = {/Users/dorothybishop/Zotero/storage/W5KA79EE/Best et al. - 2013 - Aphasia rehabilitation Does generalisation from a.pdf;/Users/dorothybishop/Zotero/storage/WDSX2AB2/Best et al. - 2013 - Aphasia rehabilitation Does generalisation from a.pdf;/Users/dorothybishop/Zotero/storage/AL7BZJHV/S0010945213000087.html;/Users/dorothybishop/Zotero/storage/Y6M2XDIU/S0010945213000087.html}
}

@article{best2019,
  title = {What Counts as Evidence? {{Swimming}} against the Tide: {{Valuing}} Both Clinically Informed Experimentally Controlled Case Series and Randomized Controlled Trials in Intervention Research},
  shorttitle = {What Counts as Evidence?},
  author = {Best, Wendy and Ping Sze, Wei and Edmundson, Anne and Nickels, Lyndsey},
  year = {2019},
  month = jul,
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {13},
  number = {3},
  pages = {107--135},
  publisher = {{Taylor \& Francis}},
  issn = {1748-9539},
  doi = {10.1080/17489539.2019.1597444},
  abstract = {Research into intervention with people with speech and language needs often takes the form of single-case/case series experimental studies (SCEDs) or randomized controlled trials (RCTs). This paper explores the nature of these designs, including their strengths/weaknesses and highlights the value of understanding the intervention outcomes for individual participants. An online survey gathered information on speech and language therapists' views on their use of the different research designs. We conclude that both research designs are used to inform practice. SCEDs, in particular, are used in developing theories of intervention and informing therapy with individuals. Sound experimental intervention studies of both designs are needed.},
  keywords = {aphasia rehabilitation research,Evidence-based practice,language disorders,language therapy,small-n design},
  annotation = {\_eprint: https://doi.org/10.1080/17489539.2019.1597444},
  file = {/Users/dorothybishop/Zotero/storage/ETVWDSTQ/Best et al. - 2019 - What counts as evidence Swimming against the tide.pdf;/Users/dorothybishop/Zotero/storage/LD285U8Z/17489539.2019.html}
}

@article{bishop1987,
  title = {Language-{{Impaired}} 4-{{Year-Olds}}},
  author = {BIshop, D. V. M. and Edmundson, A},
  year = {1987},
  month = may,
  journal = {Journal of Speech and Hearing Disorders},
  volume = {52},
  number = {2},
  pages = {156--173},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/jshd.5202.156},
  abstract = {In a prospective, longitudinal study, 87 language-impaired children were assessed          at the ages of 4, 4{$\frac{1}{2}$}, and 5{$\frac{1}{2}$} years on a battery of language measures. In 37\% of children,          who were termed the "good outcome group," the language disorder had resolved by the          age of 5{$\frac{1}{2}$} years so that children were indistinguishable from a control group. If one          restricted consideration only to those 68 children whose nonverbal ability was within          normal limits, the figure rose to 44\%. Outcome for individual children (good or poor)          could be predicted with 90\% accuracy on the basis of test measures obtained at 4 years.          The best predictor was ability to tell back a simple story to pictures. The one language          measure that did not relate to outcome was phonological competence.},
  file = {/Users/dorothybishop/Zotero/storage/E2U8HVEX/jshd.5202.html}
}

@article{bishop2006,
  title = {Resistance of Grammatical Impairment to Computerized Comprehension Training in Children with Specific and Non-Specific Language Impairments},
  author = {Bishop, D. V. M. and Adams, C. V. and Rosen, S.},
  year = {2006 Jan-Feb},
  journal = {International Journal of Language \& Communication Disorders},
  volume = {41},
  number = {1},
  pages = {19--40},
  issn = {1368-2822},
  doi = {10.1080/13682820500144000},
  abstract = {BACKGROUND: Receptive language impairments in school-age children have a poor prognosis, yet there is a dearth of research on effective interventions. AIMS: Children's responses to a computerized grammatical training program were evaluated to consider whether repeated responding to spoken sentences with variable semantic content and the same syntactic structure would lead to consistent and fluent comprehension. METHODS \& PROCEDURES: Children with receptive language impairments aged from 8 to 13 years were randomly assigned to three groups: Group S (n = 12) responded to reversible sentences in a computerized game, using speech stimuli with pauses before critical phrases. Group M (n = 12) had the same stimuli acoustically modified to lengthen and amplify dynamic portions of the signal. Group U (n = 9) was an untrained control group. On average, children in groups S and M completed over 1000 training trials, focusing on training comprehension of reversible sentences. OUTCOMES \& RESULTS: Although responses speeded up over the course of training, and most children performed well above chance, accuracy typically remained below 95\% correct for constructions such as above/below and reversible active/passive. Trained groups did not differ from untrained children on language or auditory outcomes. There was no evidence that acoustically modified speech input enhanced comprehension. CONCLUSIONS: Rote training of comprehension of reversible sentences does not seem to be an effective approach to remediating such problems. For most children, the pattern of performance suggested that the problem was not a lack of syntactic knowledge, bur rather limited processing capacity that led to failures of on-line computation of meaning.},
  langid = {english},
  pmid = {16272001},
  keywords = {Adolescent,Auditory Perception,Child,Cognition,Computer-Assisted,Discrimination Learning,Humans,Language Development Disorders,Language Therapy,Reaction Time,Speech,Speech Acoustics,Speech Discrimination Tests,Therapy,Therapy; Computer-Assisted,Treatment Outcome}
}

@article{bishop2013a,
  title = {Research {{Review}}: {{Emanuel Miller Memorial Lecture}} 2012 \textendash{} {{Neuroscientific}} Studies of Intervention for Language Impairment in Children: Interpretive and Methodological Problems},
  shorttitle = {Research {{Review}}},
  author = {Bishop, D. V. M.},
  year = {2013},
  month = mar,
  journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},
  volume = {54},
  number = {3},
  pages = {247--259},
  issn = {0021-9630},
  doi = {10.1111/jcpp.12034},
  abstract = {Background Our ability to look at structure and function of a living brain has increased exponentially since the early 1970s. Many studies of developmental disorders now routinely include a brain imaging or electrophysiological component. Amid current enthusiasm for applications of neuroscience to educational interventions, we need to pause to consider what neuroimaging data can tell us. Images of brain activity are seductive, and have been used to give credibility to commercial interventions, yet we have only a limited idea of what the brain bases of language disorders are, let alone how to alter them. Scope and findings A review of six studies of neuroimaging correlates of language intervention found recurring methodological problems: lack of an adequate control group, inadequate power, incomplete reporting of data, no correction for multiple comparisons, data dredging and failure to analyse treatment effects appropriately. In addition, there is a tendency to regard neuroimaging data as more meaningful than behavioural data, even though it is behaviour that interventions aim to alter. Conclusion In our current state of knowledge, it would be better to spend research funds doing well-designed trials of behavioural treatment to establish which methods are effective, rather than rushing headlong into functional imaging studies of unproven treatments.},
  pmcid = {PMC3593170},
  pmid = {23278309},
  file = {/Users/dorothybishop/Zotero/storage/BKYDLHVX/Bishop - 2013 - Research Review Emanuel Miller Memorial Lecture 2.pdf}
}

@article{bishop2016,
  title = {Problems in Using P-Curve Analysis and Text-Mining to Detect Rate of p-Hacking and Evidential Value},
  author = {Bishop, D. V. M. and Thompson, Paul A.},
  year = {2016},
  month = feb,
  journal = {PeerJ},
  volume = {4},
  pages = {e1715},
  publisher = {{PeerJ Inc.}},
  issn = {2167-8359},
  doi = {10.7717/peerj.1715},
  abstract = {Background. The p-curve is a plot of the distribution of p-values reported in a set of scientific studies. Comparisons between ranges of p-values have been used to evaluate fields of research in terms of the extent to which studies have genuine evidential value, and the extent to which they suffer from bias in the selection of variables and analyses for publication, p-hacking. Methods. p-hacking can take various forms. Here we used R code to simulate the use of ghost variables, where an experimenter gathers data on several dependent variables but reports only those with statistically significant effects. We also examined a text-mined dataset used by Head et al. (2015) and assessed its suitability for investigating p-hacking. Results. We show that when there is ghost p-hacking, the shape of the p-curve depends on whether dependent variables are intercorrelated. For uncorrelated variables, simulated p-hacked data do not give the ``p-hacking bump'' just below .05 that is regarded as evidence of p-hacking, though there is a negative skew when simulated variables are inter-correlated. The way p-curves vary according to features of underlying data poses problems when automated text mining is used to detect p-values in heterogeneous sets of published papers. Conclusions. The absence of a bump in the p-curve is not indicative of lack of p-hacking. Furthermore, while studies with evidential value will usually generate a right-skewed p-curve, we cannot treat a right-skewed p-curve as an indicator of the extent of evidential value, unless we have a model specific to the type of p-values entered into the analysis. We conclude that it is not feasible to use the p-curve to estimate the extent of p-hacking and evidential value unless there is considerable control over the type of data entered into the analysis. In particular, p-hacking with ghost variables is likely to be missed.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/8N5CDMZJ/Bishop and Thompson - 2016 - Problems in using p-curve analysis and text-mining.pdf;/Users/dorothybishop/Zotero/storage/E3B8HCDM/1715.html}
}

@article{bishop2017,
  title = {Phase 2 of {{CATALISE}}: A Multinational and Multidisciplinary {{Delphi}} Consensus Study of Problems with Language Development: {{Terminology}}},
  shorttitle = {Phase 2 of {{CATALISE}}},
  author = {Bishop, D. V. M. and Snowling, Margaret J. and Thompson, Paul A. and Greenhalgh, Trisha},
  year = {2017},
  journal = {Journal of Child Psychology and Psychiatry},
  volume = {58},
  number = {10},
  pages = {1068--1080},
  issn = {1469-7610},
  doi = {10.1111/jcpp.12721},
  abstract = {Background Lack of agreement about criteria and terminology for children's language problems affects access to services as well as hindering research and practice. We report the second phase of a study using an online Delphi method to address these issues. In the first phase, we focused on criteria for language disorder. Here we consider terminology. Methods The Delphi method is an iterative process in which an initial set of statements is rated by a panel of experts, who then have the opportunity to view anonymised ratings from other panel members. On this basis they can either revise their views or make a case for their position. The statements are then revised based on panel feedback, and again rated by and commented on by the panel. In this study, feedback from a second round was used to prepare a final set of statements in narrative form. The panel included 57 individuals representing a range of professions and nationalities. Results We achieved at least 78\% agreement for 19 of 21 statements within two rounds of ratings. These were collapsed into 12 statements for the final consensus reported here. The term `Language Disorder' is recommended to refer to a profile of difficulties that causes functional impairment in everyday life and is associated with poor prognosis. The term, `Developmental Language Disorder' (DLD) was endorsed for use when the language disorder was not associated with a known biomedical aetiology. It was also agreed that (a) presence of risk factors (neurobiological or environmental) does not preclude a diagnosis of DLD, (b) DLD can co-occur with other neurodevelopmental disorders (e.g. ADHD) and (c) DLD does not require a mismatch between verbal and nonverbal ability. Conclusions This Delphi exercise highlights reasons for disagreements about terminology for language disorders and proposes standard definitions and nomenclature.},
  langid = {english},
  keywords = {Consensus,definitions,Delphi Technique,Developmental language disorder,Humans,Language Development Disorders,risk factors,specific language impairment,terminology,Terminology as Topic},
  annotation = {\_eprint: https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/jcpp.12721},
  file = {/Users/dorothybishop/Zotero/storage/FLVTG6D6/Bishop et al. - 2017 - Phase 2 of CATALISE a multinational and multidisc.pdf;/Users/dorothybishop/Zotero/storage/G2S5KCJ8/Bishop et al. - 2017 - Phase 2 of CATALISE a multinational and multidisc.pdf;/Users/dorothybishop/Zotero/storage/EIH5AIJ7/jcpp.html}
}

@article{bishop2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, D. V. M.},
  year = {2019},
  month = apr,
  journal = {Nature},
  volume = {568},
  number = {7753},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/D3MMHU64/Bishop - 2019 - Rein in the four horsemen of irreproducibility.pdf;/Users/dorothybishop/Zotero/storage/JFX88ZT8/d41586-019-01307-2.html}
}

@article{bishop2020,
  title = {The Psychology of Experimental Psychologists: {{Overcoming}} Cognitive Constraints to Improve Research: {{The}} 47th {{Sir Frederic Bartlett Lecture}}},
  shorttitle = {The Psychology of Experimental Psychologists},
  author = {Bishop, D. V. M.},
  year = {2020},
  journal = {Quarterly Journal of Experimental Psychology (2006)},
  volume = {73},
  number = {1},
  pages = {1--19},
  issn = {1747-0218},
  doi = {10.1177/1747021819886519},
  abstract = {Like many other areas of science, experimental psychology is affected by a ``replication crisis'' that is causing concern in many fields of research. Approaches to tackling this crisis include better training in statistical methods, greater transparency and openness, and changes to the incentives created by funding agencies, journals, and institutions. Here, I argue that if proposed solutions are to be effective, we also need to take into account human cognitive constraints that can distort all stages of the research process, including design and execution of experiments, analysis of data, and writing up findings for publication. I focus specifically on cognitive schemata in perception and memory, confirmation bias, systematic misunderstanding of statistics, and asymmetry in moral judgements of errors of commission and omission. Finally, I consider methods that may help mitigate the effect of cognitive constraints: better training, including use of simulations to overcome statistical misunderstanding; specific programmes directed at inoculating against cognitive biases; adoption of Registered Reports to encourage more critical reflection in planning studies; and using methods such as triangulation and ``pre mortem'' evaluation of study design to foster a culture of dialogue and criticism.},
  pmcid = {PMC6909195},
  pmid = {31724919},
  keywords = {Cognitive bias,Reasoning},
  file = {/Users/dorothybishop/Zotero/storage/AIXFNULE/Bishop - 2020 - The psychology of experimental psychologists Over.pdf}
}

@article{bishop2022a,
  title = {Can We Shift Belief in the `{{Law}} of {{Small Numbers}}'?},
  author = {Bishop, D. V. M. and Thompson, Jackie and Parker, Adam J.},
  year = {2022},
  month = mar,
  journal = {Royal Society Open Science},
  volume = {9},
  number = {3},
  pages = {211028},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.211028},
  abstract = {`Sample size neglect' is a tendency to underestimate how the variability of mean estimates changes with sample size. We studied 100 participants, from science or social science backgrounds, to test whether a training task showing different-sized samples of data points (the `beeswarm' task) can help overcome this bias. Ability to judge if two samples came from the same population improved with training, and 38\% of participants reported that they had learned to wait for larger samples before making a response. Before and after training, participants completed a 12-item estimation quiz, including items testing sample size neglect (S-items). Bonus payments were given for correct responses. The quiz confirmed sample size neglect: 20\% of participants scored zero on S-items, and only two participants achieved more than 4/6 items correct. Performance on the quiz did not improve after training, regardless of how much learning had occurred on the beeswarm task. Error patterns on the quiz were generally consistent with expectation, though there were some intriguing exceptions that could not readily be explained by sample size neglect. We suggest that training with simulated data might need to be accompanied by explicit instruction to be effective in counteracting sample size neglect more generally.},
  keywords = {online training,power,sample size neglect,statistical reasoning},
  file = {/Users/dorothybishop/Zotero/storage/9IPEKUJV/Bishop et al. - 2022 - Can we shift belief in the ‘Law of Small Numbers’.pdf}
}

@misc{bishop2023a,
  title = {Using Multiple Outcomes in Intervention Studies: Improving Power While Controlling Type {{I}} Errors},
  shorttitle = {Using Multiple Outcomes in Intervention Studies},
  author = {Bishop, D. V. M.},
  year = {2023},
  month = jan,
  number = {10:991},
  institution = {{F1000Research}},
  doi = {10.12688/f1000research.73520.2},
  abstract = {Background The CONSORT guidelines for clinical trials recommend using a single primary outcome, to guard against excess false positive findings when multiple measures are considered. However, statistical power can be increased while controlling the familywise error rate if multiple outcomes are included. The MEff statistic is well-suited to this purpose, but is not well-known outside genetics. Methods Data were simulated for an intervention study, with a given sample size (N), effect size (E) and correlation matrix for a suite of outcomes ( R ). Using the variance of eigenvalues from the correlation matrix, we compute MEff, the effective number of variables that the alpha level should be divided by to control the familywise error rate. Various scenarios are simulated to consider how MEff is affected by the pattern of pairwise correlations within a set of outcomes. The power of the MEff approach is compared to Bonferroni correction, and a principal component analysis (PCA). Results In many situations, power can be increased by inclusion of multiple outcomes. Differences in power between MEff and Bonferroni correction are small if intercorrelations between outcomes are low, but the advantage of MEff is more evident as intercorrelations increase. PCA is superior in cases where the impact on outcomes is fairly uniform, but MEff is applicable when intervention effects are inconsistent across measures. Conclusions The optimal method for correcting for multiple testing depends on the underlying data structure, with PCA being superior if outcomes are all indicators of a common underlying factor. Both Bonferroni correction and MEff can be applied post hoc to evaluate published intervention studies, with MEff being superior when outcomes are moderately or highly correlated. A lookup table is provided to give alpha levels for use with Meff for cases where the correlation between outcome measures can be estimated.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {correlated outcomes,familywise error rate,intervention,methodology,multiple comparisons,power,statistics},
  file = {/Users/dorothybishop/Zotero/storage/9Y59NLS7/Bishop - 2023 - Using multiple outcomes in intervention studies i.pdf}
}

@article{block1996,
  title = {The {{Effects}} of the {{Edinburgh Masker}} on {{Stuttering}}},
  author = {Block, Susan and Ingham, Roger J. and Bench, R. John},
  year = {1996},
  month = jun,
  journal = {Australian Journal of Human Communication Disorders},
  volume = {24},
  number = {1},
  pages = {11--18},
  publisher = {{Taylor \& Francis}},
  issn = {0310-6853},
  doi = {10.3109/asl2.1996.24.issue-1.02},
  abstract = {Auditory feedback masking has long been thought to be a clinically useful procedure for the modification of stuttered speech in adults, and the Edinburgh Masker is a commercial device for providing such masking. In the present study, 18 subjects spoke under various masking and nonmasking conditions using the Edinburgh Masker, both in and beyond the clinic. Results showed that stuttering rate reduced by a mean of around 50\% in masking compared to nonmasking conditions. Only one subject completely eliminated stuttering, and did so in only one of many speaking tasks. Listeners judged masked speech to be less natural sounding than nonmasked speech. It is concluded that, for some clients, there may be some benefit in masked speech by means of the Edinburgh Masker, but that the device does not appear to produce either stutter-free or natural sounding speech.},
  annotation = {\_eprint: https://doi.org/10.3109/asl2.1996.24.issue-1.02},
  file = {/Users/dorothybishop/Zotero/storage/4NNIB7DI/asl2.1996.24.issue-1.html}
}

@article{borenstein2007,
  title = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry and Rothstein, Hannah},
  year = {2007},
  pages = {37},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/58SSEP87/Borenstein et al. - 2007 - Introduction to Meta-Analysis.pdf}
}

@book{borenstein2009,
  title = {Introduction to Meta-Analysis},
  author = {Borenstein, M and Hedges, L and Higgins, J P T and Rothstein, H},
  year = {2009},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470743386.fmatter},
  abstract = {The prelims comprise: Half-Title Page Title Page Copyright Page Table of Contents List of Tables List of Figures Acknowledgements Preface Web Site},
  isbn = {978-0-470-74338-6},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470743386.fmatter},
  file = {/Users/dorothybishop/Zotero/storage/PDJX4Z8V/2009 - Front Matter.pdf;/Users/dorothybishop/Zotero/storage/AGU9TNAN/9780470743386.html}
}

@article{bowen2020,
  title = {Independent Research and the {{Arrowsmith Program}}},
  author = {Bowen, Caroline},
  year = {2020},
  journal = {Perspectives on Language and Literacy},
  volume = {46},
  number = {1},
  pages = {47--54},
  langid = {american},
  file = {/Users/dorothybishop/Zotero/storage/WDRM8FH6/publication.html}
}

@article{boyes2021,
  title = {Piloting `{{Clever Kids}}': {{A}} Randomized-Controlled Trial Assessing Feasibility, Efficacy, and Acceptability of a Socioemotional Well-Being Programme for Children with Dyslexia},
  shorttitle = {Piloting `{{Clever Kids}}'},
  author = {Boyes, Mark E. and Leit{\~a}o, Suze and Claessen, Mary and Dzidic, Peta and Badcock, Nicholas A. and Nayton, Mandy},
  year = {2021},
  journal = {British Journal of Educational Psychology},
  volume = {91},
  number = {3},
  pages = {e12401},
  issn = {2044-8279},
  doi = {10.1111/bjep.12401},
  abstract = {Children with dyslexia are at elevated risk of internalizing (emotional) and externalizing (behavioural) problems. Clever Kids is a nine-week socioemotional well-being programme developed specifically for upper primary school children with dyslexia. In a small randomized-controlled trial, we tested the feasibility, efficacy, and acceptability of the Clever Kids programme.`Forty children (Mage = 10.45 years, 65\% male) with clinically diagnosed dyslexia too part in the study. Children were randomized to either attend Clever Kids (n = 20) or to a wait-list control condition (n = 20). Coping skills, self-esteem, resilience, emotion regulation, and internalizing and externalizing symptoms were measured at pre-programme, post-programme, and at three-month follow-up. Recruitment and retention rates indicate high feasibility for further evaluation of the programme. There was a significant interaction between intervention condition and time for non-productive coping [F(2, 76) = 4.29, p = 0.017, f2 = 0.11]. Children who attended Clever Kids significantly reduced their use of non-productive coping strategies, and this was maintained at three-month follow-up assessment. For all other outcomes, the interactions between intervention condition and time were non-significant. The programme appears acceptable to children with dyslexia and their families, but may be improved by further reducing the number of activities involving reading and writing. Clever Kids improved the coping skills of children with dyslexia; however, a larger trial is needed to replicate this finding and investigate whether programme attendance is associated with additional improvements in children's socioemotional well-being.},
  langid = {english},
  keywords = {coping,dyslexia,mental health,RCT,reading difficulties,self-esteem},
  annotation = {\_eprint: https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/bjep.12401},
  file = {/Users/dorothybishop/Zotero/storage/IXZPAM2N/Boyes et al. - 2021 - Piloting ‘Clever Kids’ A randomized-controlled tr.pdf;/Users/dorothybishop/Zotero/storage/7CRZL32R/bjep.html}
}

@book{bricker1999,
  title = {Ages and {{Stages Questionnaires}}: {{A}} Parent-Completed, Child-Monitoring System, 2nd Ed.},
  author = {Bricker, D and Squires, J},
  year = {1999},
  publisher = {{Paul H. Brookes}},
  address = {{Baltimore}}
}

@book{brown1973,
  title = {A {{First Language}} \textemdash{} {{Roger Brown}}},
  author = {Brown, Roger},
  year = {1973},
  publisher = {{Harvard University Press}},
  address = {{Cambridge, MA}},
  abstract = {For many years, Brown and his colleagues have studied the developing language of pre-school children -- the language that ultimately will permit them to understand themselves and their world. This longitudinal research records the conversational performances of three children, studying semantic and grammatical aspects of their language development.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/BTWCCNJ6/catalog.html}
}

@article{bull2007,
  title = {Sunflower Therapy for Children with Specific Learning Difficulties (Dyslexia): A Randomised, Controlled Trial},
  shorttitle = {Sunflower Therapy for Children with Specific Learning Difficulties (Dyslexia)},
  author = {Bull, Leona},
  year = {2007},
  month = feb,
  journal = {Complementary Therapies in Clinical Practice},
  volume = {13},
  number = {1},
  pages = {15--24},
  issn = {1744-3881},
  doi = {10.1016/j.ctcp.2006.07.003},
  abstract = {The aim of the study was to determine the clinical and perceived effectiveness of the Sunflower therapy in the treatment of childhood dyslexia. The Sunflower therapy includes applied kinesiology, physical manipulation, massage, homeopathy, herbal remedies and neuro-linguistic programming. A multi-centred, randomised controlled trial was undertaken with 70 dyslexic children aged 6-13 years. The research study aimed to test the research hypothesis that dyslexic children 'feel better' and 'perform better' as a result of treatment by the Sunflower therapy. Children in the treatment group and the control group were assessed using a battery of standardised cognitive, Literacy and self-esteem tests before and after the intervention. Parents of children in the treatment group gave feedback on their experience of the Sunflower therapy. Test scores were compared using the Mann Whitney, and Wilcoxon statistical tests. While both groups of children improved in some of their test scores over time, there were no statistically significant improvements in cognitive or Literacy test performance associated with the treatment. However, there were statistically significant improvements in academic self-esteem, and reading self-esteem, for the treatment group. The majority of parents (57.13\%) felt that the Sunflower therapy was effective in the treatment of learning difficulties. Further research is required to verify these findings, and should include a control group receiving a dummy treatment to exclude placebo effects.},
  langid = {english},
  pmid = {17210507},
  keywords = {Acupressure,Applied,Child,Combined Modality Therapy,Complementary Therapies,Dietary Supplements,Dyslexia,Female,Humans,Kinesiology,Kinesiology; Applied,Male,Manipulation,Manipulation; Osteopathic,Neurolinguistic Programming,Osteopathic,Phytotherapy}
}

@article{burgoyne2012,
  title = {Efficacy of a Reading and Language Intervention for Children with {{Down}} Syndrome: A Randomized Controlled Trial},
  shorttitle = {Efficacy of a Reading and Language Intervention for Children with {{Down}} Syndrome},
  author = {Burgoyne, Kelly and Duff, Fiona J and Clarke, Paula J and Buckley, Sue and Snowling, Margaret J and Hulme, Charles},
  year = {2012},
  month = oct,
  journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},
  volume = {53},
  number = {10},
  pages = {1044--1053},
  issn = {0021-9630},
  doi = {10.1111/j.1469-7610.2012.02557.x},
  abstract = {Background This study evaluates the effects of a language and literacy intervention for children with Down syndrome. Methods Teaching assistants (TAs) were trained to deliver a reading and language intervention to children in individual daily 40-min sessions. We used a waiting list control design, in which half the sample received the intervention immediately, whereas the remaining children received the treatment after a 20-week delay. Fifty-seven children with Down syndrome in mainstream primary schools in two UK locations (Yorkshire and Hampshire) were randomly allocated to intervention (40 weeks of intervention) and waiting control (20 weeks of intervention) groups. Assessments were conducted at three time points: pre-intervention, after 20 weeks of intervention, and after 40 weeks of intervention. Results After 20 weeks of intervention, the intervention group showed significantly greater progress than the waiting control group on measures of single word reading, letter-sound knowledge, phoneme blending and taught expressive vocabulary. Effects did not transfer to other skills (nonword reading, spelling, standardised expressive and receptive vocabulary, expressive information and grammar). After 40 weeks of intervention, the intervention group remained numerically ahead of the control group on most key outcome measures; but these differences were not significant. Children who were younger, attended more intervention sessions, and had better initial receptive language skills made greater progress during the course of the intervention. Conclusions A TA-delivered intervention produced improvements in the reading and language skills of children with Down syndrome. Gains were largest in skills directly taught with little evidence of generalization to skills not directly taught in the intervention.},
  pmcid = {PMC3470928},
  pmid = {22533801},
  file = {/Users/dorothybishop/Zotero/storage/TVFEQVKH/Burgoyne et al. - 2012 - Efficacy of a reading and language intervention fo.pdf}
}

@misc{burgoyne2016,
  title = {Reading and Language Intervention for Children with {{Down}} Syndrome: {{Experimental}} Data [Data Collection]},
  shorttitle = {Reading and Language Intervention},
  author = {Burgoyne, Kelly and Duff, Fiona J. and Clarke, Paula J. and Buckley, Sue and Snowling, Margaret J. and Hulme, Charles},
  year = {2016},
  abstract = {BACKGROUND: This study evaluates the effects of a language and literacy intervention for children with Down syndrome. METHODS: Teaching assistants (TAs) were trained to deliver a reading and language intervention to children in individual daily 40-min sessions. We used a waiting list control design, in which half the sample received the intervention immediately, whereas the remaining children received the treatment after a 20-week delay. Fifty-seven children with Down syndrome in mainstream primary schools in two U.K. locations (Yorkshire and Hampshire) were randomly allocated to intervention (40{$\quad$}weeks of intervention) and waiting control (20{$\quad$}weeks of intervention) groups. Assessments were conducted at three time points: pre-intervention, after 20{$\quad$}weeks of intervention, and after 40{$\quad$}weeks of intervention. RESULTS: After 20{$\quad$}weeks of intervention, the intervention group showed significantly greater progress than the waiting control group on measures of single word reading, letter-sound knowledge, phoneme blending and taught expressive vocabulary. Effects did not transfer to other skills (nonword reading, spelling, standardised expressive and receptive vocabulary, expressive information and grammar). After 40{$\quad$}weeks of intervention, the intervention group remained numerically ahead of the control group on most key outcome measures; but these differences were not significant. Children who were younger, attended more intervention sessions, and had better initial receptive language skills made greater progress during the course of the intervention. CONCLUSIONS: A TA-delivered intervention produced improvements in the reading and language skills of children with Down syndrome. Gains were largest in skills directly taught with little evidence of generalization to skills not directly taught in the intervention.},
  langid = {english},
  keywords = {Child,Child; Preschool,Down Syndrome,Dyslexia,Early Intervention,Early Intervention; Educational,Educational,Female,Humans,Language,Language Development Disorders,Language Therapy,Male,Phonetics,Preschool,Reading,Remedial Teaching,Treatment Outcome,United Kingdom,Vocabulary},
  file = {/Users/dorothybishop/Zotero/storage/ARWU5W8Z/Burgoyne et al. - 2012 - Efficacy of a reading and language intervention fo.pdf}
}

@article{burgoyne2018,
  title = {Evaluation of a Parent-Delivered Early Language Enrichment Programme: Evidence from a Randomised Controlled Trial},
  shorttitle = {Evaluation of a Parent-Delivered Early Language Enrichment Programme},
  author = {Burgoyne, Kelly and Gardner, Rachel and Whiteley, Helen and Snowling, Margaret J. and Hulme, Charles},
  year = {2018},
  month = may,
  journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},
  volume = {59},
  number = {5},
  pages = {545--555},
  issn = {1469-7610},
  doi = {10.1111/jcpp.12819},
  abstract = {BACKGROUND: It is widely believed that increasing parental involvement can improve children's educational outcomes although we lack good evidence for such claims. This study evaluated the effectiveness of a parent-delivered early language enrichment programme. METHODS: We conducted a randomised controlled trial (RCT) with 208 preschool children and their parents living in socially diverse areas in the United Kingdom. Families were allocated to an oral language programme (N~=~103) or an active control programme targeting motor skills (N~=~105). Parents delivered the programmes to their child at home in daily 20-min sessions over 30~weeks of teaching. RESULTS: Children receiving the language programme made significantly larger gains in language (d~=~.21) and narrative skills (d~=~.36) than children receiving the motor skills programme at immediate posttest. Effects on language were maintained 6~months later (d~=~.34), and at this point, the language group also scored higher on tests of early literacy (d values=.35 and .42). There was no evidence that the movement programme improved motor skills. CONCLUSIONS: This study provides evidence for the effectiveness of a parent-delivered language enrichment programme. Further large-scale evaluations of the programme are needed to confirm and extend these findings.},
  langid = {english},
  pmid = {28940192},
  keywords = {Child,Child; Preschool,Early Intervention,Early Intervention; Educational,early literacy,education,Educational,Female,Health Care,Humans,Language,Language Development,Literacy,Male,motor skills,Motor Skills,Outcome Assessment,Outcome Assessment; Health Care,parents,Parents,Preschool,randomised controlled trial,United Kingdom},
  file = {/Users/dorothybishop/Zotero/storage/9AYMML2A/Burgoyne et al. - 2018 - Evaluation of a parent-delivered early language en.pdf}
}

@article{button2018,
  title = {Reboot Undergraduate Courses for Reproducibility},
  author = {Button, K},
  year = {2018},
  month = sep,
  journal = {Nature},
  volume = {561},
  number = {7723},
  pages = {287--287},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-018-06692-8},
  abstract = {Collaboration across institutes can train students in open, team science, which better prepares them for challenges to come.},
  copyright = {2021 Nature},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: World View Subject\_term: Education, Research management},
  file = {/Users/dorothybishop/Zotero/storage/S4QSCW9S/Button - 2018 - Reboot undergraduate courses for reproducibility.pdf;/Users/dorothybishop/Zotero/storage/WC9ABEMU/d41586-018-06692-8.html}
}

@article{button2020,
  title = {Supporting `Team Science'},
  author = {Button, K},
  year = {2020},
  journal = {The Psychologist},
  volume = {33},
  pages = {30--33},
  file = {/Users/dorothybishop/Zotero/storage/6GVBB8DA/supporting-team-science.html}
}

@article{calder2018,
  title = {Combining Implicit and Explicit Intervention Approaches to Target Grammar in Young Children with {{Developmental Language Disorder}}},
  author = {Calder, Samuel D and Claessen, Mary and Leit{\~a}o, Suze},
  year = {2018},
  month = jun,
  journal = {Child Language Teaching and Therapy},
  volume = {34},
  number = {2},
  pages = {171--189},
  publisher = {{SAGE Publications Ltd}},
  issn = {0265-6590},
  doi = {10.1177/0265659017735392},
  abstract = {Children with Developmental Language Disorder are likely to experience difficulties with morphosyntax, especially regular past tense marking. Few studies have evaluated the effectiveness of intervention to improve morphosyntax in young school-aged children with DLD. This study investigated the efficacy of combined explicit and implicit intervention techniques delivered by a speech pathologist to improve receptive and expressive grammar, including the use of past tense morphosyntax, using a multiple baseline single case experimental design. Participants were aged six to seven years and received two 1:1 45 minute sessions per week for five weeks (total 7.5 hours) using Shape Coding intervention techniques combined with implicit approaches. Two of the three participants made statistically significant gains on standardized tests of general receptive and expressive grammar. Two of the three children made statistically significant improvement on measures of expressive morphosyntax, with one participant continuing to improve five weeks post treatment. Findings suggest that this approach was efficacious. These findings warrant further investigation using larger group comparison research studies.},
  langid = {english},
  keywords = {developmental language disorder,grammar,intervention,morphosyntax,shape coding},
  file = {/Users/dorothybishop/Zotero/storage/2KTJ3383/Calder et al. - 2018 - Combining implicit and explicit intervention appro.pdf}
}

@article{calder2021,
  title = {The {{Efficacy}} of an {{Explicit Intervention Approach}} to {{Improve Past Tense Marking}} for {{Early School-Age Children With Developmental Language Disorder}}},
  author = {Calder, Samuel D. and Claessen, Mary and Ebbels, Susan and Leit, {\~a}o Suze},
  year = {2021},
  month = jan,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {64},
  number = {1},
  pages = {91--104},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/2020_JSLHR-20-00132},
  abstract = {Purpose The aim of the study was to evaluate the efficacy of a theoretically motivated explicit intervention approach to improve regular past tense marking for early school-age children with developmental language disorder (DLD). Method Twenty-one children with DLD (ages 5;9\textendash 6;9 [years;months]) were included in a crossover randomized controlled trial (intervention, n = 10; waiting control, n = 11). Intervention included once-weekly sessions over 10 weeks using the SHAPE CODING system, in combination with a systematic cueing hierarchy to teach past tense marking. Once the first group completed intervention, the waiting control group crossed over to the intervention condition. The primary outcome was criterion-referenced measures of past tense marking with standardized measures of expressive and receptive grammar as the secondary outcome. Ancillary analyses on extension and behavioral control measures of morphosyntax were also conducted. Results There was a significant Time \texttimes{} Group interaction (p {$<$} .001) with a significant difference in pre\textendash post intervention improvement in favor of the intervention group (p {$<$} .001, d = 3.03). Further analysis once both groups had received the intervention revealed no improvement for either group on past tense production during the 5-week pre-intervention period, significant improvement pre\textendash post intervention (p {$<$} .001, d = 1.22), with gains maintained for 5 weeks postintervention. No significant differences were found on pre- to postintervention standardized measures of grammar, or on extension or control measures. Conclusions The efficacy of the theoretically motivated explicit grammar intervention was demonstrated. Results contribute to the evidence base supporting this intervention to improve past tense production in early school-age children with DLD, suggesting it is a viable option for clinicians to select when treating morphosyntactic difficulties for this population. Supplemental Material https://doi.org/10.23641/asha.13345202},
  file = {/Users/dorothybishop/Zotero/storage/GVW434HD/Calder et al. - 2021 - The Efficacy of an Explicit Intervention Approach .pdf}
}

@book{campbell2014,
  title = {How to Design, Analyse and Report Cluster Randomised Trials in Medicine and Health Related Research.},
  author = {Campbell, MJ and Walters, SJ},
  year = {2014},
  publisher = {{Wiley}},
  address = {{Chichester}}
}

@article{chalmers2014,
  title = {How to Increase Value and Reduce Waste When Research Priorities Are Set},
  author = {Chalmers, Iain and Bracken, Michael B. and Djulbegovic, Ben and Garattini, Silvio and Grant, Jonathan and G{\"u}lmezoglu, A. Metin and Howells, David W. and Ioannidis, John P. A. and Oliver, Sandy},
  year = {2014},
  month = jan,
  journal = {The Lancet},
  volume = {383},
  number = {9912},
  pages = {156--165},
  publisher = {{Elsevier}},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(13)62229-1},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}p{$>$}The increase in annual global investment in biomedical research\textemdash reaching US\$240 billion in 2010\textemdash has resulted in important health dividends for patients and the public. However, much research does not lead to worthwhile achievements, partly because some studies are done to improve understanding of basic mechanisms that might not have relevance for human health. Additionally, good research ideas often do not yield the anticipated results. As long as the way in which these ideas are prioritised for research is transparent and warranted, these disappointments should not be deemed wasteful; they are simply an inevitable feature of the way science works. However, some sources of waste cannot be justified. In this report, we discuss how avoidable waste can be considered when research priorities are set. We have four recommendations. First, ways to improve the yield from basic research should be investigated. Second, the transparency of processes by which funders prioritise important uncertainties should be increased, making clear how they take account of the needs of potential users of research. Third, investment in additional research should always be preceded by systematic assessment of existing evidence. Fourth, sources of information about research that is in progress should be strengthened and developed and used by researchers. Research funders have primary responsibility for reductions in waste resulting from decisions about what research to do.{$<$}/p{$>$}},
  langid = {english},
  pmid = {24411644},
  file = {/Users/dorothybishop/Zotero/storage/TS48J33T/Chalmers et al. - 2014 - How to increase value and reduce waste when resear.pdf;/Users/dorothybishop/Zotero/storage/WTHUQCA7/fulltext.html}
}

@book{chatfield2004,
  title = {The {{Analysis}} of {{Time Series}}: {{An Introduction}}},
  author = {Chatfield, C},
  year = {2004},
  publisher = {{Chapman and Hall}},
  address = {{Boca Raton}}
}

@article{chen2015,
  title = {Computing Tools for Implementing Standards for Single-Case Designs},
  author = {Chen, Li-Ting and Peng, Chao-Ying Joanne and Chen, Ming-E.},
  year = {2015},
  month = nov,
  journal = {Behavior Modification},
  volume = {39},
  number = {6},
  pages = {835--869},
  issn = {1552-4167},
  doi = {10.1177/0145445515603706},
  abstract = {In the single-case design (SCD) literature, five sets of standards have been formulated and distinguished: design standards, assessment standards, analysis standards, reporting standards, and research synthesis standards. This article reviews computing tools that can assist researchers and practitioners in meeting the analysis standards recommended by the What Works Clearinghouse: Procedures and Standards Handbook-the WWC standards. These tools consist of specialized web-based calculators or downloadable software for SCD data, and algorithms or programs written in Excel, SAS procedures, SPSS commands/Macros, or the R programming language. We aligned these tools with the WWC standards and evaluated them for accuracy and treatment of missing data, using two published data sets. All tools were tested to be accurate. When missing data were present, most tools either gave an error message or conducted analysis based on the available data. Only one program used a single imputation method. This article concludes with suggestions for an inclusive computing tool or environment, additional research on the treatment of missing data, and reasonable and flexible interpretations of the WWC standards.},
  langid = {english},
  pmid = {26358925},
  keywords = {computing,effect,Humans,intervention,Research Design,single-case design,software,Software,standards,Statistics as Topic,WWC},
  file = {/Users/dorothybishop/Zotero/storage/QV52C5SH/Chen et al. - 2015 - Computing Tools for Implementing Standards for Sin.pdf}
}

@article{cheverud2001,
  title = {A Simple Correction for Multiple Comparisons in Interval Mapping Genome Scans},
  author = {Cheverud, James M.},
  year = {2001},
  month = jul,
  journal = {Heredity},
  volume = {87},
  number = {1},
  pages = {52--58},
  publisher = {{Nature Publishing Group}},
  issn = {1365-2540},
  doi = {10.1046/j.1365-2540.2001.00901.x},
  abstract = {Several approaches have been proposed to correct point-wise significance thresholds used in interval-mapping genome scans. A method for significance threshold correction based on the Bonferroni test is presented. This test involves calculating the effective number of independent comparisons performed in a genome scan from the variance of the eigenvalues of the observed marker correlation matrix. The more highly correlated the markers, the higher the variance of the eigenvalues and the lower the number of independent tests performed on a chromosome. This approach was evaluated by mapping 1000 normally distributed phenotypes along chromosomes of varying length and marker density in a population size of 500. Experiment-wise significance thresholds obtained from the simulation are compared to those calculated using the Bonferroni criterion and the newly developed measure of the effective number of independent tests in a genome scan. The Bonferroni calculation produced significance thresholds very similar to those obtained by simulation. The threshold levels for both Bonferroni and simulation analysis depended strongly on the marker density and size of chromosomes. There was a slight bias of about 1\% in the thresholds obtained at the 5\% and 10\% point-wise significance levels. The method introduced here provides a relatively simple correction for multiple comparisons that can be easily calculated using standard statistics packages.},
  copyright = {2001 The Genetical Society of Great Britain},
  langid = {english},
  keywords = {Biomedicine,Cytogenetics,Ecology,Evolutionary Biology,general,Human Genetics,Plant Genetics and Genomics},
  file = {/Users/dorothybishop/Zotero/storage/2QUZM9Q3/Cheverud - 2001 - A simple correction for multiple comparisons in in.pdf;/Users/dorothybishop/Zotero/storage/BCA8UF7C/6889010.html}
}

@book{christensen2019,
  title = {Transparent and Reproducible Social Science Research: {{How}} to Do Open Science.},
  author = {Christensen, G and Freese, J and Miguel, E},
  year = {2019},
  publisher = {{University of California Press}},
  address = {{Oakland, CA}}
}

@book{cohen1988,
  title = {Statistical Power Analysis for the Behavioral Sciences, 2nd Edition},
  author = {Cohen, Jacob},
  year = {1988},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Hillsdale, NJ}},
  isbn = {0-8058-0283-5}
}

@article{cohen2005,
  title = {Effects of {{Computer-Based Intervention Through Acoustically Modified Speech}} ({{Fast ForWord}}) in {{Severe Mixed Receptive}}\textemdash{{Expressive Language Impairment}}},
  author = {Cohen, Wendy and Hodson, Ann and O, 'Hare Anne and Boyle, James and Durrani, Tariq and McCartney, Elspeth and Mattey, Mike and Naftalin, Lionel and Watson, Jocelynne},
  year = {2005},
  month = jun,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {48},
  number = {3},
  pages = {715--729},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/1092-4388(2005/049)},
  abstract = {Seventy-seven children between the ages of 6 and 10 years, with severe mixed receptive-expressive specific language impairment (SLI), participated in a randomized controlled trial (RCT) of Fast ForWord (FFW; Scientific Learning Corporation, 1997, 2001). FFW is a computer-based intervention for treating SLI using acoustically enhanced speech stimuli. These stimuli are modified to exaggerate their time and intensity properties as part of an adaptive training process. All children who participated in the RCT maintained their regular speech and language therapy and school regime throughout the trial. Standardized measures of receptive and expressive language were used to assess performance at baseline and to measure outcome from treatment at 9 weeks and 6 months. Children were allocated to 1 of 3 groups. Group A (n=23) received the FFWintervention as a home-based therapy for 6 weeks. Group B (n=27) received commercially available computer-based activities designed to promote language as a control for computer games exposure. Group C (n=27) received no additional study intervention. Each group made significant gains in language scores, but there was no additional effect for either computer intervention. Thus, the findings from this RCT do not support the efficacy of FFW as an intervention for children with severe mixed receptive-expressive SLI.},
  keywords = {computer applications,Fast ForWord,language disorders,randomized controlled trial},
  file = {/Users/dorothybishop/Zotero/storage/5T8ZKTTZ/Cohen et al. - 2005 - Effects of Computer-Based Intervention Through Aco.pdf}
}

@article{collier2011,
  title = {Understanding {{Process Tracing}}},
  author = {Collier, David},
  year = {2011},
  month = oct,
  journal = {PS: Political Science \& Politics},
  volume = {44},
  number = {4},
  pages = {823--830},
  publisher = {{Cambridge University Press}},
  issn = {1537-5935, 1049-0965},
  doi = {10.1017/S1049096511001429},
  abstract = {Process tracing is a fundamental tool of qualitative analysis. This method is often invoked by scholars who carry out within-case analysis based on qualitative data, yet frequently it is neither adequately understood nor rigorously applied. This deficit motivates this article, which offers a new framework for carrying out process tracing. The reformulation integrates discussions of process tracing and causal-process observations, gives greater attention to description as a key contribution, and emphasizes the causal sequence in which process-tracing observations can be situated. In the current period of major innovation in quantitative tools for causal inference, this reformulation is part of a wider, parallel effort to achieve greater systematization of qualitative methods. A key point here is that these methods can add inferential leverage that is often lacking in quantitative analysis. This article is accompanied by online teaching exercises, focused on four examples from American politics, two from comparative politics, three from international relations, and one from public health/epidemiology.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/WHIRQHHE/Collier - 2011 - Understanding Process Tracing.pdf}
}

@book{crystal1977,
  title = {The {{Grammatical Analysis}} of {{Language Disability}}: {{A Procedure}} for {{Assessment}} and {{Remediation}}},
  author = {Crystal, David and Fletcher, Paul and Garman, Michael},
  year = {1977},
  publisher = {{Edward Arnold}},
  address = {{London}}
}

@article{cunningham2013,
  title = {Exploratory Randomized Controlled Trial Evaluating the Impact of a Waiting List Control Design},
  author = {Cunningham, John A. and Kypri, Kypros and McCambridge, Jim},
  year = {2013},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {13},
  number = {1},
  pages = {150},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-150},
  abstract = {Employing waiting list control designs in psychological and behavioral intervention research may artificially inflate intervention effect estimates. This exploratory randomized controlled trial tested this proposition in a study employing a brief intervention for problem drinkers, one domain of research in which waiting list control designs are used.},
  keywords = {Alcohol,Alternate explanation,Brief intervention,Randomized controlled trials,Research methods,Waiting list control design},
  file = {/Users/dorothybishop/Zotero/storage/3FUAKS46/Cunningham et al. - 2013 - Exploratory randomized controlled trial evaluating.pdf;/Users/dorothybishop/Zotero/storage/MLRLSHHF/1471-2288-13-150.html}
}

@article{deangelis2004,
  title = {Clinical Trial Registration: A Statement from the {{International Committee}} of {{Medical Journal Editors}}},
  shorttitle = {Clinical Trial Registration},
  author = {De Angelis, Catherine and Drazen, Jeffrey M. and Frizelle, Frank A. and Haug, Charlotte and Hoey, John and Horton, Richard and Kotzin, Sheldon and Laine, Christine and Marusic, Ana and Overbeke, A. John P. M. and Schroeder, Torben V. and Sox, Hal C. and Van Der Weyden, Martin B. and {International Committee of Medical Journal Editors}},
  year = {2004},
  month = sep,
  journal = {Lancet (London, England)},
  volume = {364},
  number = {9438},
  pages = {911--912},
  issn = {1474-547X},
  doi = {10.1016/S0140-6736(04)17034-7},
  langid = {english},
  pmid = {15364170},
  keywords = {Biomedical and Behavioral Research,Clinical Trials as Topic,Editorial Policies,International Committee of Medical Journal Editors,Periodicals as Topic,Registries}
}

@article{delage2022,
  title = {A {{Preliminary Examination}} of the {{Impact}} of {{Working Memory Training}} on {{Syntax}} and {{Processing Speed}} in {{Children}} with {{ASD}}},
  author = {Delage, H{\'e}l{\`e}ne and Eigsti, Inge-Marie and Stanford, Emily and Durrleman, Stephanie},
  year = {2022},
  month = oct,
  journal = {Journal of Autism and Developmental Disorders},
  volume = {52},
  number = {10},
  pages = {4233--4251},
  issn = {1573-3432},
  doi = {10.1007/s10803-021-05295-z},
  abstract = {In addition to deficits in pragmatics, children with autism spectrum disorders (ASD) have weaknesses in complex syntax and working memory (WM). These two deficits may be closely related. Previous work investigated the effects of WM training in developmental language disorders and showed significant improvement in both WM and syntax. The current study tests the impact of 12~h of WM training across 8~weeks in 30 children with ASD, aged 5 to 11. Results showed direct improvements on untrained WM tasks, as well as transfer effects to  syntax and processing speed. Stronger WM led to better syntactic abilities. While they must be replicated, these exciting results provide impetus for further studies of WM interventions.},
  langid = {english},
  keywords = {Attention,Autism spectrum disorder,Children,Syntax,Training,Working memory},
  file = {/Users/dorothybishop/Zotero/storage/C9TXHSQG/Delage et al. - 2022 - A Preliminary Examination of the Impact of Working.pdf}
}

@article{denman2017,
  title = {Psychometric {{Properties}} of {{Language Assessments}} for {{Children Aged}} 4\textendash 12 {{Years}}: {{A Systematic Review}}},
  shorttitle = {Psychometric {{Properties}} of {{Language Assessments}} for {{Children Aged}} 4\textendash 12 {{Years}}},
  author = {Denman, Deborah and Speyer, Ren{\'e}e and Munro, Natalie and Pearce, Wendy M. and Chen, Yu-Wei and Cordier, Reinie},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.01515},
  abstract = {Abstract Introduction: Standardized assessments are widely used by speech pathologists in clinical and research settings to evaluate the language abilities of school-aged children and inform decisions about diagnosis, eligibility for services and intervention. Given the significance of these decisions, it is important that assessments have sound psychometric properties. Objective: The aim of this systematic review was to examine the psychometric quality of currently available comprehensive language assessments for school-aged children and identify assessments with the best evidence for use. Methods: Using the PRISMA framework as a guideline, a search of five databases and a review of websites and textbooks was undertaken to identify language assessments and published material on the reliability and validity of these assessments. The methodological quality of selected studies was evaluated using the COSMIN taxonomy and checklist. Results: Fifteen assessments were evaluated. For most assessments evidence of hypothesis testing (convergent and discriminant validity) was identified; with a smaller number of assessments having some evidence of reliability and content validity. No assessments presented with evidence of structural validity, internal consistency or error measurement. Overall, all assessments were identified as having limitations with regards to evidence of psychometric quality. Conclusions: Further research is required to provide good evidence of psychometric quality for currently available language assessments. Of the assessments evaluated, the Assessment of Literacy and Language, the Clinical Evaluation of Language Fundamentals- 5th Edition, the Clinical Evaluation of Language Fundamentals \textendash{} Preschool: 2nd Edition and the Preschool Language Scales \textendash{} 5th Edition presented with most evidence and are thus recommended for use.},
  langid = {english},
  keywords = {assessment reliability,assessment validity,Language assessment,Language impairment,psychometric properties},
  file = {/Users/dorothybishop/Zotero/storage/4Z74WMQS/Denman et al. - 2017 - Psychometric Properties of Language Assessments fo.pdf}
}

@misc{derringer2018a,
  title = {A Simple Correction for Non-Independent Tests},
  author = {Derringer, Jaime},
  year = {2018},
  month = apr,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/f2tyw},
  abstract = {Psychologists wrestle with how to best handle multiple comparisons, while maintaining a balance between false positives and false negatives. Undercorrection, such as ignoring the presence of multiple comparisons altogether, is known to yield an unacceptably high rate of false positives. Overcorrection, such as treating all tests as independent when they are not, results in overly conservative evaluations of statistical significance. This tutorial demonstrates \$M\_\{eff\}\$ correction, a method for adjusting statistical significance thresholds for multiple comparisons, without the assumption of independence of tests. This method, in which the effective number of tests (\$M\_\{eff\}\$) is estimated from the correlations among the variables being tested, was developed and validated in the field of genetics, but is based on statistical concepts (eigenvalues) that are very familiar to psychologists. \$M\_\{eff\}\$ correction can be applied in psychological research to balance the necessity of correction for multiple comparisons with the concerns that arise from complex, correlated tests.},
  langid = {american},
  keywords = {multiple comparisons,null hypothesis significance testing,Quantitative Methods,Social and Behavioral Sciences,spectral decomposition,tutorial},
  file = {/Users/dorothybishop/Zotero/storage/IT3C5KR7/Derringer - 2018 - A simple correction for non-independent tests.pdf}
}

@article{dipper2020,
  title = {Treatment for Improving Discourse in Aphasia: A Systematic Review and Synthesis of the Evidence Base},
  shorttitle = {Treatment for Improving Discourse in Aphasia},
  author = {Dipper, L. and Marshall, J. and Boyle, M. and Botting, N. and Hersh, D. and Pritchard, M. and Cruice, M.},
  year = {2020},
  month = jun,
  journal = {Aphasiology},
  publisher = {{Informa UK Limited}},
  issn = {0268-7038},
  doi = {10.1080/02687038.2020.1765305},
  abstract = {Background Improved discourse production is a priority for all key stakeholders in aphasia rehabilitation. A Cochrane review of randomised controlled trials (RCTs) for aphasia found speech and language therapy treatment to be effective for improving the ability to communicate in everyday interaction. However, this large-scale review did not focus exclusively on treatment for discourse production and did not include other treatment research designs. Thus, the extent of the evidence base addressing discourse interventions is currently unclear. Objective The present study undertakes the first systematic review of research on treatment for discourse production in aphasia, appraises the quality of the evidence base; characterises the methods for measuring outcomes; and describes discourse treatment in terms of both content and efficacy. Design Scopus, Medline, and EmBase databases were searched, providing 334 records. Twenty-five studies (reporting on 127 participants) met inclusion criteria and were reviewed with the following research questions: What is the quality of the study designs used? How complete is the intervention reporting? What is the range, type, and content of outcome measures used? What is the range, type, and content of discourse treatments reported to date? Are discourse treatments efficacious? Results Seven of the 25 studies met the criteria for quality review, with 3 RCTs scoring moderately well and 3 (of 4) case studies scoring moderate-low. Most studies had adequate levels of completeness of treatment reporting, with 3 scoring highly. There were 514 different outcome measures reported across the 25 studies, with measures of words-in-discourse the most common. Studies were grouped into six treatment categories: ``word production in discourse'', ``sentence production in discourse'', ``discourse macrostructure'', ``discourse scripts'', ``multi-level'', and ``no consensus''. Twenty-two studies reported post-treatment gains, most commonly noted in increased word production. Changes in sentence production and discourse macrostructure were present but infrequently assessed. Conclusions Discourse treatment is an emerging field of research. Despite limitations in the evidence base, there are clear positive signs that discourse treatment is efficacious. There is emerging evidence for beneficial effects on word and sentence production in discourse, for improved discourse macrostructure, and for treatments working at multiple levels of language. To strengthen the evidence in this field and improve outcomes for people with aphasia, we need more discourse treatment research using an explicit theoretical rationale, high-quality study designs, more complete reporting, and agreed treatment and assessment methods.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/A7X2TJN4/Dipper et al. - 2020 - Treatment for improving discourse in aphasia a sy.pdf;/Users/dorothybishop/Zotero/storage/9KWKXWG3/24272.html}
}

@article{dockrell2007,
  title = {Measuring and Understanding Patterns of Change in Intervention Studies with Children: {{Implications}} for Evidence-Based Practice},
  shorttitle = {Measuring and Understanding Patterns of Change in Intervention Studies with Children},
  author = {Dockrell, Julie E. and Law, James},
  year = {2007},
  month = jun,
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {1},
  number = {2},
  pages = {86--97},
  publisher = {{Taylor \& Francis}},
  issn = {1748-9539},
  doi = {10.1080/17489530701437204},
  abstract = {Purpose: Comparisons across studies of the effects of intervention are problematic. Such analyses raise both methodological and statistical challenges. A single data set was examined to investigate whether different established approaches to measuring change in children with specific language impairments alter the conclusions that can be drawn regarding the efficacy of an intervention. Methods: Measures of cognitive and language skills were collected at baseline and at six months following an intervention. Reliable and valid psychometric measures were used. Data from the intervention study were used to explore the patterns of results obtained using four different measures of change: change of diagnostic category, differential improvement across assessment measures, item specific changes and predictors of individual change. Results: Associations between different tests purporting to measure similar constructs were modest. The measures identified different children as impaired both at baseline and follow-up. No effect of intervention was evident when a categorical analysis of impairment was used. Both treatment and comparison children changed significantly across time on the majority of measures, providing evidence of development, but specific effects of the intensive intervention were evident using ANCOVAs. Item analysis indicated that one of the standardized language tests adopted in the evaluation was insensitive to change over a six month period. Change in individual children's performance was predicted by language level on entry to the project. Conclusion: The implications of the results are discussed in terms of the range of analytic approaches available to intervention researchers and the need to consider combinations of methods when analysing outcome data. \textdagger We would like to thank ICAN, the health trusts involved and the two research officers, Kerry Williams and Belinda Seeff, who collected the data.},
  keywords = {intervention,Language,measuring change,SLI},
  annotation = {\_eprint: https://doi.org/10.1080/17489530701437204},
  file = {/Users/dorothybishop/Zotero/storage/5LLGEGEZ/Dockrell and Law - 2007 - Measuring and understanding patterns of change in .pdf;/Users/dorothybishop/Zotero/storage/3Z6UMVDZ/17489530701437204.html}
}

@article{dockrell2015,
  title = {Measurement {{Issues}}: {{Assessing}} Language Skills in Young Children},
  shorttitle = {Measurement {{Issues}}},
  author = {Dockrell, Julie E. and Marshall, Chlo{\"e} R.},
  year = {2015},
  journal = {Child and Adolescent Mental Health},
  volume = {20},
  number = {2},
  pages = {116--125},
  issn = {1475-3588},
  doi = {10.1111/camh.12072},
  abstract = {Background Language and communication skills are central to children's ability to engage in social relationships and access learning experiences. This paper identifies issues which practitioners and researchers should consider when assessing language skills. A range of current language assessments is reviewed. Key findings Current screening measures do not meet psychometric prerequisites to identify language problems. There are significant challenges in the interpretation of language assessments, where socioeconomic status, language status and dialect, hearing impairment and test characteristics impact results. Conclusions Psychometrically sound assessments of language are an essential component of developing effective and efficient interventions. The language trajectories of preschool children vary substantially; current screening measures have significant limitations. Composite measures of language performance are better indicators of language problems and disorders than single measures of component skills.},
  langid = {english},
  keywords = {assessment,dynamic,Language,preschool,psychometrics},
  annotation = {\_eprint: https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/camh.12072},
  file = {/Users/dorothybishop/Zotero/storage/AJDYINNM/Dockrell and Marshall - 2015 - Measurement Issues Assessing language skills in y.pdf;/Users/dorothybishop/Zotero/storage/2MZNMZ3U/camh.html}
}

@article{duan2013,
  title = {Single-Patient (n-of-1) Trials: A Pragmatic Clinical Decision Methodology for Patient-Centered Comparative Effectiveness Research},
  shorttitle = {Single-Patient (n-of-1) Trials},
  author = {Duan, Naihua and Kravitz, Richard L. and Schmid, Christopher H.},
  year = {2013},
  month = aug,
  journal = {Journal of Clinical Epidemiology},
  volume = {66},
  number = {8 Suppl},
  pages = {S21-28},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2013.04.006},
  abstract = {OBJECTIVE: To raise awareness among clinicians and epidemiologists that single-patient (n-of-1) trials are potentially useful for informing personalized treatment decisions for patients with chronic conditions. STUDY DESIGN AND SETTING: We reviewed the clinical and statistical literature on methods and applications of single-patient trials and then critically evaluated the needs for further methodological developments. RESULTS: Existing literature reports application of 2,154 single-patient trials in 108 studies for diverse clinical conditions; various recent commentaries advocate for wider application of such trials in clinical decision making. Preliminary evidence from several recent pilot acceptability studies suggests that single-patient trials have the potential for widespread acceptance by patients and clinicians as an effective modality for increasing the therapeutic precision. Bayesian and adaptive statistical methods hold promise for increasing the informational yield of single-patient trials while reducing participant burden, but are not widely used. Personalized applications of single-patient trials can be enhanced through further development and application of methodologies on adaptive trial design, stopping rules, network meta-analysis, washout methods, and methods for communicating trial findings to patients and clinicians. CONCLUSIONS: Single-patient trials may be poised to emerge as an important part of the methodological armamentarium for comparative effectiveness research and patient-centered outcomes research. By permitting direct estimation of individual treatment effects, they can facilitate finely graded individualized care, enhance therapeutic precision, improve patient outcomes, and reduce costs.},
  langid = {english},
  pmcid = {PMC3972259},
  pmid = {23849149},
  keywords = {Adaptive trial,Bayes Theorem,Bayesian method,Borrow from strength,Carryover effect,Chronic Disease,Comparative Effectiveness Research,Cross-Over Studies,Crossover trial,Decision Support Techniques,Evidence-Based Medicine,Health Care,Humans,Outcome Assessment,Outcome Assessment; Health Care,Patient Participation,Patient Selection,Pilot Projects,Randomized Controlled Trials as Topic,Sequential trial,Washout},
  file = {/Users/dorothybishop/Zotero/storage/DBQBAGBV/Duan et al. - 2013 - Single-patient (n-of-1) trials a pragmatic clinic.pdf}
}

@article{duff2015,
  title = {Validity and Sensitivity of the Phonics Screening Check: Implications for Practice},
  shorttitle = {Validity and Sensitivity of the Phonics Screening Check},
  author = {Duff, Fiona J. and Mengoni, Silvana E. and Bailey, Alison M. and Snowling, Margaret J.},
  year = {2015},
  journal = {Journal of Research in Reading},
  volume = {38},
  number = {2},
  pages = {109--123},
  issn = {1467-9817},
  doi = {10.1111/1467-9817.12029},
  abstract = {Background Introduced in June 2012, the phonics screening check aims to assess whether 6-year-old children are meeting an appropriate standard in phonic decoding and to identify children struggling with phonic skills. Aims We investigated whether the check is a valid measure of phonic skill and is sensitive in identifying children at risk of reading difficulties. Sample We obtained teacher assessments of phonic skills for 292 six-year-old children and additional psychometric data for 160 of these children. Methods Teacher assessment data were accessed from schools via the local authority; psychometric tests were administered by researchers shortly after the phonics screening check. Results The check was strongly correlated with other literacy skills and was sensitive in identifying at-risk readers. So too were teacher judgements of phonics. Conclusions Although the check fulfils its aims, we argue that resources might be better focused on training and supporting teachers in their ongoing monitoring of phonics.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9817.12029},
  file = {/Users/dorothybishop/Zotero/storage/CHMZAGWY/Duff et al. - 2015 - Validity and sensitivity of the phonics screening .pdf;/Users/dorothybishop/Zotero/storage/BL3K9R4K/1467-9817.html}
}

@article{durrleman2022,
  title = {Training {{Syntax}} to {{Enhance Theory}} of {{Mind}} in {{Children}} with {{ASD}}},
  author = {Durrleman, Stephanie and Bentea, Anamaria and Prisecaru, Andreea and Thommen, Evelyne and Delage, H{\'e}l{\`e}ne},
  year = {2022},
  month = mar,
  journal = {Journal of Autism and Developmental Disorders},
  issn = {1573-3432},
  doi = {10.1007/s10803-022-05507-0},
  abstract = {Preschool children with neurotypical development (ND) trained on sentential complements (``X thinks/says that'') improve their Theory of Mind (ToM) performance. Can complementation training also enhance ToM~in children with Autism Spectrum Disorder (ASD)? Thirty-three children with ASD (Mage\,=\,8;11) and 20 younger ND peers (Mage\,=\,4;3) were trained on sentential complements (4\textendash 6~weeks, 2\textendash 3 times per week, via the DIRE i-Pad App). Pre-training and post-training comparisons show that (1) training boosted both complementation and ToM performance across groups; (2) improvements remained 4\textendash 6~weeks after training ended; (3) participants with milder ASD symptoms made most gains. Training on sentential complements thus seems beneficial for addressing ToM difficulties in children with ASD, especially those with milder symptoms.},
  langid = {english},
  keywords = {Autism,False belief,Linguistic intervention,Theory of Mind,Training program},
  file = {/Users/dorothybishop/Zotero/storage/J3AWWWLY/Durrleman et al. - 2022 - Training Syntax to Enhance Theory of Mind in Child.pdf}
}

@article{ebbels2019,
  title = {Evidence-Based Pathways to Intervention for Children with Language Disorders},
  author = {Ebbels, Susan H. and McCartney, Elspeth and Slonims, Vicky and Dockrell, Julie E. and Norbury, Courtenay Frazier},
  year = {2019},
  month = jan,
  journal = {International Journal of Language \& Communication Disorders},
  volume = {54},
  number = {1},
  pages = {3--19},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {1368-2822},
  doi = {10.1111/1460-6984.12387},
  abstract = {Background Paediatric speech and language therapist (SLT) roles often involve planning individualized intervention for specific children, working collaboratively with families and education staff, providing advice, training and coaching and raising awareness. A tiered approach to service delivery is currently recommended whereby services become increasingly specialized and individualized for children with greater needs. Aims To stimulate discussion regarding delivery of SLT services by examining evidence regarding the effectiveness of (1) intervention for children with language disorders at different tiers and (2) SLT roles within these tiers; and to propose an evidence-based model of SLT service delivery and a flowchart to aid clinical decision-making. Methods \& Procedures Meta-analyses and systematic reviews, together with controlled, peer-reviewed group studies where recent systematic reviews were not available, of interventions for children with language disorders are discussed, alongside the differing roles SLTs play in these interventions. Gaps in the evidence base are highlighted. Main Contribution The service-delivery model presented resembles the tiered model commonly used in education services, but divides individualized (Tier 3) services into Tier 3A: indirect intervention delivered by non-SLTs, and Tier 3B: direct intervention by an SLT. We report evidence for intervention effectiveness, which children might best be served by each tier, the role SLTs could take within each tier and the effectiveness of these roles. Regarding universal interventions provided to all children (Tier 1) and those targeted at children with language weaknesses or vulnerabilities (Tier 2), there is growing evidence that approaches led by education services can be effective when staff are highly trained and well supported. There is currently limited evidence regarding additional benefit of SLT-specific roles at Tiers 1 and 2. With regard to individualized intervention (Tier 3), children with complex or pervasive language disorders can progress following direct individualized intervention (Tier 3B), whereas children with milder or less pervasive difficulties can make progress when intervention is managed by an SLT, but delivered indirectly by others (Tier 3A), provided they are well trained and supported, and closely monitored. Conclusions \& Implications SLTs have a contribution to make at all tiers, but where prioritization for clinical services is a necessity, we need to establish the relative benefits and cost-effectiveness at each tier. Good evidence exists for SLTs delivering direct individualized intervention and we should ensure that this is available to children with pervasive and/or complex language disorders. In cases where service models are being provided which lack evidence, we strongly recommend that SLTs investigate the effectiveness of their approaches.},
  langid = {english},
  keywords = {clinical marker,communication outcomes,evidence based practice (EBP),intervention,joint attention intervention,language disorder,oral language,randomized controlled-trial,school-age-children,service   delivery models,service delivery models,vocabulary intervention,word   knowledge,word knowledge,young-children},
  annotation = {WOS:000454678800001},
  file = {/Users/dorothybishop/Zotero/storage/URTS9DXG/Ebbels et al. - 2019 - Evidence-based pathways to intervention for childr.pdf}
}

@article{elliott2002,
  title = {What Are We Doing to Waiting List Controls?},
  author = {Elliott, S. A. and Brown, J. S. L.},
  year = {2002},
  month = sep,
  journal = {Behaviour Research and Therapy},
  volume = {40},
  number = {9},
  pages = {1047--1052},
  issn = {0005-7967},
  doi = {10.1016/s0005-7967(01)00082-1},
  abstract = {For ethical reasons waiting list controls have been preferred to no treatment controls, provided the wait is still shorter than that for routine services. However, could significant differences arise from the wait being detrimental rather than the intervention being beneficial? Despite the number of studies employing this design, few have analysed intervention trials from the perspective of the waiting list controls rather than the experimental group. A Full Day Stress Management Workshop programme which had run successfully in Birmingham, was repeated in three areas in the South East of England. The data from the four areas were reanalysed to assess progress within the control group and to compare the final assessment points for the two groups. The control group did not show any significant deterioration during the three month wait for their workshop. Three months after their respective workshops, scores in the control groups did not differ significantly from those of the experimental group.},
  langid = {english},
  pmid = {12296489},
  keywords = {Adult,Clinical,Cognitive Behavioral Therapy,Ethics,Ethics; Clinical,Follow-Up Studies,Group,Humans,Psychological,Psychotherapy,Psychotherapy; Group,Stress,Stress; Psychological,Waiting Lists}
}

@article{ferguson2012,
  title = {A {{Vast Graveyard}} of {{Undead Theories}}: {{Publication Bias}} and {{Psychological Science}}'s {{Aversion}} to the {{Null}}},
  shorttitle = {A {{Vast Graveyard}} of {{Undead Theories}}},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {555--561},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612459059},
  abstract = {Publication bias remains a controversial issue in psychological science.The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications.We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science's capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous ``undead'' theories that are ideologically popular but have little basis in fact.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/ETEMJXK7/Ferguson and Heene - 2012 - A Vast Graveyard of Undead Theories Publication B.pdf}
}

@book{fielding1996,
  title = {Bridget {{Jones}}' {{Diary}}},
  author = {Fielding, Helen},
  year = {1996},
  publisher = {{Picador}},
  address = {{London}},
  isbn = {0-670-88072-8}
}

@techreport{foliano2019,
  title = {Changing Mindsets: {{Effectiveness}} Trial},
  author = {Foliano, F and Rolfe, H. and Buzzeo, J and Runge, J and Wilkinson, D},
  year = {2019},
  abstract = {Pupils in schools that received the intervention did not make any additional progress in literacy nor numeracy\textemdash as measured  by the national Key Stage 2 tests in reading, grammar, punctuation, and spelling (GPS), and maths\textemdash compared to pupils  in the control group. This finding has high security.  This  evaluation  also  examined  four  measures  of  non-cognitive  skills:  intrinsic  value, self-efficacy,  test  anxiety,  and self- regulation. The evaluation did not find evidence of an impact on these measures for pupils in schools that received Changing  Mindsets. A positive impact was found for the intrinsic value measure, but the impact was small and was not statistically  significant.    3. Among pupils eligible for free school meals (`FSM pupils'), those in schools that received the intervention did not make any  additional progress in literacy nor numeracy\textemdash as measured by the national Key Stage 2 tests in reading, GPS, and maths\textemdash{} compared to FSM pupils in schools that did not receive the intervention.   4. One explanation for the absence of a measurable impact on pupil attainment is the widespread use of the growth mindset  theory. Most teachers in the comparison schools (that did not receive the intervention) were familiar with this, and over a  third reported that they had attended training days based on the growth mindset approach.}
}

@article{forsythe2019,
  title = {Patient {{Engagement In Research}}: {{Early Findings From The Patient-Centered Outcomes Research Institute}}},
  shorttitle = {Patient {{Engagement In Research}}},
  author = {Forsythe, Laura P. and Carman, Kristin L. and Szydlowski, Victoria and Fayish, Lauren and Davidson, Laurie and Hickam, David H. and Hall, Courtney and Bhat, Geeta and Neu, Denese and Stewart, Lisa and Jalowsky, Maggie and Aronson, Naomi and Anyanwu, Chinenye Ursla},
  year = {2019},
  month = mar,
  journal = {Health Affairs},
  volume = {38},
  number = {3},
  pages = {359--367},
  publisher = {{Health Affairs}},
  issn = {0278-2715},
  doi = {10.1377/hlthaff.2018.05067},
  abstract = {Charged with ensuring that research produces useful evidence to inform health decisions, the Patient-Centered Outcomes Research Institute (PCORI) requires investigators to engage patients and other health care stakeholders, such as clinicians and payers, in the research process. Many PCORI studies result in articles published in peer-reviewed journals that detail research findings and engagement's role in research. To inform practices for engaging patients and others as research partners, we analyzed 126 articles that described engagement approaches and contributions to research. PCORI projects engaged patients and others as consultants and collaborators in determining the study design, selecting study outcomes, tailoring interventions to meet patients' needs and preferences, and enrolling participants. Many articles reported that engagement provided valuable contributions to research feasibility, acceptability, rigor, and relevance, while a few noted trade-offs of engagement. The findings suggest that engagement can support more relevant research through better alignment with patients' and clinicians' real-world needs and concerns.},
  file = {/Users/dorothybishop/Zotero/storage/2LQV6S87/Forsythe et al. - 2019 - Patient Engagement In Research Early Findings Fro.pdf}
}

@article{freedman1987,
  title = {Equipoise and the {{Ethics}} of {{Clinical Research}}},
  author = {Freedman, Benjamin},
  year = {1987},
  month = jul,
  journal = {New England Journal of Medicine},
  volume = {317},
  number = {3},
  pages = {141--145},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJM198707163170304},
  abstract = {THERE is widespread agreement that ethics requires that each clinical trial begin with an honest null hypothesis.1 , 2 In the simplest model, testing a new treatment B on a defined patient population P for which the current accepted treatment is A, it is necessary that the clinical investigator be in a state of genuine uncertainty regarding the comparative merits of treatments A and B for population P. If a physician knows that these treatments are not equivalent, ethics requires that the superior treatment be recommended. Following Fried, I call this state of uncertainty about the relative merits of A and B . . .},
  pmid = {3600702},
  annotation = {\_eprint: https://doi.org/10.1056/NEJM198707163170304},
  file = {/Users/dorothybishop/Zotero/storage/QEXBZQL4/Freedman - 1987 - Equipoise and the Ethics of Clinical Research.pdf;/Users/dorothybishop/Zotero/storage/6DVI4Y3D/NEJM198707163170304.html}
}

@article{fricke2017,
  title = {The Efficacy of Early Language Intervention in Mainstream School Settings: A Randomized Controlled Trial},
  shorttitle = {The Efficacy of Early Language Intervention in Mainstream School Settings},
  author = {Fricke, Silke and Burgoyne, Kelly and {Bowyer-Crane}, Claudine and Kyriacou, Maria and Zosimidou, Alexandra and Maxwell, Liam and Lerv{\aa}g, Arne and Snowling, Margaret J. and Hulme, Charles},
  year = {2017},
  journal = {Journal of Child Psychology and Psychiatry},
  volume = {58},
  number = {10},
  pages = {1141--1151},
  issn = {1469-7610},
  doi = {10.1111/jcpp.12737},
  abstract = {Background Oral language skills are a critical foundation for literacy and more generally for educational success. The current study shows that oral language skills can be improved by providing suitable additional help to children with language difficulties in the early stages of formal education. Methods We conducted a randomized controlled trial with 394 children in England, comparing a 30-week oral language intervention programme starting in nursery (N = 132) with a 20-week version of the same programme starting in Reception (N = 133). The intervention groups were compared to an untreated waiting control group (N = 129). The programmes were delivered by trained teaching assistants (TAs) working in the children's schools/nurseries. All testers were blind to group allocation. Results Both the 20- and 30-week programmes produced improvements on primary outcome measures of oral language skill compared to the untreated control group. Effect sizes were small to moderate (20-week programme: d = .21; 30-week programme: d = .30) immediately following the intervention and were maintained at follow-up 6 months later. The difference in improvement between the 20-week and 30-week programmes was not statistically significant. Neither programme produced statistically significant improvements in children's early word reading or reading comprehension skills (secondary outcome measures). Conclusions This study provides further evidence that oral language interventions can be delivered successfully by trained TAs to children with oral language difficulties in nursery and Reception classes. The methods evaluated have potentially important policy implications for early education.},
  langid = {english},
  keywords = {Early intervention,education,language,RCT design,reading},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcpp.12737},
  file = {/Users/dorothybishop/Zotero/storage/YDIVCNPA/Fricke et al. - 2017 - The efficacy of early language intervention in mai.pdf;/Users/dorothybishop/Zotero/storage/6AV7P8A4/jcpp.html}
}

@article{friedman2004,
  title = {Relationship {{Between Conflicts}} of {{Interest}} and {{Research Results}}},
  author = {Friedman, Lee S and Richter, Elihu D},
  year = {2004},
  month = jan,
  journal = {Journal of General Internal Medicine},
  volume = {19},
  number = {1},
  pages = {51--56},
  issn = {0884-8734},
  doi = {10.1111/j.1525-1497.2004.30617.x},
  abstract = {CONTEXT To date, research regarding the influence of conflicts of interest on the presentation of findings by researchers has been limited. OBJECTIVE To evaluate the sources of funding for published manuscripts, and association between reported findings and conflicts of interest. METHODS Data from both print and electronic issues of The New England Journal of Medicine (NEJM) and The Journal of the American Medical Association (JAMA) were analyzed for sources of funding, areas of investigation, conflict of interest (COI), and presentation of results. We reviewed all original manuscripts published during the year 2001 within NEJM (N = 193) and JAMA (N = 205). We use 3 definitions for COI in this paper: a broadly defined criterion, the criterion used by The International Council of Medical Journal Editors (ICMJE), and a criterion defined by the authors. RESULTS Depending on the COI criteria used, 16.6\% to 32.6\% of manuscripts had 1 or more author with COI. Based on ICMJE criterion, 38.7\% of studies investigating drug treatments had authors with COI. We observed a strong association between those studies whose authors had COI and reported positive findings (P {$<$} .001). When controlling for sample size, study design, and country of primary authors, we observed a strong association between positive results and COI (ICMJE definition) among all treatment studies (adjusted odds ratio [OR], 2.35; 95\% confidence interval [CI], 1.08 to 5.09) and drug studies alone (OR, 2.64; 95\% CI, 1.09 to 6.39). CONCLUSION COI is widespread among the authors of published manuscripts and these authors are more likely to present positive findings.},
  pmcid = {PMC1494677},
  pmid = {14748860},
  file = {/Users/dorothybishop/Zotero/storage/VUUBYX4B/Friedman and Richter - 2004 - Relationship Between Conflicts of Interest and Res.pdf}
}

@article{furukawa2014,
  title = {Waiting List May Be a Nocebo Condition in Psychotherapy Trials: A Contribution from Network Meta-Analysis},
  shorttitle = {Waiting List May Be a Nocebo Condition in Psychotherapy Trials},
  author = {Furukawa, T. A. and Noma, H. and Caldwell, D. M. and Honyashiki, M. and Shinohara, K. and Imai, H. and Chen, P. and Hunot, V. and Churchill, R.},
  year = {2014},
  journal = {Acta Psychiatrica Scandinavica},
  volume = {130},
  number = {3},
  pages = {181--192},
  issn = {1600-0447},
  doi = {10.1111/acps.12275},
  abstract = {Objective Various control conditions have been employed in psychotherapy trials, but there is growing suspicion that they may lead to different effect size estimates. The present study aims to examine the differences among control conditions including waiting list (WL), no treatment (NT) and psychological placebo (PP). Method We comprehensively searched for all randomized controlled trials (RCTs) comparing cognitive-behaviour therapies (CBT) against various control conditions in the acute phase treatment of depression, and applied network meta-analysis (NMA) to combine all direct and indirect comparisons among the treatment and control arms. Results We identified 49 RCTs (2730 participants) comparing WL, NT, PP and CBT. This network of evidence was consistent, and the effect size estimates for CBT were substantively different depending on the control condition. The odds ratio of response for NT over WL was statistically significant at 2.9 (95\% CI: 1.3\textendash 5.7). However, the quality of evidence, including publication bias, was less than ideal and none of the preplanned sensitivity analyses limiting to high-quality studies could be conducted, while findings of significant differences did not persist in post hoc sensitivity analyses trying to adjust for publication bias. Conclusion There may be important differences in control conditions currently used in psychotherapy trials.},
  langid = {english},
  keywords = {clinical trials,cognitive therapy,control groups,placebo,waiting lists},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/acps.12275},
  file = {/Users/dorothybishop/Zotero/storage/V243F79J/Furukawa et al. - 2014 - Waiting list may be a nocebo condition in psychoth.pdf;/Users/dorothybishop/Zotero/storage/WFRB53XT/acps.html}
}

@article{garralda2019,
  title = {New Clinical Trial Designs in the Era of Precision Medicine},
  author = {Garralda, Elena and Dienstmann, Rodrigo and Piris-Gim{\'e}nez, Alejandro and Bra{\~n}a, Irene and Rodon, Jordi and Tabernero, Josep},
  year = {2019},
  month = mar,
  journal = {Molecular Oncology},
  volume = {13},
  number = {3},
  pages = {549--557},
  issn = {1574-7891},
  doi = {10.1002/1878-0261.12465},
  abstract = {Cancer treatment has made significant strides towards the promise of personalized medicine. Recent scientific advances have shown that there are numerous genetic deregulations that are common in multiple cancer types, raising the possibility of developing drugs targeting those deregulations irrespective of the tumour type. Precision Cancer Medicine (PCM) was born out of accumulated evidence matching targeted agents with these tumour molecular deregulations. At the same time, the therapeutic armamentarium is rapidly increasing and the number of new drugs (including immune-oncology agents) entering drug development continues to rise. These factors, added to strong collaboration with regulatory agencies, which have approved novel agents based on data obtained from phase 1/2 trials, have led to unprecedented evolution in the design of early-stage clinical trials. Currently, we have seen rapid phase 1 dose-escalation trials followed by remarkably large expansion cohorts, and are witnessing the emergence of new trials, such as adaptive studies with basket and umbrella designs aimed at optimizing the biomarker\textendash drug co-development process. Alongside the growing complexity of these clinical trials, new frameworks for stronger and faster collaboration between all stakeholders in drug development, including academic institutions and frameworks, clinicians, pharma companies and regulatory agencies, have been established. In this review article, we describe the main challenges and opportunities that these new trial designs may provide for a more efficient drug development process, which may ultimately help ensure that PCM becomes a reality for patients.},
  pmcid = {PMC6396357},
  pmid = {30698321},
  file = {/Users/dorothybishop/Zotero/storage/NXP5JN6H/Garralda et al. - 2019 - New clinical trial designs in the era of precision.pdf}
}

@article{gillam2008,
  title = {The {{Efficacy}} of {{Fast ForWord-Language Intervention}} in {{School-Age Children}} with {{Language Impairment}}: {{A Randomized Controlled Trial}}},
  shorttitle = {The {{Efficacy}} of {{Fast ForWord-Language Intervention}} in {{School-Age Children}} with {{Language Impairment}}},
  author = {Gillam, Ronald B. and Loeb, Diane Frome and Hoffman, LaVae M. and Bohman, Thomas and Champlin, Craig A. and Thibodeau, Linda and Widen, Judith and Brandel, Jayne and {Friel-Patti}, Sandy},
  year = {2008},
  month = feb,
  journal = {Journal of speech, language, and hearing research : JSLHR},
  volume = {51},
  number = {1},
  pages = {97--119},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2008/007)},
  abstract = {Purpose A randomized controlled trial (RCT) was conducted to compare the language and auditory processing outcomes of children assigned to Fast ForWord-Language (FFW-L) to the outcomes of children assigned to nonspecific or specific language intervention comparison treatments that did not contain modified speech. Method Two hundred and sixteen children between the ages of 6 and 9 years with language impairments were randomly assigned to one of four arms: Fast ForWord-Language (FFW-L), academic enrichment (AE), computer-assisted language intervention (CALI), or individualized language intervention (ILI) provided by a speech-language pathologist. All children received 1 hour and 40 minutes of treatment, 5 days per week, for 6 weeks. Language and auditory processing measures were administered to the children by blinded examiners before treatment, immediately after treatment, 3 months after treatment, and 6 months after treatment. Results The children in all four arms improved significantly on a global language test and a test of backward masking. Children with poor backward masking scores who were randomized to the FFW-L arm did not present greater improvement on the language measures than children with poor backward masking scores who were randomized to the other three arms. Effect sizes, analyses of standard error of measurement, and normalization percentages supported the clinical significance of the improvements on the CASL. There was a treatment effect for the Blending Words subtest on the Comprehensive Test of Phonological Processing (). Participants in the FFW-L and CALI arms earned higher phonological awareness scores than children in the ILI and AE arms at the six-month follow-up testing. Conclusion Fast ForWord-Language, the language intervention that provided modified speech to address a hypothesized underlying auditory processing deficit, was not more effective at improving general language skills or temporal processing skills than a nonspecific comparison treatment (AE) or specific language intervention comparison treatments (CALI and ILI) that did not contain modified speech stimuli. These findings call into question the temporal processing hypothesis of language impairment and the hypothesized benefits of using acoustically modified speech to improve language skills. The finding that children in the three treatment arms and the active comparison arm made clinically relevant gains on measures of language and temporal auditory processing informs our understanding of the variety of intervention activities that can facilitate development.},
  pmcid = {PMC2361096},
  pmid = {18230858},
  file = {/Users/dorothybishop/Zotero/storage/PPWTSQAL/Gillam et al. - 2008 - The Efficacy of Fast ForWord-Language Intervention.pdf}
}

@article{goldacre2019,
  title = {{{COMPare}}: A Prospective Cohort Study Correcting and Monitoring 58 Misreported Trials in Real Time},
  shorttitle = {{{COMPare}}},
  author = {Goldacre, Ben and Drysdale, Henry and Dale, Aaron and Milosevic, Ioan and Slade, Eirion and Hartley, Philip and Marston, Cicely and {Powell-Smith}, Anna and Heneghan, Carl and Mahtani, Kamal R.},
  year = {2019},
  month = feb,
  journal = {Trials},
  volume = {20},
  number = {1},
  pages = {118},
  issn = {1745-6215},
  doi = {10.1186/s13063-019-3173-2},
  abstract = {Discrepancies between pre-specified and reported outcomes are an important source of bias in trials. Despite legislation, guidelines and public commitments on correct reporting from journals, outcome misreporting continues to be prevalent. We aimed to document the extent of misreporting, establish whether it was possible to publish correction letters on all misreported trials as they were published, and monitor responses from editors and trialists to understand why outcome misreporting persists despite public commitments to address it.},
  keywords = {Audit,CONSORT,Editorial conduct,ICMJE,Misreporting,Outcomes,Trials},
  file = {/Users/dorothybishop/Zotero/storage/8HZGQN2U/Goldacre et al. - 2019 - COMPare a prospective cohort study correcting and.pdf;/Users/dorothybishop/Zotero/storage/PN7SUD5E/s13063-019-3173-2.html}
}

@article{green2017,
  title = {Randomised Trial of a Parent-mediated Intervention for Infants at High Risk for Autism: Longitudinal Outcomes to Age 3 Years},
  shorttitle = {Randomised Trial of a Parent-mediated Intervention for Infants at High Risk for Autism},
  author = {Green, Jonathan and Pickles, Andrew and Pasco, Greg and Bedford, Rachael and Wan, Ming Wai and Elsabbagh, Mayada and Slonims, Vicky and Gliga, Teea and Jones, Emily and Cheung, Celeste and Charman, Tony and Johnson, Mark and Baron-Cohen, Simon and Bolton, Patrick and Davies, Kim and Liew, Michelle and Fernandes, Janice and Gammer, Isobel and Salomone, Erica and Ribeiro, Helena and Tucker, Leslie and Taylor, Carol and Booth, Rhonda and Harrop, Claire and Holsgrove, Samina and McNally, Janet},
  year = {2017},
  month = dec,
  journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},
  volume = {58},
  number = {12},
  pages = {1330--1340},
  issn = {0021-9630},
  doi = {10.1111/jcpp.12728},
  abstract = {Background There has been increasing interest in the potential for pre-emptive interventions in the prodrome of autism, but little investigation as to their effect. Methods A two-site, two-arm assessor-blinded randomised controlled trial (RCT) of a 12-session parent-mediated social communication intervention delivered between 9 and 14~months of age (Intervention in the British Autism Study of Infant Siblings-Video Interaction for Promoting Positive Parenting), against no intervention. Fifty-four infants (28 intervention, 26 nonintervention) at familial risk of autism but not otherwise selected for developmental atypicality were assessed at 9-month baseline, 15-month treatment endpoint, and 27- and 39-month follow-up. Primary outcome: severity of autism prodromal symptoms, blind-rated on Autism Observation Schedule for Infants or Autism Diagnostic Observation Schedule 2nd Edition across the four assessment points. Secondary outcomes: blind-rated parent\textendash child interaction and child language; nonblind parent-rated communication and socialisation. Prespecified intention-to-treat analysis combined estimates from repeated measures within correlated regressions to estimate the overall effect of the infancy intervention over time. Results Effect estimates in favour of intervention on autism prodromal symptoms, maximal at 27~months, had confidence intervals (CIs) at each separate time point including the null, but showed a significant overall effect over the course of the intervention and follow-up period (effect size [ES]~=~0.32; 95\% CI 0.04, 0.60; p~=~.026). Effects on proximal intervention targets of parent nondirectiveness/synchrony (ES~=~0.33; CI 0.04, 0.63; p~=~.013) and child attentiveness/communication initiation (ES~=~0.36; 95\% CI 0.04, 0.68; p~=~.015) showed similar results. There was no effect on categorical diagnostic outcome or formal language measures. Conclusions Follow-up to 3~years of the first RCT of a very early social communication intervention for infants at familial risk of developing autism has shown a treatment effect, extending 24~months after intervention end, to reduce the overall severity of autism prodromal symptoms and enhance parent\textendash child dyadic social communication over this period. We highlight the value of extended follow-up and repeat assessment for early intervention trials.},
  pmcid = {PMC5724485},
  pmid = {28393350},
  file = {/Users/dorothybishop/Zotero/storage/MWVXNIBV/Green et al. - 2017 - Randomised trial of a parent‐mediated intervention.pdf}
}

@article{greenwald1975,
  title = {Consequences of Prejudice against the Null Hypothesis},
  author = {Greenwald, Anthony G.},
  year = {1975},
  journal = {Psychological Bulletin},
  volume = {82},
  number = {1},
  pages = {1--20},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/h0076157},
  abstract = {Examined the consequences of prejudice against accepting the null hypothesis through (a) a mathematical model intended to stimulate the research-publication process and (b) case studies of apparent erroneous rejections of the null hypothesis in published psychological research. The input parameters for the model characterize investigators' probabilities of selecting a problem for which the null hypothesis is true, of reporting, following up on, or abandoning research when data do or do not reject the null hypothesis, and they characterize editors' probabilities of publishing manuscripts concluding in favor of or against the null hypothesis. With estimates of the input parameters based on a questionnaire survey of 75 social psychologists, the model output indicates a dysfunctional research-publication system. Particularly, the model indicates that there may be relatively few publications on problems for which the null hypothesis is (at least to a reasonable approximation) true, and of these, a high proportion will erroneously reject the null hypothesis. The case studies provide additional support for this conclusion. It is concluded that research traditions and customs of discrimination against accepting the null hypothesis may be very detrimental to research progress. (44 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Consequence,Experimentation,Mathematical Modeling,Null Hypothesis Testing},
  file = {/Users/dorothybishop/Zotero/storage/CJ4YJ6Y2/Greenwald - 1975 - Consequences of prejudice against the null hypothe.pdf;/Users/dorothybishop/Zotero/storage/JUYISGWX/1975-08612-001.html}
}

@article{haley2017a,
  title = {Oral Language Skills Intervention in Pre-School\textemdash a Cautionary Tale},
  author = {Haley, Allyson and Hulme, Charles and {Bowyer-Crane}, Claudine and Snowling, Margaret J. and Fricke, Silke},
  year = {2017},
  journal = {International Journal of Language \& Communication Disorders},
  volume = {52},
  number = {1},
  pages = {71--79},
  issn = {1460-6984},
  doi = {10.1111/1460-6984.12257},
  abstract = {Background While practitioners are increasingly asked to be mindful of the evidence base of intervention programmes, evidence from rigorous trials for the effectiveness of interventions that promote oral language abilities in the early years is sparse. Aims To evaluate the effectiveness of a language intervention programme for children identified as having poor oral language skills in preschool classes. Methods \& Procedures A randomized controlled trial was carried out in 13 UK nursery schools. In each nursery, eight children (N = 104, mean age = 3 years 11 months) with the poorest performance on standardized language measures were selected to take part. All but one child were randomly allocated to either an intervention (N = 52) or a waiting control group (N = 51). The intervention group received a 15-week oral language programme in addition to their standard nursery curriculum. The programme was delivered by trained teaching assistants and aimed to foster vocabulary knowledge, narrative and listening skills. Outcomes \& Results Initial results revealed significant differences between the intervention and control group on measures of taught vocabulary. No group differences were found on any standardized language measure; however, there were gains of moderate effect size in listening comprehension. Conclusions \& Implications The study suggests that an intervention, of moderate duration and intensity, for small groups of preschool children successfully builds vocabulary knowledge, but does not generalize to non-taught areas of language. The findings strike a note of caution about implementing language interventions of moderate duration in preschool settings. The findings also highlight the importance of including a control group in intervention studies.},
  langid = {english},
  keywords = {intervention,language,nursery,pre-school,RCT},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1460-6984.12257},
  file = {/Users/dorothybishop/Zotero/storage/FQX5Q44L/Haley et al. - 2017 - Oral language skills intervention in pre-school—a .pdf;/Users/dorothybishop/Zotero/storage/BPV7TB3J/1460-6984.html}
}

@article{hall,
  title = {Forty {{Years}} of {{Reading Intervention Research}} for {{Elementary Students}} with or at {{Risk}} for {{Dyslexia}}: {{A Systematic Review}} and {{Meta-Analysis}}},
  shorttitle = {Forty {{Years}} of {{Reading Intervention Research}} for {{Elementary Students}} with or at {{Risk}} for {{Dyslexia}}},
  author = {Hall, Colby and {Dahl-Leonard}, Katlynn and Cho, Eunsoo and Solari, Emily J. and Capin, Philip and Conner, Carlin L. and Henry, Alyssa R. and Cook, Lysandra and Hayes, Latisha and Vargas, Isabel and Richmond, Cassidi L. and Kehoe, Karen F.},
  journal = {Reading Research Quarterly},
  volume = {n/a},
  number = {n/a},
  issn = {1936-2722},
  doi = {10.1002/rrq.477},
  abstract = {This meta-analysis included experimental or quasi-experimental intervention studies conducted between 1980 and 2020 that aimed to improve reading outcomes for Grade K-5 students with or at risk for dyslexia (i.e., students with or at risk for word reading difficulties, defined as scoring at or below norm-referenced screening or mean baseline performance thresholds articulated in our inclusion criteria). In all, 53 studies reported in 52 publications met inclusion criteria (m = 351; total student N = 6,053). We employed robust variance estimation to address dependent effect sizes arising from multiple outcomes and comparisons within studies. Results indicated a statistically significant main effect of instruction on norm-referenced reading outcomes (g = 0.33; p {$<$} .001). Because there was significant heterogeneity in effect sizes across studies (p {$<$} .01), we used meta-regression to identify the degree to which student characteristics (i.e., grade level), intervention characteristics (i.e., dosage, instructional components, multisensory nature, instructional group size), reading outcome domain (i.e., phonological awareness, word reading/spelling, passage reading, or reading comprehension), or research methods (i.e., sample size, study design) influenced intervention effects. Dosage and reading outcome domain were the only variables that significantly moderated intervention effects (p = .040 and p = .024, respectively), with higher dosage studies associated with larger effects (b = 0.002) and reading comprehension outcomes associated with smaller effects than word reading/spelling outcomes (b = -0.080).},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rrq.477},
  file = {/Users/dorothybishop/Zotero/storage/KDJ7X2ES/Hall et al. - Forty Years of Reading Intervention Research for E.pdf}
}

@misc{hardwicke2021,
  title = {Preregistration: {{A}} Pragmatic Tool to Reduce Bias and Calibrate Confidence in Scientific Research},
  shorttitle = {Preregistration},
  author = {Hardwicke, Tom E. and Wagenmakers, Eric-Jan},
  year = {2021},
  month = apr,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/d7bcu},
  abstract = {Scientific research is performed by fallible humans. Degrees of freedom in the construction and selection of evidence and hypotheses grant scientists considerable latitude to obtain study outcomes that align more with their preferences than is warranted. This creates a risk of bias and can lead to scientists fooling themselves and fooling others. Preregistration involves archiving study information (e.g., hypotheses, methods, and analyses) in a public registry before data are inspected. This offers two potential benefits: (1) reduce bias by ensuring that research decisions are made independently of study outcomes; and (2) calibrate confidence in research by transparently communicating information about a study's risk of bias. In this article, we briefly review the historical evolution of preregistration in medicine, psychology, and other domains, clarify its pragmatic functions, discuss relevant meta-research, and provide recommendations for scientists and journal editors.},
  keywords = {bias,Medicine and Health Sciences,meta-research,multiplicity,preregistration,Social and Behavioral Sciences,transparency},
  file = {/Users/dorothybishop/Zotero/storage/CLVWMBL4/Hardwicke and Wagenmakers - 2021 - Preregistration A pragmatic tool to reduce bias a.pdf}
}

@techreport{hemingway2009,
  title = {What Is a Systematic Review? 2nd Edition.},
  author = {Hemingway, P and Brereton, N. J.},
  year = {2009}
}

@article{henrich2010,
  title = {Most People Are Not {{WEIRD}}},
  author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
  year = {2010},
  month = jul,
  journal = {Nature},
  volume = {466},
  number = {7302},
  pages = {29--29},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/466029a},
  abstract = {To understand human psychology, behavioural scientists must stop doing most of their experiments on Westerners, argue Joseph Henrich, Steven J. Heine and Ara Norenzayan.},
  copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Psychology and behaviour;Scientific community Subject\_term\_id: psychology-and-behaviour;scientific-community},
  file = {/Users/dorothybishop/Zotero/storage/T4625YL3/Henrich et al. - 2010 - Most people are not WEIRD.pdf;/Users/dorothybishop/Zotero/storage/NFAT4AQK/466029a.html}
}

@misc{hernan2018,
  title = {Causal {{Inference}} from {{Observational Data}}},
  author = {Hernan, Miguel},
  year = {2018},
  month = aug,
  journal = {Miguel Hernan's Faculty Website},
  abstract = {Try explaining to your extended family that you are considered an expert in causal inference. That's why, when people ask, I just say that my job is to learn what works for the prevention and\ldots},
  howpublished = {https://www.hsph.harvard.edu/miguel-hernan/research/causal-inference-from-observational-data/},
  langid = {american},
  file = {/Users/dorothybishop/Zotero/storage/YYUYPZZX/causal-inference-from-observational-data.html}
}

@article{hey2014,
  title = {The Questionable Use of Unequal Allocation in Confirmatory Trials},
  author = {Hey, Spencer Phillips and Kimmelman, Jonathan},
  year = {2014},
  month = jan,
  journal = {Neurology},
  volume = {82},
  number = {1},
  pages = {77--79},
  issn = {1526-632X},
  doi = {10.1212/01.wnl.0000438226.10353.1c},
  abstract = {Randomization is the standard means for addressing known and unknown confounders within the patient population in clinical trials. Although random assignment to treatment arms on a 1:1 basis has long been the norm, many 2-armed confirmatory trials now use unequal allocation schemes where the number of patients receiving investigational interventions exceeds those in the comparator arm. In what follows, we offer 3 arguments for why investigators, institutional review boards, and data and safety monitoring boards should exercise caution when planning or reviewing 2-armed confirmatory trials involving unequal allocation ratios. We close by laying out some of the conditions where uneven allocation can be justified ethically.},
  langid = {english},
  pmcid = {PMC3873626},
  pmid = {24306005},
  keywords = {Animals,Clinical Protocols,Humans,Random Allocation,Randomized Controlled Trials as Topic,Research Design},
  file = {/Users/dorothybishop/Zotero/storage/TKXSHKH8/Hey and Kimmelman - 2014 - The questionable use of unequal allocation in conf.pdf}
}

@misc{higgins2021,
  title = {Cochrane {{Handbook}} for {{Systematic Reviews}} of {{Interventions}}, Version 6.2 (Updated {{February}} 2021)},
  author = {Higgins, J P T and Thomas, J and Chandler, J and Cumpston, M and Li, T and Page, M J and Welch, V A},
  year = {2021},
  howpublished = {https://training.cochrane.org/handbook/current},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/FJU67HHD/current.html}
}

@article{hoare2013,
  title = {Introduction to a Generalized Method for Adaptive Randomization in Trials},
  author = {Hoare, Zo{\"e} SJ and Whitaker, Christopher J. and Whitaker, Rhiannon},
  year = {2013},
  month = jan,
  journal = {Trials},
  volume = {14},
  number = {1},
  pages = {19},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-14-19},
  abstract = {Ideally clinical trials should use some form of randomization for allocating participants to the treatment groups under trial. As an integral part of the process of assessing the effectiveness of these treatment groups, randomization performed well can reduce, if not eliminate, some forms of bias that can be evident in non-randomized trials. Given the vast set of possible randomization methods to choose from we demonstrate a method that incorporates many of the advantages of these other methods.},
  keywords = {Allocation Ratio,Consort Statement,Relevant Level,Simple Randomization,Stratification Variable},
  file = {/Users/dorothybishop/Zotero/storage/KADX5AIF/Hoare et al. - 2013 - Introduction to a generalized method for adaptive .pdf;/Users/dorothybishop/Zotero/storage/J6979AP8/1745-6215-14-19.html}
}

@article{holman2015,
  title = {Evidence of {{Experimental Bias}} in the {{Life Sciences}}: {{Why We Need Blind Data Recording}}},
  shorttitle = {Evidence of {{Experimental Bias}} in the {{Life Sciences}}},
  author = {Holman, Luke and Head, Megan L. and Lanfear, Robert and Jennions, Michael D.},
  year = {2015},
  month = jul,
  journal = {PLOS Biology},
  volume = {13},
  number = {7},
  pages = {e1002190},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002190},
  abstract = {Observer bias and other ``experimenter effects'' occur when researchers' expectations influence study outcome. These biases are strongest when researchers expect a particular result, are measuring subjective variables, and have an incentive to produce data that confirm predictions. To minimize bias, it is good practice to work ``blind,'' meaning that experimenters are unaware of the identity or treatment group of their subjects while conducting research. Here, using text mining and a literature review, we find evidence that blind protocols are uncommon in the life sciences and that nonblind studies tend to report higher effect sizes and more significant p-values. We discuss methods to minimize bias and urge researchers, editors, and peer reviewers to keep blind protocols in mind.},
  langid = {english},
  keywords = {Body weight,Clinical trials,Evolutionary biology,Medical journals,Medicine and health sciences,Metaanalysis,Statistical data,Text mining},
  file = {/Users/dorothybishop/Zotero/storage/BRHN28X2/Holman et al. - 2015 - Evidence of Experimental Bias in the Life Sciences.pdf;/Users/dorothybishop/Zotero/storage/7PSM5MPA/article.html}
}

@incollection{howard2003,
  title = {Chapter 16: {{Single Cases}}, {{Group Studies}} and {{Case Series}} in {{Aphasia Therapy}}},
  shorttitle = {{{PII}}},
  booktitle = {The {{Sciences}} of {{Aphasia}}: {{From Theory}} to {{Therapy}}. {{Ilias Papathanasiou}} and {{Ria De Bleser}} ({{Eds}}).},
  author = {Howard, David},
  year = {2003},
  pages = {245--258},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-008044073-6/50017-1},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/MPJNHFHN/PII B978-0-08-044073-6.50017-1  Elsevier Enhance.pdf;/Users/dorothybishop/Zotero/storage/JVPIJRV2/B9780080440736500171.html}
}

@book{humphreys2021,
  title = {Research {{Design}}: {{Declare}}, {{Diagnose}}, {{Redesign}}},
  author = {Humphreys, Alexander Coppock, {and} Macartan, Graeme Blair},
  year = {2021},
  abstract = {At its heart, a research design is a procedure for generating empirical answers to theoretical questions. Research designs can be strong or weak. Assessing whether a design is strong requires...},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/JTUG96PU/what-is-a-research-design.html}
}

@book{huntington-klein,
  title = {The Effect: An Introduction to Research Design and Causality},
  author = {{Huntington-Klein}, Nick},
  publisher = {{CBC Press}}
}

@article{ingham1981,
  title = {Some Effects of the {{Edinburgh Masker}} on Stuttering during Oral Reading and Spontaneous Speech},
  author = {Ingham, Roger J. and Southwood, Helen and Horsburgh, Gay},
  year = {1981},
  month = jun,
  journal = {Journal of Fluency Disorders},
  volume = {6},
  number = {2},
  pages = {135--154},
  issn = {0094-730X},
  doi = {10.1016/0094-730X(81)90011-5},
  abstract = {This study assessed the effect of a voice-activated masking unit, known as the Edinburgh Masker, on the speech of four stutterers during oral reading and spontaneous speech. The results show that one stutterer reduced stuttering almost completely whenever the masker was activated. Two subjects showed either marginal or temporary reductions of stuttering during one speaking condition but showed no change in the other condition. The other subject reduced stuttering only during spontaneous speech. No reduction in stuttering was associated with reduced speech rate. A perceptual analysis procedure conducted to assess for altered speech quality during masking conditions found changes in speech quality were evident in two subjects. The clinical implications of these findings are discussed.},
  langid = {english},
  keywords = {Preschool Language Scale,psychometrics},
  file = {/Users/dorothybishop/Zotero/storage/Y2EXW5NP/Ingham et al. - 1981 - Some effects of the Edinburgh Masker on stuttering.pdf;/Users/dorothybishop/Zotero/storage/5QWG7ZQ6/0094730X81900115.html}
}

@article{irwin2009,
  title = {The {{Role}} of {{Conflict}} of {{Interest}} in {{Reporting}} of {{Scientific Information}}},
  author = {Irwin, Richard S.},
  year = {2009},
  month = jul,
  journal = {Chest},
  volume = {136},
  number = {1},
  pages = {253--259},
  issn = {0012-3692},
  doi = {10.1378/chest.09-0890},
  abstract = {We have come to appreciate that scientific misconduct is often not intuitively obvious to those who perpetrate it. Therefore, this commentary has been written to review what we know about the role of conflict of interest (COI) in the reporting of scientific information and to challenge those of us in educator roles to do a better job in mentoring our trainees, junior faculty, and associates on what is right and wrong; what is ethical and unethical. The review addresses the following questions: (1) Why has the public trust in the clinical research industry been eroded? (2) How often is the ethical concept of equipoise violated in industry-sponsored randomized controlled clinical trials? (3) How often are negative trials underreported and favorable trials selectively or redundantly over-reported in industry-sponsored randomized controlled clinical trials? (4) What is being done to restore the public trust? While there are multiple strategies to mitigate COI in the reporting of scientific information, we have come to appreciate that the disclosure of potential conflicts of interest is not enough. It is our hope that this article and its contents can serve as a stimulus for the development and incorporation of an educational series in all training programs on what is ethical and unethical in the conducting and reporting of scientific studies.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/F55K7NNT/Irwin - 2009 - The Role of Conflict of Interest in Reporting of S.pdf;/Users/dorothybishop/Zotero/storage/LN8LKNHT/S001236920960430X.html}
}

@article{j2018,
  title = {Different Ways to Estimate Treatment Effects in Randomised Controlled Trials},
  author = {J, Twisk and L, Bosman and T, Hoekstra and J, Rijnhart and M, Welten and M, Heymans},
  year = {2018},
  month = jun,
  journal = {Contemporary Clinical Trials Communications},
  volume = {10},
  pages = {80--85},
  issn = {2451-8654},
  doi = {10.1016/j.conctc.2018.03.008},
  abstract = {Background Regarding the analysis of RCT data there is a debate going on whether an adjustment for the baseline value of the outcome variable should be made. When an adjustment is made, there is a lot of misunderstanding regarding the way this should be done. Therefore, the aims of this educational paper are: 1) to explain different methods used to estimate treatment effects in RCTs, 2) to illustrate the different methods with a real life example and 3) to give an advise on how to analyse RCT data. Methods Longitudinal analysis of covariance, repeated measures analysis in which also the baseline value is used as outcome and the analysis of changes were theoretically explained and applied to an example dataset investigating a systolic blood pressure lowering treatment. Results It was shown that differences at baseline should be taken into account and that regular repeated measures analysis and regular analysis of changes did not adjust for the baseline differences between the groups and therefore lead to biased estimates of the treatment effect. In the real life example, due to the differences at baseline between the treatment and control group, the different methods lead to different estimates of the treatment effect. Conclusion Regarding the analysis of RCT data, it is advised to use longitudinal analysis of covariance or a repeated measures analysis without the treatment variable, but with the interaction between treatment and time in the model.},
  langid = {english},
  keywords = {Analysis of changes,Longitudinal of covariance,Randomised controlled trials,Regression to the mean,Repeated measures},
  file = {/Users/dorothybishop/Zotero/storage/EHVCCZPV/J et al. - 2018 - Different ways to estimate treatment effects in ra.pdf;/Users/dorothybishop/Zotero/storage/9JG9PCRX/S2451865417301849.html}
}

@article{jacobson1991,
  title = {Clinical Significance: A Statistical Approach to Defining Meaningful Change in Psychotherapy Research},
  shorttitle = {Clinical Significance},
  author = {Jacobson, N. S. and Truax, P.},
  year = {1991},
  month = feb,
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {59},
  number = {1},
  pages = {12--19},
  issn = {0022-006X},
  doi = {10.1037//0022-006x.59.1.12},
  abstract = {In 1984, Jacobson, Follette, and Revenstorf defined clinically significant change as the extent to which therapy moves someone outside the range of the dysfunctional population or within the range of the functional population. In the present article, ways of operationalizing this definition are described, and examples are used to show how clients can be categorized on the basis of this definition. A reliable change index (RC) is also proposed to determine whether the magnitude of change for a given client is statistically reliable. The inclusion of the RC leads to a twofold criterion for clinically significant change.},
  langid = {english},
  pmid = {2002127},
  keywords = {Clinical Protocols,Female,Humans,Male,Marital Therapy,Marriage,Models,Models; Statistical,Psychotherapy,Research Design,Statistical},
  file = {/Users/dorothybishop/Zotero/storage/94H2UDF2/Jacobson and Truax - 1991 - Clinical significance a statistical approach to d.pdf}
}

@article{kasari2018,
  title = {{{SMARTer Approach}} to {{Personalizing Intervention}} for {{Children With Autism Spectrum Disorder}}},
  author = {Kasari, Connie and Sturm, Alexandra and Shih, Wendy},
  year = {2018},
  month = nov,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {61},
  number = {11},
  pages = {2629--2640},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/2018_JSLHR-L-RSAUT-18-0029},
  abstract = {Purpose This review article introduces research methods for personalization of intervention. Our goals are to review evidence-based practices for improving social communication impairment in children with autism spectrum disorder generally and then how these practices can be systematized in ways that personalize intervention, especially for children who respond slowly to an initial evidence-based practice. Method The narrative reflects on the current status of modular and targeted interventions on social communication outcomes in the field of autism research. Questions are introduced regarding personalization of interventions that can be addressed through research methods. These research methods include adaptive treatment designs and the Sequential Multiple Assignment Randomized Trial. Examples of empirical studies using research designs are presented to answer questions of personalization. Conclusion Bridging the gap between research studies and clinical practice can be advanced by research that attempts to answer questions pertinent to the broad heterogeneity in children with autism spectrum disorder, their response to interventions, and the fact that a single intervention is not effective for all children. Presentation Video https://doi.org/10.23641/asha.7298021},
  file = {/Users/dorothybishop/Zotero/storage/QHFGISSJ/Kasari et al. - 2018 - SMARTer Approach to Personalizing Intervention for.pdf}
}

@article{kerr1998,
  title = {{{HARKing}}: Hypothesizing after the Results Are Known},
  shorttitle = {{{HARKing}}},
  author = {Kerr, N. L.},
  year = {1998},
  journal = {Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc},
  volume = {2},
  number = {3},
  pages = {196--217},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english},
  pmid = {15647155},
  file = {/Users/dorothybishop/Zotero/storage/HNUJNUMK/Kerr - 1998 - HARKing hypothesizing after the results are known.pdf}
}

@article{koutsoftas2009,
  title = {The Effect of {{Tier}} 2 Intervention for Phonemic Awareness in a Response-to-Intervention Model in Low-Income Preschool Classrooms},
  author = {Koutsoftas, Anthony D. and Harmon, Mary Towle and Gray, Shelley},
  year = {2009},
  month = apr,
  journal = {Language, Speech, and Hearing Services in Schools},
  volume = {40},
  number = {2},
  pages = {116--130},
  issn = {0161-1461},
  doi = {10.1044/0161-1461(2008/07-0101)},
  abstract = {PURPOSE: This study assessed the effectiveness of a Tier 2 intervention that was designed to increase the phonemic awareness skills of low-income preschoolers who were enrolled in Early Reading First classrooms. METHOD: Thirty-four preschoolers participated in a multiple baseline across participants treatment design. Tier 2 intervention for beginning sound awareness was provided twice weekly in small groups over 6 weeks by trained teachers and speech-language pathologists (SLPs). RESULTS: The intervention was successful for 71\% of the children, as indicated by medium to large effect sizes. Comparisons between children who did and did not qualify for intervention suggest that Tier 2 intervention helped narrow the gap in beginning sound awareness that had begun to emerge before treatment. Although children receiving special education and those learning English as a second language were enrolled in the classrooms, they were not overrepresented in the group qualifying for Tier 2 intervention, and most who did qualify demonstrated a positive response to intervention. CONCLUSION: In a relatively short period of time, preschoolers' phonemic awareness skills were increased through small-group Tier 2 intervention provided by teachers and SLPs. Findings indicate the potential of Tier 2 interventions to positively impact the future reading skills of children who are at risk for later reading difficulties.},
  langid = {english},
  pmid = {18952818},
  keywords = {Child,Child; Preschool,Early Intervention,Early Intervention; Educational,Education,Education; Special,Educational,Female,Humans,Language Tests,Male,Phonetics,Poverty,Preschool,Schools,Special,Speech Therapy},
  file = {/Users/dorothybishop/Zotero/storage/F4Q35TAQ/Koutsoftas et al. - 2009 - The Effect of Tier 2 Intervention for Phonemic Awa.pdf}
}

@article{krasny-pacini2018,
  title = {Single-Case Experimental Designs to Assess Intervention Effectiveness in Rehabilitation: {{A}} Practical Guide},
  shorttitle = {Single-Case Experimental Designs to Assess Intervention Effectiveness in Rehabilitation},
  author = {{Krasny-Pacini}, Agata and Evans, Jonathan},
  year = {2018},
  month = may,
  journal = {Annals of Physical and Rehabilitation Medicine},
  volume = {61},
  number = {3},
  pages = {164--179},
  issn = {1877-0657},
  doi = {10.1016/j.rehab.2017.12.002},
  abstract = {Single-case experimental designs (SCED) are experimental designs aiming at testing the effect of an intervention using a small number of patients (typically one to three), using repeated measurements, sequential ({$\pm$}randomized) introduction of an intervention and method-specific data analysis, including visual analysis and specific statistics. The aim of this paper is to familiarise professionals working in different fields of rehabilitation with SCEDs and provide practical advice on how to design and implement a SCED in clinical rehabilitation practice. Research questions suitable for SCEDs and the different types of SCEDs (e.g., alternating treatment designs, introduction/withdrawal designs and multiple baseline designs) are reviewed. Practical steps in preparing a SCED design are outlined. Examples from different rehabilitation domains are provided throughout the paper. Challenging issues such as the choice of the repeated measure, assessment of generalisation, randomization, procedural fidelity, replication and generalizability of findings are discussed. Simple rules and resources for data analysis are presented. The utility of SCEDs in physical and rehabilitation medicine (PRM) are discussed.},
  langid = {english},
  keywords = {Alternating treatment,Methodology,Multiple baseline,Rehabilitation,Single-case},
  file = {/Users/dorothybishop/Zotero/storage/GTXS9KFF/Krasny-Pacini and Evans - 2018 - Single-case experimental designs to assess interve.pdf;/Users/dorothybishop/Zotero/storage/ISHQCBBA/S1877065717304542.html}
}

@article{kwok,
  title = {Measuring {{Change During Intervention Using Norm-Referenced}}, {{Standardized Measures}}: {{A Comparison}} of {{Raw Scores}}, {{Standard Scores}}, {{Age Equivalents}}, and {{Growth Scale Values From}} the {{Preschool Language Scales}}\textendash{{Fifth Edition}}},
  shorttitle = {Measuring {{Change During Intervention Using Norm-Referenced}}, {{Standardized Measures}}},
  author = {Kwok, Elaine and Feiner, Hannah and Grauzer, Jeffrey and Kaat, Aaron and Roberts, Megan Y.},
  journal = {Journal of Speech, Language, and Hearing Research},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/2022_JSLHR-22-00122},
  abstract = {Purpose:  Norm-referenced, standardized measures are tools designed to characterize a child's abilities relative to their same-age peers, but they also have been used to measure changes in skills during intervention. This study compared the psychometric properties of four types of available scores from one commonly used standardized measure, the Preschool Language Scales\textendash Fifth Edition (PLS-5), to detect changes in children's language skills during and after a language intervention. Method:  This study included data from 110 autistic children aged 18\textendash 48 months whose mother participated in an 8-week parent-mediated language intervention. Children's language skills were measured at 3 time points using the PLS-5. Changes in children's expressive and receptive language skills were calculated using raw scores, standard scores, age equivalents, and growth scale values (GSVs). Results:  Analysis of raw scores, age equivalents, and GSVs indicated significant improvement in the scores of autistic children in both receptive and expressive language throughout the study (i.e., during the intervention period and in the 3-month period after the intervention). Standard scores suggested improvement only in the receptive language scale during the intervention period. Standard scores showed a floor effect for children who scored at -3 SD below the mean. Conclusions:  Findings suggested that GSVs were not only psychometrically sound but also the most sensitive measure of direct changes in skills compared to raw, standard, and age-equivalent scores. Floor effects may limit the sensitivity of standard scores to detect changes in children's skills. Strengths, limitations, and interpretations of each of the scoring approaches in measuring changes in skills during intervention were discussed. Supplemental Material:  https://doi.org/10.23641/asha.21498522},
  keywords = {psychometric}
}

@article{lakens2018,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  copyright = {2018 The Publisher},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Human behaviour;Statistics Subject\_term\_id: human-behaviour;statistics},
  file = {/Users/dorothybishop/Zotero/storage/BXBZEA62/Lakens et al. - 2018 - Justify your alpha.pdf;/Users/dorothybishop/Zotero/storage/AMY3LWTK/s41562-018-0311-x.html}
}

@misc{lakens2021,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Daniel},
  year = {2021},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/9d3yf},
  abstract = {An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  keywords = {Experimental Design and Sample Surveys,power analysis,Quantitative Methods,sample size justification,Social and Behavioral Sciences,study design,value of information},
  file = {/Users/dorothybishop/Zotero/storage/6T93MGWI/Lakens - 2021 - Sample Size Justification.pdf}
}

@article{lakens2021a,
  title = {The {{Practical Alternative}} to the p {{Value Is}} the {{Correctly Used}} p {{Value}}},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {3},
  pages = {639--648},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620958012},
  abstract = {Because of the strong overreliance on p values in the scientific literature, some researchers have argued that we need to move beyond p values and embrace practical alternatives. When proposing alternatives to p values statisticians often commit the ?statistician?s fallacy,? whereby they declare which statistic researchers really ?want to know.? Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p value. As long as null-hypothesis tests have been criticized, researchers have suggested including minimum-effect tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask. If anyone believes p values affect the quality of scientific research, preventing the misinterpretation of p values by developing better evidence-based education and user-centered statistical software should be a top priority. Polarized discussions about which statistic scientists should use has distracted us from examining more important questions, such as asking researchers what they want to know when they conduct scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/DUV9LH4I/Lakens - 2021 - The Practical Alternative to the p Value Is the Co.pdf}
}

@article{lam2022,
  title = {Double-{{Blind}}, {{Sham-Controlled Randomized Trial Testing}} the {{Efficacy}} of {{fMRI Neurofeedback}} on {{Clinical}} and {{Cognitive Measures}} in {{Children With ADHD}}},
  author = {Lam, Sheut-Ling and Criaud, Marion and Lukito, Steve and Westwood, Samuel J. and Agbedjro, Deborah and Kowalczyk, Olivia S. and Curran, Sarah and Barret, Nadia and Abbott, Chris and Liang, Holan and Simonoff, Emily and Barker, Gareth J. and Giampietro, Vincent and Rubia, Katya},
  year = {2022},
  month = nov,
  journal = {American Journal of Psychiatry},
  pages = {appi.ajp.21100999},
  publisher = {{American Psychiatric Publishing}},
  issn = {0002-953X},
  doi = {10.1176/appi.ajp.21100999},
  abstract = {Objective: Functional MRI neurofeedback (fMRI-NF) could potentially be a novel, safe nonpharmacological treatment for attention deficit hyperactivity disorder (ADHD). A proof-of-concept randomized controlled trial of fMRI-NF of the right inferior frontal cortex (rIFC), compared to an active control condition, showed promising improvement of ADHD symptoms (albeit in both groups) and in brain function. However, comparison with a placebo condition in a larger trial is required to test efficacy. Methods: This double-blind, sham-controlled randomized controlled trial tested the effectiveness and efficacy of fMRI-NF of the rIFC on symptoms and executive functions in 88 boys with ADHD (44 each in the active and sham arms). To investigate treatment-related changes, groups were compared at the posttreatment and 6-month follow-up assessments, controlling for baseline scores, age, and medication status. The primary outcome measure was posttreatment score on the ADHD Rating Scale (ADHD-RS). Results: No significant group differences were found on the ADHD-RS. Both groups showed similar decreases in other clinical and cognitive measures, except for a significantly greater decrease in irritability and improvement in motor inhibition in sham relative to active fMRI-NF at the posttreatment assessment, covarying for baseline. There were no significant side effects or adverse events. The active relative to the sham fMRI-NF group showed enhanced activation in rIFC and other frontal and temporo-occipital-cerebellar self-regulation areas. However, there was no progressive rIFC upregulation, correlation with ADHD-RS scores, or transfer of learning. Conclusions: Contrary to the hypothesis, the study findings do not suggest that fMRI-NF of the rIFC is effective in improving clinical symptoms or cognition in boys with ADHD.},
  keywords = {Attention Deficit Hyperactivity Disorder (ADHD),controls,Neurodevelopmental Disorders,Neurofeedback,Neuroimaging,placebo},
  file = {/Users/dorothybishop/Zotero/storage/V7YCBJMJ/Lam et al. - 2022 - Double-Blind, Sham-Controlled Randomized Trial Tes.pdf}
}

@article{law2003,
  title = {Speech and Language Therapy Interventions for Children with Primary Speech and Language Delay or Disorder},
  author = {Law, James and Garrett, Zoe and Nye, Chad},
  year = {2003},
  journal = {Cochrane Database of Systematic Reviews},
  number = {3},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1465-1858},
  doi = {10.1002/14651858.CD004110},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/LC4A4AL5/information.html}
}

@article{law2004,
  title = {The Efficacy of Treatment for Children with Developmental Speech and Language Delay/Disorder: A Meta-Analysis},
  shorttitle = {The Efficacy of Treatment for Children with Developmental Speech and Language Delay/Disorder},
  author = {Law, James and Garrett, Zoe and Nye, Chad},
  year = {2004},
  month = aug,
  journal = {Journal of speech, language, and hearing research: JSLHR},
  volume = {47},
  number = {4},
  pages = {924--943},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2004/069)},
  abstract = {A meta-analysis was carried out of interventions for children with primary developmental speech and language delays/disorders. The data were categorized depending on the control group used in the study (no treatment, general stimulation, or routine speech and language therapy) and were considered in terms of the effects of intervention on expressive and receptive phonology, syntax, and vocabulary. The outcomes used in the analysis were dependent on the aims of the study; only the primary effects of intervention are considered in this review. These were investigated at the level of the target of therapy, measures of overall linguistic development, and broader measures of linguistic functioning taken from parent report or language samples. Thirty-six articles reporting 33 different trials were found. Of these articles, 25 provided sufficient information for use in the meta-analyses; however, only 13 of these, spanning 25 years, were considered to be sufficiently similar to be combined. The results indicated that speech and language therapy might be effective for children with phonological or expressive vocabulary difficulties. There was mixed evidence concerning the effectiveness of intervention for children with expressive syntax difficulties and little evidence available considering the effectiveness of intervention for children with receptive language difficulties. No significant differences were found between interventions administered by trained parents and those administered by clinicians. The review identified longer duration ({$>$}8 weeks) of therapy as being a potential factor in good clinical outcomes. A number of gaps in the evidence base are identified.},
  langid = {english},
  pmid = {15324296},
  keywords = {Child,Humans,Language Development Disorders,Language Tests,Language Therapy,Linguistics,Phonetics,Randomized Controlled Trials as Topic,Severity of Illness Index}
}

@article{law2017,
  title = {Speech and Language Therapy Interventions for Children with Primary Speech and/or Language Disorders},
  author = {Law, James and Dennis, Jane A. and Charlton, Jenna JV},
  year = {2017},
  journal = {Cochrane Database of Systematic Reviews},
  number = {1},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1465-1858},
  doi = {10.1002/14651858.CD012490},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/DUF7CF46/Law et al. - 2017 - Speech and language therapy interventions for chil.pdf;/Users/dorothybishop/Zotero/storage/WIXFQEAV/information.html}
}

@book{ledford2018,
  title = {Single {{Case Research Methodology Applications}} in {{Special Education}} and {{Behavioral Sciences}}, 3rd Edition},
  editor = {Ledford, J. R. and Gast, D. L.},
  year = {2018},
  publisher = {{Routledge}}
}

@article{ledford2019,
  title = {A {{Primer}} on {{Single-Case Research Designs}}: {{Contemporary Use}} and {{Analysis}}},
  shorttitle = {A {{Primer}} on {{Single-Case Research Designs}}},
  author = {Ledford, Jennifer R. and Barton, Erin E. and Severini, Katherine E. and Zimmerman, Kathleen N.},
  year = {2019},
  month = jan,
  journal = {American Journal on Intellectual and Developmental Disabilities},
  volume = {124},
  number = {1},
  pages = {35},
  publisher = {{American Association of Intellectual \& Developmental Disabilities}},
  address = {{Washington, United States}},
  issn = {19447515},
  doi = {http://dx.doi.org/10.1352/1944-7558-124.1.35},
  abstract = {The overarching purpose of this article is to provide an introduction to the use of rigorous single-case research designs (SCRDs) in special education and related fields. Authors first discuss basic design types and research questions that can be answered with SCRDs, examine threats to internal validity and potential ways to control for and detect common threats, and provide guidelines for selection of specific designs. Following, contemporary standards regarding rigor, measurement, description, and outcomes are presented. Then, authors discuss data analytic techniques, differentiating rigor, positive outcomes, functional relations, and magnitude of effects. Alternate abstract: Le but principal de cet article est de fournir une introduction a l'utilisation de devis de recherche rigoureux a cas unique en education specialisee et dans des domaines connexes. Les auteures abordent d'abord les types de conception de base et les questions de recherche pouvant etre resolues avec des devis de recherche a cas unique, examinent les menaces a la validite interne et les moyens potentiels de controler et de d\'etecter les menaces courantes, puis fournissent des directives pour la selection de conceptions specifiques. Les normes contemporaines concernant la rigueur, la mesure, la description et les resultats sont presentees. Finalement, les auteures discutent des techniques d'analyse des donnees, de la differenciation de la rigueur, des resultats positifs, des relations fonctionnelles et de l'ampleur des effets. Alternate abstract: El objetivo general de este art\'i\'iculo es proporcionar una introducci\'on al uso de dise\~nos rigurosos de investigaci\'on de caso unico (DRICu) en educaci\'on especial y campos relacionados. Los autores primero discuten los tipos de dise\~no basico y las preguntas de investigacio\'i n que pueden responderse con DRICu, examinan las amenazas a la validez interna y las posibles formas de controlar y detectar amenazas comunes, y proporcionan pautas para la selecci\'on de dise\~nos espec\'ificos. A continuaci\'on, se presentan los esta\'indares contempora\'ineos con respecto al rigor, la medici\'on, la descripci\'on y los resultados. Luego, los autores discuten tecnicas anal\'iticas de datos, rigor de diferenciaci\'on, resultados positivos, relaciones funcionales y magnitud de los efectos.},
  copyright = {Copyright American Association of Intellectual \& Developmental Disabilities Jan 2019},
  langid = {english},
  keywords = {Behavior,Civil rights,Developmental disabilities,Intervention,Medical Sciences--Psychiatry And Neurology,Medical Sciences–Psychiatry And Neurology,Research methodology,Special education,Standards,Validity},
  file = {/Users/dorothybishop/Zotero/storage/NIMCUUII/Ledford et al. - 2019 - A Primer on Single-Case Research Designs Contempo.pdf}
}

@book{leng2020,
  title = {The {{Matter}} of {{Facts}}: {{Skepticism}}, {{Persuasion}}, and {{Evidence}} in {{Science}}},
  author = {Leng, G and Leng, R. I},
  year = {2020},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}}
}

@article{leniston2021,
  title = {Investigation into the Effectiveness of Electropalatography in Treating Persisting Speech Sound Disorders in Adolescents with Co-Occurring Developmental Language Disorder},
  author = {Leniston, Hannah and Ebbels, Susan},
  year = {2021},
  month = jul,
  journal = {Clinical Linguistics \& Phonetics},
  pages = {1--16},
  issn = {1464-5076},
  doi = {10.1080/02699206.2021.1957022},
  abstract = {This study aimed to assess the effectiveness of Electropalatography (EPG) intervention in targeting specific phonemes/words in seven adolescents aged 14:10-18:06 with co-occurring speech sound and language disorders. Progress on individualised targets versus controls was evaluated following intervention undertaken as part of the participants' usual speech and language therapy provision. As a group, the participants showed significantly greater progress on their targets than controls, indicating that the EPG intervention was effective. However, performance varied between participants, targets and school terms. Factors that may have influenced the effectiveness of intervention include spending more time on targets and focusing on a specific phoneme. Overall, the results suggest EPG should be considered as an intervention approach for this client group, even in the late teenage years.},
  langid = {english},
  pmid = {34325597},
  keywords = {developmental language disorder,Electropalatography,intervention,school-aged children,speech sound disorder},
  file = {/Users/dorothybishop/Zotero/storage/C4V4PQDC/Leniston and Ebbels - 2021 - Investigation into the effectiveness of electropal.pdf}
}

@article{levitt2011,
  title = {Was {{There Really}} a {{Hawthorne Effect}} at the {{Hawthorne Plant}}? {{An Analysis}} of the {{Original Illumination Experiments}}},
  shorttitle = {Was {{There Really}} a {{Hawthorne Effect}} at the {{Hawthorne Plant}}?},
  author = {Levitt, Steven D. and List, John A.},
  year = {2011},
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {3},
  number = {1},
  pages = {224--238},
  issn = {1945-7782},
  doi = {10.1257/app.3.1.224},
  abstract = {The "Hawthorne effect" draws its name from a landmark set of studies conducted at the Hawthorne plant in the 1920s. The data from the first and most influential of these studies, the "Illumination Experiment," were never formally analyzed and were thought to have been destroyed. Our research has uncovered these data. Existing descriptions of supposedly remarkable data patterns prove to be entirely fictional. We do find more subtle manifestations of possible Hawthorne effects. We also propose a new means of testing for Hawthorne effects based on excess responsiveness to experimenter- induced variations relative to naturally occurring variation. (JEL C90, J24, J28, M12, M54, N32)},
  langid = {english},
  keywords = {and Philanthropy: U.S.,Canada: 1913-,Demography,Design of Experiments: General,Design of Experiments: General; Human Capital,Economic History: Labor and Consumers,Education,Executive Compensation,Executive Compensation; Personnel Economics: Labor Management; Economic History: Labor and Consumers; Demography; Education; Health; Welfare; Income; Wealth; Religion; and Philanthropy: U.S.,Health,Human Capital,Income,Job Satisfaction,Labor Productivity,Labor Productivity; Safety,Occupational Choice,Personnel Economics: Labor Management,Personnel Management,Related Public Policy,Related Public Policy; Personnel Management,Religion,Safety,Skills,Wealth,Welfare},
  file = {/Users/dorothybishop/Zotero/storage/25H6FQU2/Levitt and List - 2011 - Was There Really a Hawthorne Effect at the Hawthor.pdf;/Users/dorothybishop/Zotero/storage/C5D8FVF7/articles.html}
}

@book{lo2009,
  title = {Conflict of {{Interest}} in {{Medical Research}}, {{Education}}, and {{Practice}}},
  author = {Lo, B and Field, M. J},
  year = {2009},
  publisher = {{National Academies Press}},
  address = {{Washington, DC}},
  isbn = {13: 978-0-309-13188-9}
}

@article{loeb2001,
  title = {Language Changes Associated with {{Fast ForWord-language}}: {{Evidence}} from Case Studies},
  shorttitle = {Language Changes Associated with {{Fast ForWord-language}}},
  author = {Loeb, Diane Frome and Stoke, Christene and Fey, Marc E.},
  year = {2001},
  journal = {American Journal of Speech-Language Pathology},
  volume = {10},
  number = {3},
  pages = {216--230},
  publisher = {{American Speech-Language-Hearing Assn}},
  address = {{US}},
  issn = {1558-9110(Electronic),1058-0360(Print)},
  doi = {10.1044/1058-0360(2001/020)},
  abstract = {The Scientific Learning Corporation claims that Fast ForWord-Language (FFW-L) yields 1-1/2 to 3 years language gain over a 6-week period. The authors evaluated various aspects of this claim by measuring the language changes of 4 children (aged 5 yrs 6 mo to 8 yrs 1 mo) who received FFW-L language intervention in their homes. Language change was assessed immediately following intervention and 3 months later, using standardized language measures, spontaneous measures of syntactic complexity, reading measures, pragmatic measures, and parent and teacher reports. Three of the 4 children successfully completed FFW-L, and all made gains on some of the same standardized measures used by P. Tallal et al (1996), although the improvements the authors observed were generally smaller than those previously reported. All children also made gains on measures of pragmatic performance. However, very few changes were observed in the children's Developmental Sentence Scores. Parents and teachers did not report many differences in performance after intervention; however, parental satisfaction with the program was generally high. 61\% of the gains observed at posttesting were maintained 3 months following intervention. Significant positive change occurred on 10\% of the items. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Communication Disorders,Computer Applications,Computer Software,Language Disorders},
  file = {/Users/dorothybishop/Zotero/storage/AHF8NP79/2001-11320-003.html}
}

@book{lord1968,
  title = {Stastistical Theories of Mental Test Scores},
  author = {Lord, F. M. and Novick, M. R. and Birnbaum, Allan},
  year = {1968},
  publisher = {{Addison-Wesley}}
}

@misc{ludlow2013,
  title = {Need for {{Adaptive Research Designs}} in {{Speech-Language Pathology}}},
  author = {Ludlow, C},
  year = {2013},
  month = nov,
  journal = {ASHA Journals Academy},
  abstract = {This presentation was delivered as part of a workshop session on the application of adaptive design principles to clinical trials in speech-language pathology. For an overview of adaptive designs, see Adaptive Trial Designs for the Development of Treatment Parameters. The ... Read More},
  howpublished = {https://academy.pubs.asha.org/2013/11/need-for-adaptive-research-designs-in-speech-language-pathology/},
  langid = {american},
  file = {/Users/dorothybishop/Zotero/storage/KVI8BBEW/need-for-adaptive-research-designs-in-speech-language-pathology.html}
}

@book{mahoney1976,
  title = {Scientist as {{Subject}}: {{The Psychological Imperative}}},
  author = {Mahoney, Michael J},
  year = {1976},
  publisher = {{Ballinger Publishing Company}},
  address = {{Cambridge, MA}}
}

@article{mallick2018,
  title = {A Cluster Randomised Trial of a Classroom Communication Resource Program to Change Peer Attitudes towards Children Who Stutter among Grade 7 Students},
  author = {Mallick, Rizwana and Kathard, Harsha and Borhan, A. S. M. and Pillay, Mershen and Thabane, Lehana},
  year = {2018},
  month = nov,
  journal = {Trials},
  volume = {19},
  number = {1},
  pages = {664},
  issn = {1745-6215},
  doi = {10.1186/s13063-018-3043-3},
  abstract = {Classroom-based stuttering intervention addressing negative peer attitudes, perceptions, teasing and bullying of children who stutter (CWS) is required as part of holistic stuttering management because of its occurrence in primary school. This study was conducted in 2017, in 10 primary schools in the Western Cape, South Africa within lower (second and third) and higher (fourth and fifth) quintiles.},
  keywords = {Peer Attitudes,Quintile Groups,School Clusters,Significant Subgroup Effect,Subgroup Objects},
  file = {/Users/dorothybishop/Zotero/storage/4R3TJ9EH/Mallick et al. - 2018 - A cluster randomised trial of a classroom communic.pdf;/Users/dorothybishop/Zotero/storage/NA7LD3PU/s13063-018-3043-3.html}
}

@article{manolov2017,
  title = {How {{Can Single-Case Data Be Analyzed}}? {{Software Resources}}, {{Tutorial}}, and {{Reflections}} on {{Analysis}}},
  shorttitle = {How {{Can Single-Case Data Be Analyzed}}?},
  author = {Manolov, Rumen and Moeyaert, Mariola},
  year = {2017},
  month = mar,
  journal = {Behavior Modification},
  volume = {41},
  number = {2},
  pages = {179--228},
  publisher = {{SAGE Publications Inc}},
  issn = {0145-4455},
  doi = {10.1177/0145445516664307},
  abstract = {The present article aims to present a series of software developments in the quantitative analysis of data obtained via single-case experimental designs (SCEDs), as well as the tutorial describing these developments. The tutorial focuses on software implementations based on freely available platforms such as R and aims to bring statistical advances closer to applied researchers and help them become autonomous agents in the data analysis stage of a study. The range of analyses dealt with in the tutorial is illustrated on a typical single-case dataset, relying heavily on graphical data representations. We illustrate how visual and quantitative analyses can be used jointly, giving complementary information and helping the researcher decide whether there is an intervention effect, how large it is, and whether it is practically significant. To help applied researchers in the use of the analyses, we have organized the data in the different ways required by the different analytical procedures and made these data available online. We also provide Internet links to all free software available, as well as all the main references to the analytical techniques. Finally, we suggest that appropriate and informative data analysis is likely to be a step forward in documenting and communicating results and also for increasing the scientific credibility of SCEDs.},
  langid = {english},
  keywords = {data analysis,effect size,single-case designs,software,tutorial},
  file = {/Users/dorothybishop/Zotero/storage/3D8CZFYG/Manolov and Moeyaert - 2017 - How Can Single-Case Data Be Analyzed Software Res.pdf;/Users/dorothybishop/Zotero/storage/QM9IIIIZ/Manolov and Moeyaert - 2017 - How Can Single-Case Data Be Analyzed Software Res.pdf}
}

@article{manolov2017a,
  title = {Recommendations for {{Choosing Single-Case Data Analytical Techniques}}},
  author = {Manolov, Rumen and Moeyaert, Mariola},
  year = {2017},
  month = jan,
  journal = {Behavior Therapy},
  volume = {48},
  number = {1},
  pages = {97--114},
  issn = {0005-7894},
  doi = {10.1016/j.beth.2016.04.008},
  abstract = {The current paper responds to the need to provide guidance to applied single-case researchers regarding the possibilities of data analysis. The amount of available single-case data analytical techniques has been growing during recent years and a general overview, comparing the possibilities of these techniques, is missing. Such an overview is provided that refers to techniques that yield results in terms of a raw or standardized difference and procedures related to regression analysis, as well as nonoverlap and percentage change indices. The comparison is provided in terms of the type of quantification provided, data features taken into account, conditions in which the techniques are appropriate, possibilities for meta-analysis, and evidence available on their performance. Moreover, we provide a set of recommendations for choosing appropriate analysis techniques, pointing at specific situations (aims, types of data, researchers' resources) and the data analytical techniques that are most appropriate in these situations. The recommendations are contextualized using a variety of published single-case data sets in order to illustrate a range of realistic situations that researchers have faced and may face in their investigations.},
  langid = {english},
  keywords = {data analysis,recommendations,single-case designs},
  file = {/Users/dorothybishop/Zotero/storage/ZHGIYUT6/Manolov and Moeyaert - 2017 - Recommendations for Choosing Single-Case Data Anal.pdf;/Users/dorothybishop/Zotero/storage/CCLSMHEJ/S0005789416300284.html}
}

@article{manolov2018,
  title = {Analyzing Data from Single-Case Alternating Treatments Designs},
  author = {Manolov, Rumen and Onghena, Patrick},
  year = {2018},
  journal = {Psychological Methods},
  volume = {23},
  number = {3},
  pages = {480--504},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000133},
  abstract = {Alternating treatments designs (ATDs) have received comparatively less attention than other single-case experimental designs in terms of data analysis, as most analytical proposals and illustrations have been made in the context of designs including phases with several consecutive measurements in the same condition. One of the specific features of ATDs is the rapid (and usually randomly determined) alternation of conditions, which requires adapting the analytical techniques. First, we review the methodologically desirable features of ATDs, as well as the characteristics of the published single-case research using an ATD, which are relevant for data analysis. Second, we review several existing options for ATD data analysis. Third, we propose 2 new procedures, suggested as alternatives improving some of the limitations of extant analytical techniques. Fourth, we illustrate the application of existing techniques and the new proposals in order to discuss their differences and similarities. We advocate for the use of the new proposals in ATDs, because they entail meaningful comparisons between the conditions without assumptions about the design or the data pattern. We provide R code for all computations and for the graphical representation of the comparisons involved. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Experimental Design,Graphical Displays,Statistical Analysis,Statistical Regression,Test Construction,Trends},
  file = {/Users/dorothybishop/Zotero/storage/7235E2VE/Manolov and Onghena - 2018 - Analyzing data from single-case alternating treatm.pdf;/Users/dorothybishop/Zotero/storage/FYSC95P9/2017-12026-001.html}
}

@article{mcarthur2008,
  title = {Does {{What Works Clearinghouse Work}}? {{A Brief Review}} of {{Fast ForWord}}\textregistered},
  shorttitle = {Does {{What Works Clearinghouse Work}}?},
  author = {McArthur, Genevieve},
  year = {2008},
  month = apr,
  journal = {Australasian Journal of Special Education},
  volume = {32},
  number = {1},
  pages = {101--107},
  publisher = {{Routledge}},
  issn = {1030-0112},
  doi = {10.1080/10300110701845953},
  abstract = {The What Works Clearinghouse (WWC) provides online reports to the public about the scientific evidence for educational interventions. The quality of these reports is important because they effectively tell the non-scientific community which programmes do and do not work. The aim of this brief review is to assess WWC's report on a clinically popular, yet theoretically controversial, intervention called Fast ForWord\textregistered{} (FFW). Some of the methods used by WWC to assess FFW were problematic: the literature review included studies that had not passed peer review; it failed to include a key study that had passed peer review; alphabetic skills were assessed with phonological awareness outcomes; effectiveness ratings were based on statistical significance; terms peculiar to WWC were not clearly defined; and existing quality control procedures failed to detect an error in the WWC report. These problems could be addressed by making minor adjustments to WWC's existing methods and by subjecting WWC reports to the scientific peer-review process before they are released to the public.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/10300110701845953},
  file = {/Users/dorothybishop/Zotero/storage/7CWLZ5QU/10300110701845953.html}
}

@article{mcgillion2017,
  title = {A Randomised Controlled Trial to Test the Effect of Promoting Caregiver Contingent Talk on Language Development in Infants from Diverse Socioeconomic Status Backgrounds},
  author = {McGillion, Michelle and Pine, Julian M. and Herbert, Jane S. and Matthews, Danielle},
  year = {2017},
  journal = {Journal of Child Psychology and Psychiatry},
  volume = {58},
  number = {10},
  pages = {1122--1131},
  issn = {1469-7610},
  doi = {10.1111/jcpp.12725},
  abstract = {Background Early language skills are critical for later academic success. Lower socioeconomic status (SES) children tend to start school with limited language skills compared to advantaged peers. We test the hypothesis that this is due in part to differences in caregiver contingent talk during infancy (how often the caregiver talks about what is in the focus of the infant's attention). Methods In a randomised controlled trial with high and low SES families, 142 11-month olds and their caregivers were randomly allocated to either a contingent talk intervention or a dental health control. Families in the language intervention watched a video about contingent talk and were asked to practise it for 15 min a day for a month. Caregiver communication was assessed at baseline and after 1 month. Infant communication was assessed at baseline, 12, 15, 18 and 24 months. Results At baseline, social gradients were observed in caregiver contingent talk to their 11-month olds (but not in infant communication). At posttest, when infants were 12 months old, caregivers across the SES spectrum who had been allocated to the language intervention group engaged in significantly more contingent talk. Lower SES caregivers in this intervention group also reported that their children produced significantly more words at 15 and 18 months. Effects of the intervention did not persist at 24 months. Instead expressive vocabulary at this age was best predicted by baseline infant communication, baseline contingent talk and SES. Conclusions A social gradient in children's communication emerges during the second year of life. A low-intensity intervention demonstrated that it is possible to increase caregiver contingent talk and that this is effective in promoting vocabulary growth for lower SES infants in the short term. However, these effects are not long-lasting, suggesting that follow-up interventions may be necessary to yield benefits lasting to school entry.},
  langid = {english},
  keywords = {Infancy,intervention,language,parenting,socioeconomic status,vocabulary},
  annotation = {\_eprint: https://acamh.onlinelibrary.wiley.com/doi/pdf/10.1111/jcpp.12725},
  file = {/Users/dorothybishop/Zotero/storage/ACAW4PEC/McGillion et al. - 2017 - A randomised controlled trial to test the effect o.pdf;/Users/dorothybishop/Zotero/storage/U5L7BV5H/jcpp.html}
}

@article{mirza2017,
  title = {The History and Development of {{N-of-1}} Trials},
  author = {Mirza, RD and Punja, S and Vohra, S and Guyatt, G},
  year = {2017},
  month = aug,
  journal = {Journal of the Royal Society of Medicine},
  volume = {110},
  number = {8},
  pages = {330--340},
  publisher = {{SAGE Publications}},
  issn = {0141-0768},
  doi = {10.1177/0141076817721131},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/PAEEEBQM/Mirza et al. - 2017 - The history and development of N-of-1 trials.pdf}
}

@article{moeyaert2014,
  title = {The {{Influence}} of the {{Design Matrix}} on {{Treatment Effect Estimates}} in the {{Quantitative Analyses}} of {{Single-Subject Experimental Design Research}}},
  author = {Moeyaert, Mariola and Ugille, Maaike and Ferron, John M. and Beretvas, S. Natasha and {Van den Noortgate}, Wim},
  year = {2014},
  month = sep,
  journal = {Behavior Modification},
  volume = {38},
  number = {5},
  pages = {665--704},
  publisher = {{SAGE Publications Inc}},
  issn = {0145-4455},
  doi = {10.1177/0145445514535243},
  abstract = {The quantitative methods for analyzing single-subject experimental data have expanded during the last decade, including the use of regression models to statistically analyze the data, but still a lot of questions remain. One question is how to specify predictors in a regression model to account for the specifics of the design and estimate the effect size of interest. These quantitative effect sizes are used in retrospective analyses and allow synthesis of single-subject experimental study results which is informative for evidence-based decision making, research and theory building, and policy discussions. We discuss different design matrices that can be used for the most common single-subject experimental designs (SSEDs), namely, the multiple-baseline designs, reversal designs, and alternating treatment designs, and provide empirical illustrations. The purpose of this article is to guide single-subject experimental data analysts interested in analyzing and meta-analyzing SSED data.},
  langid = {english},
  keywords = {alternating treatment design,design matrix,multiple-baseline design,piecewise regression equation,reversal design,single-subject experimental design},
  file = {/Users/dorothybishop/Zotero/storage/KIH2XW3Y/Moeyaert et al. - 2014 - The Influence of the Design Matrix on Treatment Ef.pdf}
}

@article{moher2010,
  title = {{{CONSORT}} 2010 Explanation and Elaboration: Updated Guidelines for Reporting Parallel Group Randomised Trials},
  shorttitle = {{{CONSORT}} 2010 Explanation and Elaboration},
  author = {Moher, David and Hopewell, Sally and Schulz, Kenneth F. and Montori, Victor and G{\o}tzsche, Peter C. and Devereaux, P. J. and Elbourne, Diana and Egger, Matthias and Altman, Douglas G.},
  year = {2010},
  month = mar,
  journal = {BMJ (Clinical research ed.)},
  volume = {340},
  pages = {c869},
  issn = {1756-1833},
  doi = {10.1136/bmj.c869},
  langid = {english},
  pmcid = {PMC2844943},
  pmid = {20332511},
  keywords = {Consensus,Practice Guidelines as Topic,Randomized Controlled Trials as Topic,Research Design},
  file = {/Users/dorothybishop/Zotero/storage/G2GQ8TCK/Moher et al. - 2010 - CONSORT 2010 explanation and elaboration updated .pdf}
}

@article{mokkink2010,
  title = {The {{COSMIN}} Study Reached International Consensus on Taxonomy, Terminology, and Definitions of Measurement Properties for Health-Related Patient-Reported Outcomes},
  author = {Mokkink, Lidwine B. and Terwee, Caroline B. and Patrick, Donald L. and Alonso, Jordi and Stratford, Paul W. and Knol, Dirk L. and Bouter, Lex M. and {de Vet}, Henrica C. W.},
  year = {2010},
  month = jul,
  journal = {Journal of Clinical Epidemiology},
  volume = {63},
  number = {7},
  pages = {737--745},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2010.02.006},
  abstract = {Objective Lack of consensus on taxonomy, terminology, and definitions has led to confusion about which measurement properties are relevant and which concepts they represent. The aim was to clarify and standardize terminology and definitions of measurement properties by reaching consensus among a group of experts and to develop a taxonomy of measurement properties relevant for evaluating health instruments. Study Design and Setting An international Delphi study with four written rounds was performed. Participating experts had a background in epidemiology, statistics, psychology, and clinical medicine. The panel was asked to rate their (dis)agreement about proposals on a five-point scale. Consensus was considered to be reached when at least 67\% of the panel agreed. Results Of 91 invited experts, 57 agreed to participate and 43 actually participated. Consensus was reached on positions of measurement properties in the taxonomy (68\textendash 84\%), terminology (74\textendash 88\%, except for structural validity [56\%]), and definitions of measurement properties (68\textendash 88\%). The panel extensively discussed the positions of internal consistency and responsiveness in the taxonomy, the terms ``reliability'' and ``structural validity,'' and the definitions of internal consistency and reliability. Conclusions Consensus on taxonomy, terminology, and definitions of measurement properties was reached. Hopefully, this will lead to a more uniform use of terms and definitions in the literature on measurement properties.},
  langid = {english},
  keywords = {Classification,Delphi technique,Outcome assessment,Psychometrics,Quality of life,Questionnaire,Terminology},
  file = {/Users/dorothybishop/Zotero/storage/2FDX4ZX4/Mokkink et al. - 2010 - The COSMIN study reached international consensus o.pdf;/Users/dorothybishop/Zotero/storage/SCEVHV4S/S0895435610000909.html}
}

@article{morris2007,
  title = {Masking Is Better than Blinding},
  author = {Morris, Daniel and Fraser, Scott and Wormald, Richard},
  year = {2007},
  month = apr,
  journal = {BMJ},
  volume = {334},
  number = {7597},
  pages = {799--799},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.39175.503299.94},
  abstract = {{$<$}p{$>$}Why the term ``blinding'' should not be used in clinical trials{$<$}/p{$>$}},
  chapter = {Views \&amp; Reviews},
  copyright = {\textcopyright{} BMJ Publishing Group Ltd 2007},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/SKVZJBKC/Morris et al. - 2007 - Masking is better than blinding.pdf;/Users/dorothybishop/Zotero/storage/634APA8X/799.html}
}

@techreport{nahum-shani2019,
  title = {An {{Introduction}} to {{Adaptive Interventions}} and {{SMART Designs}} in {{Education}}},
  author = {{Nahum-Shani}, Inbal and Almirall, Daniel and Buckley, Jacquelyn},
  year = {2019},
  pages = {45},
  institution = {{US Department of Education: Institute of Education Sciences}},
  abstract = {Education practice often requires teachers and other school personnel to adapt interventions over time in order to address between-student heterogeneity in response to intervention (e.g., what works for one student may not work for the other) or within-student heterogeneity (e.g., what works now may not work in the future for the same student). An adaptive intervention allows education practitioners to do this in a prespecified, systematic, and replicable way through a sequence of decision rules that guides whether, how, and when to modify interventions. In an adaptive intervention, the practitioner modifies the dosage or type of intervention, or the mode of delivery to meet the unique and changing needs of students as they progress over time. The sequential, multiple assignment, randomized trial (SMART) is one type of multistage, experimental design that can help education researchers build high-quality adaptive interventions. Despite the critical role adaptive interventions can play in various domains of education, research about adaptive interventions and about the use of SMART designs to develop effective adaptive interventions in education is in its infancy. This paper defines an adaptive intervention and reviews the components of this design, discusses the key features of the SMART, and introduces common research questions for which SMARTs may be appropriate.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/R87CFJFE/Nahum-Shani et al. - An Introduction to Adaptive Interventions and SMAR.pdf}
}

@article{neuman2011,
  title = {{Educational effects of a vocabulary intervention on preschoolers word knowledge and conceptual development: A cluster-randomized trial}},
  shorttitle = {{Educational effects of a vocabulary intervention on preschoolers word knowledge and conceptual development}},
  author = {Neuman, Susan B. and Newman, Ellen H. and Dwyer, Julie},
  year = {2011},
  month = jul,
  journal = {Reading Research Quarterly},
  volume = {46},
  number = {3},
  pages = {249--272},
  publisher = {{International Reading Association}},
  issn = {0034-0553},
  doi = {10.1598/RRQ.46.3.3},
  langid = {English (US)},
  file = {/Users/dorothybishop/Zotero/storage/KVIKLI5F/educational-effects-of-a-vocabulary-intervention-on-preschoolers-.html}
}

@article{nitido2020,
  title = {Diagnosis of {{Developmental Language Disorder}} in {{Research Studies}}},
  author = {Nitido, Hallie and Plante, Elena},
  year = {2020},
  month = aug,
  journal = {Journal of Speech, Language and Hearing Research (Online)},
  volume = {63},
  number = {8},
  pages = {2777--2788},
  publisher = {{American Speech-Language-Hearing Association}},
  address = {{Rockville, United States}},
  doi = {http://dx.doi.org/10.1044/2020_JSLHR-20-00091},
  abstract = {Purpose: The aim of this study was to investigate the extent to which researchers in the field of developmental language disorder are utilizing validated methods to diagnose their research participants. Method: We examined 90 research articles published from 2015 to 2019 that included English-speaking participants from the United States who were identified as having a developmental language disorder or specific language impairment. From these articles, we identified the tests and measures used to identify participants and classify them as healthy or impaired. We then consulted the test manuals and the literature to find information on sensitivity and specificity of the test and the evidence-based cut score that maximized identification accuracy. Results: Of the 90 articles examined, 38 (42\%) were found to reflect validated diagnostic methods, and 51 (58\%) did not. Conclusion: Our results illustrate that validated methods are used less than half of the time even by those who should have a high level of expertise and despite calls for increasing scientific rigor in research practices.},
  chapter = {Research Note},
  copyright = {Copyright American Speech-Language-Hearing Association Aug 2020},
  langid = {english},
  keywords = {Accuracy,Clinical medicine,Developmental disabilities,Handicapped--Hearing Impaired,Handicapped–Hearing Impaired,Language disorders,Medical Sciences--Otorhinolaryngology,Medical Sciences–Otorhinolaryngology,Quantitative psychology,Research,Specific language impairment,Standard scores},
  file = {/Users/dorothybishop/Zotero/storage/BWZ62FWR/Nitido and Plante - 2020 - Diagnosis of Developmental Language Disorder in Re.pdf}
}

@article{noseworthy1994,
  title = {The Impact of Blinding on the Results of a Randomized, Placebo-controlled Multiple Sclerosis Clinical Trial},
  author = {Noseworthy, J. H. and Ebers, G. C. and Vandervoort, M. K. and Farquhar, R. E. and Yetisir, E. and Roberts, R.},
  year = {1994},
  month = jan,
  journal = {Neurology},
  volume = {44},
  number = {1},
  pages = {16--16},
  publisher = {{Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology}},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.44.1.16},
  abstract = {In the randomized, placebo-controlled, physician-blinded Canadian cooperative trial of cyclophosphamide and plasma exchange, neither active treatment regimens (group I: IV cyclophosphamide and prednisone; group II: weekly plasma exchange, oral cyclophosphamide, and prednisone) were superior to placebo (group III: sham plasma exchange and placebo medications) using the blinded, evaluating neurologists' assessments of disease course (primary analysis). All patients were examined by both a blinded and an unblinded neurologist at each assessment in this trial. We compared the blinded and unblinded neurologists' judgment of treatment response and analyzed the clinical behavior of patients who correctly guessed their treatment. The unblinded (but not the blinded) neurologists' scores demonstrated an apparent treatment benefit at 6, 12, and 24 months for the group II patients (not group I or placebo; p {$<$}0.05, two-tailed). There were no significant differences in the time to treatment failure or in the proportions of patients improved, stable, or worse between the group II and group III patients who correctly guessed their treatment assignments and those who did not. Physician blinding prevented an erroneous conclusion about treatment efficacy (false positive, type 1 error).},
  chapter = {Articles},
  copyright = {\textcopyright{} 1994 by the American Academy of Neurology},
  langid = {english},
  pmid = {8290055},
  file = {/Users/dorothybishop/Zotero/storage/Y6TZ73H9/16.html}
}

@article{oconnell2017,
  title = {Methods for {{Analysis}} of {{Pre-Post Data}} in {{Clinical Research}}: {{A Comparison}} of {{Five Common Methods}}},
  shorttitle = {Methods for {{Analysis}} of {{Pre-Post Data}} in {{Clinical Research}}},
  author = {O'Connell, Nathaniel S. and Dai, Lin and Jiang, Yunyun and Speiser, Jaime L. and Ward, Ralph and Wei, Wei and Carroll, Rachel and Gebregziabher, Mulugeta},
  year = {2017},
  month = feb,
  journal = {Journal of biometrics \& biostatistics},
  volume = {8},
  number = {1},
  pages = {1--8},
  issn = {2155-6180},
  doi = {10.4172/2155-6180.1000334},
  abstract = {Often repeated measures data are summarized into pre-post-treatment measurements. Various methods exist in the literature for estimating and testing treatment effect, including ANOVA, analysis of covariance (ANCOVA), and linear mixed modeling (LMM). Under the first two methods, outcomes can either be modeled as the post treatment measurement (ANOVA-POST or ANCOVA-POST), or a change score between pre and post measurements (ANOVA-CHANGE, ANCOVA-CHANGE). In LMM, the outcome is modeled as a vector of responses with or without Kenward-Rogers adjustment. We consider five methods common in the literature, and discuss them in terms of supporting simulations and theoretical derivations of variance. Consistent with existing literature, our results demonstrate that each method leads to unbiased treatment effect estimates, and based on precision of estimates, 95\% coverage probability, and power, ANCOVA modeling of either change scores or post-treatment score as the outcome, prove to be the most effective. We further demonstrate each method in terms of a real data example to exemplify comparisons in real clinical context.},
  pmcid = {PMC6290914},
  pmid = {30555734},
  file = {/Users/dorothybishop/Zotero/storage/5ZJBHGPB/O'Connell et al. - 2017 - Methods for Analysis of Pre-Post Data in Clinical .pdf}
}

@article{ogieladianea.2021,
  title = {Norm-{{Referenced Language Test Selection Practices}} for {{Elementary School Children With Suspected Developmental Language Disorder}}},
  author = {{Ogiela Diane A.} and {Montzka Jennifer L.}},
  year = {2021},
  month = jan,
  journal = {Language, Speech, and Hearing Services in Schools},
  volume = {52},
  number = {1},
  pages = {288--303},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/2020_LSHSS-19-00067},
  abstract = {Purpose       Standardized norm-referenced tests are an important aspect of language assessment          for school-age children. This study explored the language test selection practices          of school-based speech-language pathologists (SLPs) working with elementary school          children suspected of having developmental language disorder. Specifically, we investigated          which tests were most commonly selected as clinicians' first-choice and follow-up          tests, which factors impacted their test selection decisions, and what sources of          information they used to determine the psychometric quality of tests.              Method       School-based SLPs completed a web-based questionnaire regarding their use of norm-referenced          language tests. A total of 370 elementary school SLPs completed the questionnaire.              Results       The vast majority of participants indicated that omnibus language tests are their          first choice of test. For follow-up tests, participants selected semantics tests,          especially single-word vocabulary tests, significantly more often than tests of pragmatics,          processing skills, and morphology/syntax. Participants identified multiple factors          as affecting test selection, including availability, familiarity, psychometric features,          and others. Although more SLPs reported using data-based than subjective sources of          information to judge the psychometric quality of tests, a substantial proportion reported          that they relied on subjective sources.              Conclusions       Clinicians have a strong preference for using omnibus language tests. Follow-up test          selection does not appear to align with the language difficulties most associated          with developmental language disorder. The substantial use of subjective information          about psychometric qualities of tests suggests that many SLPs may not attend to the          technical meanings of terms such as validity, reliability, and diagnostic accuracy.          These results indicate a need for improvement in evidence-based language assessment          practices.              Supplemental Material       https://doi.org/10.23641/asha.13022471},
  file = {/Users/dorothybishop/Zotero/storage/G5MN3G7M/Ogiela Diane A. and Montzka Jennifer L. - 2021 - Norm-Referenced Language Test Selection Practices .pdf;/Users/dorothybishop/Zotero/storage/LN9GZYGB/2020_LSHSS-19-00067.html}
}

@techreport{oxman2022,
  title = {Key {{Concepts}} for Assessing Claims about Treatment Effects and Making Well-Informed Treatment Choices ({{Version}} 2022)},
  author = {Oxman, Andrew D. and Chalmers, Iain and Dahlgren, Astrid},
  year = {2022},
  month = jun,
  institution = {{Zenodo}},
  doi = {10.5281/zenodo.6611932},
  abstract = {There are endless claims about treatments in the mass media, advertisements, and everyday personal communication. Some are true and some are false. Many are unsubstantiated: we do not know whether they are true or false. Unsubstantiated claims about the effects of treatments often turn out to be wrong. Consequently, people who believe and act on these claims suffer unnecessarily and waste resources by doing things that do not help and might be harmful, and by not doing things that do help. In response to these challenges, we developed the Informed Health Choices Key Concepts as the first step in the Informed Health Choices (IHC) project, an initiative supported by the Research Council of Norway. The aim of the IHC project and ongoing work by the IHC Network is to help people make informed health choices. In this document, we use the term ``treatment'' to include any intervention (action) intended to improve health, including preventive, therapeutic and rehabilitative interventions, and public health or health system interventions. Although we have developed and framed the Key Concepts to address treatment claims, people in other disciplines may find them relevant; for example, for assessing claims about the effects of educational interventions or environmental measures. The Informed Health Choices (IHC) Key Concepts serve as the basis for developing learning resources to help people understand and apply the concepts when claims about the effects of treatments (and other interventions) are made, and when they make health choices. They are also the basis for an item bank of multiple-choice questions (the Claim Evaluation Tools item bank) that can be used for assessing people's ability to apply the IHC Key Concepts. The concepts are principles for evaluating the trustworthiness of treatment claims, comparisons, and choices. The concepts can help people to: Recognise when a claim about the effects of treatments has an untrustworthy basis Recognise when evidence from comparisons of treatments is trustworthy and when it is not Make well-informed choices about treatments They can help anyone, not just researchers, to think critically about whether to believe a treatment claim and what to do. This is sometimes referred to as critical health literacy. The Key Concepts are intended for people using research, not for doing research. In this update, we started with the explanations and implications from the 2019 version of the IHC Key Concepts. For each concept, we have provided one or more examples to illustrate each explanation, and the basis for each concept, drawing on relevant research that informed the development of the IHC Key Concepts. We have received only a few suggestions since the 2019 version was published and did not publish a new version of the Key Concepts in 2020 or 2021. We have decided that the version presented here will be the last revision made by us. This does not mean that this list of concepts cannot be further improved, but we will leave any further development of the IHC Key Concepts to others.},
  langid = {english},
  keywords = {concepts; critical thinking; critical appraisal;  critical health literacy; causal inference; treatment claims; informed decision making; epistemology},
  file = {/Users/dorothybishop/Zotero/storage/IDZSTNCD/Oxman et al. - 2022 - Key Concepts for assessing claims about treatment .pdf}
}

@article{pallmann2018,
  title = {Adaptive Designs in Clinical Trials: Why Use Them, and How to Run and Report Them},
  shorttitle = {Adaptive Designs in Clinical Trials},
  author = {Pallmann, Philip and Bedding, Alun W. and {Choodari-Oskooei}, Babak and Dimairo, Munyaradzi and Flight, Laura and Hampson, Lisa V. and Holmes, Jane and Mander, Adrian P. and Odondi, Lang'o and Sydes, Matthew R. and Villar, Sof{\'i}a S. and Wason, James M. S. and Weir, Christopher J. and Wheeler, Graham M. and Yap, Christina and Jaki, Thomas},
  year = {2018},
  month = feb,
  journal = {BMC Medicine},
  volume = {16},
  number = {1},
  pages = {29},
  issn = {1741-7015},
  doi = {10.1186/s12916-018-1017-7},
  abstract = {Adaptive designs can make clinical trials more flexible by utilising results accumulating in the trial to modify the trial's course in accordance with pre-specified rules. Trials with an adaptive design are often more efficient, informative and ethical than trials with a traditional fixed design since they often make better use of resources such as time and money, and might require fewer participants. Adaptive designs can be applied across all phases of clinical research, from early-phase dose escalation to confirmatory trials. The pace of the uptake of adaptive designs in clinical research, however, has remained well behind that of the statistical literature introducing new methods and highlighting their potential advantages. We speculate that one factor contributing to this is that the full range of adaptations available to trial designs, as well as their goals, advantages and limitations, remains unfamiliar to many parts of the clinical community. Additionally, the term adaptive design has been misleadingly used as an all-encompassing label to refer to certain methods that could be deemed controversial or that have been inadequately implemented.},
  keywords = {Adaptive design,Design modification,Flexible design,Interim analysis,Seamless design,Statistical methods},
  file = {/Users/dorothybishop/Zotero/storage/LMB6U8GL/Pallmann et al. - 2018 - Adaptive designs in clinical trials why use them,.pdf;/Users/dorothybishop/Zotero/storage/BTBTXBMI/s12916-018-1017-7.html}
}

@article{parmar2016,
  title = {How Do You Design Randomised Trials for Smaller Populations? {{A}} Framework},
  shorttitle = {How Do You Design Randomised Trials for Smaller Populations?},
  author = {Parmar, Mahesh K. B. and Sydes, Matthew R. and Morris, Tim P.},
  year = {2016},
  month = nov,
  journal = {BMC Medicine},
  volume = {14},
  number = {1},
  pages = {183},
  issn = {1741-7015},
  doi = {10.1186/s12916-016-0722-3},
  abstract = {How should we approach trial design when we can get some, but not all, of the way to the numbers required for a randomised phase III trial?},
  keywords = {Randomised trials,Smaller populations,Trial design,Uncommon diseases},
  file = {/Users/dorothybishop/Zotero/storage/INQZA6QY/Parmar et al. - 2016 - How do you design randomised trials for smaller po.pdf;/Users/dorothybishop/Zotero/storage/CVHSW3VJ/s12916-016-0722-3.html}
}

@article{patsopoulos2011,
  title = {A Pragmatic View on Pragmatic Trials},
  author = {Patsopoulos, Nikolaos A.},
  year = {2011},
  month = jun,
  journal = {Dialogues in Clinical Neuroscience},
  volume = {13},
  number = {2},
  pages = {217--224},
  issn = {1294-8322},
  abstract = {Clinical trials have been the main tool used by the health sciences community to test and evaluate interventions. Trials can fall into two broad categories: pragmatic and explanatory. Pragmatic trials are designed to evaluate the effectiveness of interventions in real-life routine practice conditions, whereas explanatory trials aim to test whether an intervention works under optimal situations. Pragmatic trials produce results that can be generalized and applied in routine practice settings. Since most results from exploratory trials fail to be broadly generalizable, the ``pragmatic design'' has gained momentum. This review describes the concept of pragmatism, and explains in particular that there is a continuum between pragmatic and explanatory trials, rather than a dichotomy. Special focus is put on the limitations of the pragmatic trials, while recognizing the importance for and impact of this design on medical practice.},
  pmcid = {PMC3181997},
  pmid = {21842619},
  file = {/Users/dorothybishop/Zotero/storage/CE7YNSBC/Patsopoulos - 2011 - A pragmatic view on pragmatic trials.pdf}
}

@article{perdices2009,
  title = {Single-Subject Designs as a Tool for Evidence-Based Clinical Practice: {{Are}} They Unrecognised and Undervalued?},
  shorttitle = {Single-Subject Designs as a Tool for Evidence-Based Clinical Practice},
  author = {Perdices, Michael and Tate, Robyn L.},
  year = {2009},
  month = dec,
  journal = {Neuropsychological Rehabilitation},
  volume = {19},
  number = {6},
  pages = {904--927},
  publisher = {{Routledge}},
  issn = {0960-2011},
  doi = {10.1080/09602010903040691},
  abstract = {One could be forgiven for thinking that the only road to evidence-based clinical practice is the application of results from randomised controlled trials (or systematic reviews of such). By contrast, single-subject designs in the context of evidence-based clinical practice are believed by many to be strange bedfellows. In this paper, we argue that single-subject designs play an important role in evidence-based clinical practice. We survey the contents of Neuropsychological Rehabilitation in relation to single-subject designs and tackle the main criticisms that have been levelled against them. We offer practical guidance for rating the methodological quality of single-subject designs and applying statistical techniques to measure treatment efficacy. These guides are equally applicable to research studies and everyday clinical practice with individual patients.},
  pmid = {19657974},
  keywords = {Corrigendum,Methodology,N-of-1 trials,Single-case experimental designs},
  annotation = {\_eprint: https://doi.org/10.1080/09602010903040691},
  file = {/Users/dorothybishop/Zotero/storage/7HWKAVGX/Perdices and Tate - 2009 - Single-subject designs as a tool for evidence-base.pdf;/Users/dorothybishop/Zotero/storage/9NYTR32I/09602010903040691.html}
}

@article{pico,
  title = {Interventions {{Designed}} to {{Improve Narrative Language}} in {{School-Age Children}}: {{A Systematic Review With Meta-Analyses}}},
  shorttitle = {Interventions {{Designed}} to {{Improve Narrative Language}} in {{School-Age Children}}},
  author = {Pico, Danielle L. and Hessling, Prahl Alison and Biel, Christa Haring and Peterson, Amy K. and Biel, Eric J. and Woods, Christine and Contesse, Valentina A.},
  journal = {Language, Speech, and Hearing Services in Schools},
  publisher = {{American Speech-Language-Hearing Association}},
  doi = {10.1044/2021_LSHSS-20-00160},
  abstract = {Purpose The purpose of this systematic review with meta-analyses was to examine interventions that aimed to improve narrative language outcomes for preschool and elementary school\textendash age children in the United States. Our goal was to examine peer-reviewed publications to describe the characteristics of these interventions and synthesize their overall effectiveness on narrative comprehension and production via meta-analysis. Method We searched electronic databases, examined previously published reviews, and consulted experts in the field to identify published studies that employed robust experimental and quasi-experimental designs. We included randomized controlled trials, studies with nonrandomized comparison groups, and single-case design (SCD) studies. We completed a qualitative synthesis of study factors for all identified studies and calculated meta-analyses for the studies that had sufficient data. All included studies were analyzed for risk of bias. Results Our systematic search yielded 40 studies that included one or more narrative language outcomes as part of their assessment battery. Twenty-four of the included studies were group design studies, including randomized controlled trials and quasi-experimental designs, and the other 16 were SCD studies. Effect sizes were analyzed based on narrative production and comprehension outcomes. The meta-analyses of 26 studies indicated overall positive effects of the interventions, with effect sizes of d = 0.51 and 0.54 in the group design studies and d = 1.24 in the SCD studies. Conclusions A variety of effective interventions were found that improve narrative production and comprehension outcomes in children with diverse learner characteristics. Some common characteristics across these interventions include manualized curricula, opportunities to produce narrative language, verbal and visual supports, direct instruction of story grammar, and use of authentic children's literature. Supplemental Material https://doi.org/10.23641/asha.15079173},
  file = {/Users/dorothybishop/Zotero/storage/33PE8ZAK/Pico et al. - Interventions Designed to Improve Narrative Langua.pdf}
}

@article{pirosca2022,
  title = {Tolerating Bad Health Research: The Continuing Scandal},
  shorttitle = {Tolerating Bad Health Research},
  author = {Pirosca, Stefania and Shiely, Frances and Clarke, Mike and Treweek, Shaun},
  year = {2022},
  month = jun,
  journal = {Trials},
  volume = {23},
  number = {1},
  pages = {458},
  issn = {1745-6215},
  doi = {10.1186/s13063-022-06415-5},
  abstract = {At the 2015 REWARD/EQUATOR conference on research waste, the late Doug Altman revealed that his only regret about his 1994 BMJ paper `The scandal of poor medical research' was that he used the word `poor' rather than `bad'. But how much research is bad? And what would improve things?},
  keywords = {Methodologists,Randomised trials,Research waste,Risk of bias,Statisticians},
  file = {/Users/dorothybishop/Zotero/storage/7S72RBLV/Pirosca et al. - 2022 - Tolerating bad health research the continuing sca.pdf}
}

@article{pocock1975,
  title = {Sequential Treatment Assignment with Balancing for Prognostic Factors in the Controlled Clinical Trial},
  author = {Pocock, S. J. and Simon, R.},
  year = {1975},
  month = mar,
  journal = {Biometrics},
  volume = {31},
  number = {1},
  pages = {103--115},
  issn = {0006-341X},
  abstract = {In controlled clinical trials there are usually several prognostic factors known or thought to influence the patient's ability to respond to treatment. Therefore, the method of sequential treatment assignment needs to be designed so that treatment balance is simultaneously achieved across all such patients factor. Traditional methods of restricted randomization such as "permuted blocks within strata" prove inadequate once the number of strata, or combinations of factor levels, approaches the sample size. A new general procedure for treatment assignment is described which concentrates on minimizing imbalance in the distributions of treatment numbers within the levels of each individual prognostic factor. The improved treatment balance obtained by this approach is explored using simulation for a simple model of a clinical trial. Further discussion centers on the selection, predictability and practicability of such a procedure.},
  langid = {english},
  pmid = {1100130},
  keywords = {Carmustine,Clinical Trials as Topic,Cyclophosphamide,Evaluation Studies as Topic,Humans,Lymphoma,Models,Models; Theoretical,Prednisone,Prognosis,Statistics as Topic,Theoretical,Vincristine}
}

@article{porzsolt2015,
  title = {Efficacy and Effectiveness Trials Have Different Goals, Use Different Tools, and Generate Different Messages},
  author = {Porzsolt, Franz and Rocha, Nat{\'a}lia Galito and {Toledo-Arruda}, Alessandra C and Thomaz, Tania G and Moraes, Cristiane and {Bessa-Guerra}, Thais R and Le{\~a}o, Mauricio and Migowski, Arn and {Araujo da Silva}, Andr{\'e} R and Weiss, Christel},
  year = {2015},
  month = nov,
  journal = {Pragmatic and Observational Research},
  volume = {6},
  pages = {47--54},
  issn = {1179-7266},
  doi = {10.2147/POR.S89946},
  abstract = {The discussion about the optimal design of clinical trials reflects the perspectives of theory-based scientists and practice-based clinicians. Scientists compare the theory with published results. They observe a continuum from explanatory to pragmatic trials. Clinicians compare the problem they want to solve by completing a clinical trial with the results they can read in the literature. They observe a mixture of what they want and what they get. None of them can solve the problem without the support of the other. Here, we summarize the results of discussions with scientists and clinicians. All participants were interested to understand and analyze the arguments of the other side. As a result of this process, we conclude that scientists tell what they see, a continuum from clear explanatory to clear pragmatic trials. Clinicians tell what they want to see, a clear explanatory trial to describe the expected effects under ideal study conditions and a clear pragmatic trial to describe the observed effects under real-world conditions. Following this discussion, the solution was not too difficult. When we accept what we see, we will not get what we want. If we discuss a necessary change of management, we will end up with the conclusion that two types of studies are necessary to demonstrate efficacy and effectiveness. Efficacy can be demonstrated in an explanatory, ie, a randomized controlled trial (RCT) completed under ideal study conditions. Effectiveness can be demonstrated in an observational, ie, a pragmatic controlled trial (PCT) completed under real-world conditions. It is impossible to design a trial which can detect efficacy and effectiveness simultaneously. The RCTs describe what we may expect in health care, while the PCTs describe what we really observe.},
  pmcid = {PMC5045025},
  pmid = {27774032},
  file = {/Users/dorothybishop/Zotero/storage/9R4NJAW3/Porzsolt et al. - 2015 - Efficacy and effectiveness trials have different g.pdf}
}

@article{pustejovsky2019,
  title = {An {{Examination}} of {{Measurement Procedures}} and {{Characteristics}} of {{Baseline Outcome Data}} in {{Single-Case Research}}},
  author = {Pustejovsky, James E. and Swan, Daniel M. and English, Kyle W.},
  year = {2019},
  month = aug,
  journal = {Behavior Modification},
  pages = {0145445519864264},
  publisher = {{SAGE Publications Inc}},
  issn = {0145-4455},
  doi = {10.1177/0145445519864264},
  abstract = {There has been growing interest in using statistical methods to analyze data and estimate effect size indices from studies that use single-case designs (SCDs), as a complement to traditional visual inspection methods. The validity of a statistical method rests on whether its assumptions are plausible representations of the process by which the data were collected, yet there is evidence that some assumptions\textemdash particularly regarding normality of error distributions\textemdash may be inappropriate for single-case data. To develop more appropriate modeling assumptions and statistical methods, researchers must attend to the features of real SCD data. In this study, we examine several features of SCDs with behavioral outcome measures in order to inform development of statistical methods. Drawing on a corpus of over 300 studies, including approximately 1,800 cases, from seven systematic reviews that cover a range of interventions and outcome constructs, we report the distribution of study designs, distribution of outcome measurement procedures, and features of baseline outcome data distributions for the most common types of measurements used in single-case research. We discuss implications for the development of more realistic assumptions regarding outcome distributions in SCD studies, as well as the design of Monte Carlo simulation studies evaluating the performance of statistical analysis techniques for SCD data.},
  langid = {english},
  keywords = {alternating renewal process,behavioral observation,single-case research,systematic review},
  file = {/Users/dorothybishop/Zotero/storage/JTJDRGQK/Pustejovsky et al. - 2019 - An Examination of Measurement Procedures and Chara.pdf}
}

@article{quintana2018,
  title = {Bayesian Alternatives for Common Null-Hypothesis Significance Tests in Psychiatry: A Non-Technical Guide Using {{JASP}}},
  shorttitle = {Bayesian Alternatives for Common Null-Hypothesis Significance Tests in Psychiatry},
  author = {Quintana, Daniel S. and Williams, Donald R.},
  year = {2018},
  month = jun,
  journal = {BMC Psychiatry},
  volume = {18},
  number = {1},
  pages = {178},
  issn = {1471-244X},
  doi = {10.1186/s12888-018-1761-4},
  abstract = {Despite its popularity as an inferential framework, classical null hypothesis significance testing (NHST) has several restrictions. Bayesian analysis can be used to complement NHST, however, this approach has been underutilized largely due to a dearth of accessible software options. JASP is a recently developed open-source statistical package that facilitates both Bayesian and NHST analysis using a graphical interface. This article provides an applied introduction to Bayesian inference with Bayes factors using JASP.},
  keywords = {Bayesian analysis,Null hypothesis significance testing,p-values,Research methods,Software,Statistics},
  file = {/Users/dorothybishop/Zotero/storage/SQZJKRRZ/Quintana and Williams - 2018 - Bayesian alternatives for common null-hypothesis s.pdf;/Users/dorothybishop/Zotero/storage/M3R667WF/s12888-018-1761-4.html}
}

@article{rabbitt2004,
  title = {Practice and Drop-out Effects during a 17-Year Longitudinal Study of Cognitive Aging},
  author = {Rabbitt, Patrick and Diggle, Peter and Holland, Fiona and McInnes, Lynn},
  year = {2004},
  month = mar,
  journal = {The Journals of Gerontology. Series B, Psychological Sciences and Social Sciences},
  volume = {59},
  number = {2},
  pages = {P84-97},
  issn = {1079-5014},
  doi = {10.1093/geronb/59.2.p84},
  abstract = {Interpretations of longitudinal studies of cognitive aging are misleading unless effects of practice and selective drop-out are considered. A random effects model taking practice and drop-out into account analyzed data from four successive presentations of each of two intelligence tests, two vocabulary tests, and two verbal memory tests during a 17-year longitudinal study of 5,899 community residents whose ages ranged from 49 to 92 years. On intelligence tests, substantial practice effects counteracted true declines observed over 3 to 5 years of aging and remained significant even with intervals of 7 years between successive assessments. Adjustment for practice and drop-out revealed accelerating declines in fluid intelligence and cumulative learning, linear declines in verbal free recall, and no substantial change in vocabulary. Socioeconomic status and basal levels of general fluid ability did not affect rates of decline. After further adjustment for demographics, variability between individuals was seen to increase as the sample aged.},
  langid = {english},
  pmid = {15014091},
  keywords = {80 and over,Aged,Aged; 80 and over,Aging,Bias,Cognition,Cohort Studies,Female,Humans,Intelligence,Learning,Longitudinal Studies,Male,Mental Recall,Middle Aged,Models,Models; Statistical,Neuropsychological Tests,Patient Dropouts,Practice,Practice; Psychological,Psychological,Statistical,Verbal Learning,Vocabulary,Wechsler Scales},
  file = {/Users/dorothybishop/Zotero/storage/3WU4NKFT/Rabbitt et al. - 2004 - Practice and drop-out effects during a 17-year lon.pdf}
}

@article{ratner2016,
  title = {Your {{Laptop}} to the {{Rescue}}: {{Using}} the {{Child Language Data Exchange System Archive}} and {{CLAN Utilities}} to {{Improve Child Language Sample Analysis}}},
  shorttitle = {Your {{Laptop}} to the {{Rescue}}},
  author = {Ratner, Nan Bernstein and MacWhinney, Brian},
  year = {2016},
  month = may,
  journal = {Seminars in Speech and Language},
  volume = {37},
  number = {2},
  pages = {74--84},
  issn = {1098-9056},
  doi = {10.1055/s-0036-1580742},
  abstract = {In this article, we review the advantages of language sample analysis (LSA) and explain how clinicians can make the process of LSA faster, easier, more accurate, and more insightful than LSA done "by hand" by using free, available software programs such as Computerized Language Analysis (CLAN). We demonstrate the utility of CLAN analysis in studying the expressive language of a very large cohort of 24-month-old toddlers tracked in a recent longitudinal study; toddlers in particular are the most likely group to receive LSA by clinicians, but existing reference "norms" for this population are based on fairly small cohorts of children. Finally, we demonstrate how a CLAN utility such as KidEval can now extract potential normative data from the very large number of corpora now available for English and other languages at the Child Language Data Exchange System project site. Most of the LSA measures that we studied appear to show developmental profiles suggesting that they may be of specifically higher value for children at certain ages, because they do not show an even developmental trajectory from 2 to 7 years of age.},
  langid = {english},
  pmid = {27111268},
  keywords = {Child,Child Language,Humans,Information Dissemination,Language,Longitudinal Studies}
}

@article{reise2005,
  title = {Item {{Response Theory}}: {{Fundamentals}}, {{Applications}}, and {{Promise}} in {{Psychological Research}}},
  shorttitle = {Item {{Response Theory}}},
  author = {Reise, Steven P. and Ainsworth, Andrew T. and Haviland, Mark G.},
  year = {2005},
  month = apr,
  journal = {Current Directions in Psychological Science},
  volume = {14},
  number = {2},
  pages = {95--101},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1111/j.0963-7214.2005.00342.x},
  abstract = {Item response theory (IRT) is an increasingly popular approach to the development, evaluation, and administration of psychological measures. We introduce, first, three IRT fundamentals: (a) item response functions, (b) information functions, and (c) invariance. We next illustrate how IRT modeling can improve the quality of psychological measurement. Available evidence suggests that the differences between IRT and traditional psychometric methods are not trivial; IRT applications can improve the precision and validity of psychological research across a wide range of subjects.},
  langid = {english},
  keywords = {classical test theory,item response theory,psychometrics,scaling},
  file = {/Users/dorothybishop/Zotero/storage/CH5GV4Q6/Reise et al. - 2005 - Item Response Theory Fundamentals, Applications, .pdf}
}

@book{renfrew1967,
  title = {Action {{Picture Test}}},
  author = {Renfrew, Catherine},
  year = {1967},
  publisher = {{Catherine Renfrew}},
  address = {{Oxford}},
  abstract = {*The Renfrew Action Picture Test cards are now available for free, to assist with online assessments and the ease of administrating the test, but in order to fully score the RAPT the pack will need to be purchased. You can find the downloadable cards under `Support Materials' on the Routledge.com product page* Since its first publication in 1967, the Renfrew Action Picture Test has been a reach-for assessment used by a range of professionals dedicated to the speech and language development},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/FUPVGH4D/9781138586208.html}
}

@book{renfrew2010,
  title = {Bus {{Story Test}}: {{Revised Edition}}},
  shorttitle = {Bus {{Story Test}}},
  author = {Renfrew, Catherine},
  year = {2010},
  publisher = {{Routledge}},
  abstract = {The age level of consecutive speech used in retelling a story can be assessed from the information content, sentence length and grammatical usage of this revised test. The test includes a coloured picture story book, a scoring form to photocopy and a manual, but also requires the use of audio recording equipment. Catherine Renfrew's three tests have been used for many years and provide a means of assessing children's speech and language. All tests are suitable for use with 3-8 year olds are norm},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/WXQ7ZTHE/9780863888083.html}
}

@article{robinson2011,
  title = {A {{Systematic Examination}} of the {{Citation}} of {{Prior Research}} in {{Reports}} of {{Randomized}}, {{Controlled Trials}}},
  author = {Robinson, Karen A. and Goodman, Steven N.},
  year = {2011},
  month = jan,
  journal = {Annals of Internal Medicine},
  volume = {154},
  number = {1},
  pages = {50--55},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-154-1-201101040-00007},
  abstract = {In reports of RCTs published over 4 decades, fewer than 25\% of preceding trials were cited, comprising fewer than 25\% of the participants enrolled in all relevant prior trials. A median of 2 trials was cited, regardless of the number of prior trials that had been conducted. Research is needed to explore the explanations for and consequences of this phenomenon. Potential implications include ethically unjustifiable trials, wasted resources, incorrect conclusions, and unnecessary risks for trial participants.},
  file = {/Users/dorothybishop/Zotero/storage/NAWJBHN3/Robinson and Goodman - 2011 - A Systematic Examination of the Citation of Prior .pdf}
}

@article{rogde2019,
  title = {The Effect of Linguistic Comprehension Instruction on Generalized Language and Reading Comprehension Skills: {{A}} Systematic Review},
  shorttitle = {The Effect of Linguistic Comprehension Instruction on Generalized Language and Reading Comprehension Skills},
  author = {Rogde, Kristin and Hagen, {\AA}ste M. and {Melby-Lerv{\aa}g}, Monica and Lerv{\aa}g, Arne},
  year = {2019},
  journal = {Campbell Systematic Reviews},
  volume = {15},
  number = {4},
  pages = {e1059},
  issn = {1891-1803},
  doi = {10.1002/cl2.1059},
  copyright = {\textcopyright{} 2019 The Authors. Campbell Systematic Reviews published by John Wiley \& Sons Ltd on behalf of The Campbell Collaboration.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cl2.1059},
  file = {/Users/dorothybishop/Zotero/storage/2Y4K3U46/Rogde et al. - 2019 - The effect of linguistic comprehension instruction.pdf;/Users/dorothybishop/Zotero/storage/7NKTY7M4/cl2.html}
}

@article{rosenthal1963,
  title = {The Effect of Experimenter Bias on the Performance of the Albino Rat},
  author = {Rosenthal, Robert and Fode, Kermit L.},
  year = {1963},
  journal = {Behavioral Science},
  volume = {8},
  number = {3},
  pages = {183--189},
  issn = {00057940, 10991743},
  doi = {10.1002/bs.3830080302},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/TNLR2ETH/Rosenthal and Fode - 2007 - The effect of experimenter bias on the performance.pdf}
}

@article{rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors},
  file = {/Users/dorothybishop/Zotero/storage/8C86KKCC/1979-27602-001.html;/Users/dorothybishop/Zotero/storage/ME9HEVTA/1979-27602-001.html}
}

@article{rothwell2005,
  title = {External Validity of Randomised Controlled Trials: ``{{To}} Whom Do the Results of This Trial Apply?''},
  shorttitle = {External Validity of Randomised Controlled Trials},
  author = {Rothwell, Peter M},
  year = {2005},
  month = jan,
  journal = {The Lancet},
  volume = {365},
  number = {9453},
  pages = {82--93},
  issn = {0140-6736},
  doi = {10.1016/S0140-6736(04)17670-8},
  abstract = {In making treatment decisions, doctors and patients must take into account relevant randomised controlled trials (RCTs) and systematic reviews. Relevance depends on external validity (or generalisability)\textemdash ie, whether the results can be reasonably applied to a definable group of patients in a particular clinical setting in routine practice. There is concern among clinicians that external validity is often poor, particularly for some pharmaceutical industry trials, a perception that has led to underuse of treatments that are effective. Yet researchers, funding agencies, ethics committees, the pharmaceutical industry, medical journals, and governmental regulators alike all neglect external validity, leaving clinicians to make judgments. However, reporting of the determinants of external validity in trial publications and systematic reviews is usually inadequate. This review discusses those determinants, presents a checklist for clinicians, and makes recommendations for greater consideration of external validity in the design and reporting of RCTs.},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/QZXHVGQ4/Rothwell - 2005 - External validity of randomised controlled trials.pdf;/Users/dorothybishop/Zotero/storage/NYP2KIN8/S0140673604176708.html}
}

@article{saul2021,
  title = {A {{Randomized Case Series Approach}} to {{Testing Efficacy}} of {{Interventions}} for {{Minimally Verbal Autistic Children}}},
  author = {Saul, Jo and Norbury, Courtenay},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {12},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.621920},
  abstract = {Background: Randomized Controlled Trials (RCTs) are the gold standard for assessing whether an intervention is effective; however, they require large sample sizes in order to detect small effects. For rare or complex populations, we advocate a case series approach as a more realistic and useful first step for intervention evaluation. We consider the importance of randomization to such designs, and advocate for the use of Randomization Tests and Between Case Effect Sizes to provide a robust and statistically powerful evaluation of outcomes. In this tutorial, we describe the method, procedures, and analysis code necessary to conduct robust single case series, using an empirical example with minimally verbal autistic children. Method: We applied a pre-registered (https://osf.io/9gvbs) randomized baseline design with between-case effect size to a case series (n=19), to test the efficacy of a novel, parent-mediated, app-based speech production intervention (BabbleBooster) for minimally verbal autistic children. Parent-rated probe scores were used to densely sample performance accuracy over time. Results: Parents were able to reliably code their children's speech productions using BabbleBooster. A non-significant Randomization Test and small Between-Case Effect Size (d=0.267), suggested there was no evidence that BabbleBooster improved speech production in minimally verbal autistic children, relative to baseline scores, during this brief period of intervention. Conclusion: The current analyses exemplify a more robust approach to examining treatment effects in rare or complex populations, where RCT may be difficult or premature to implement. To facilitate adoption of this method by researchers and practitioners, we provide analysis code that can be adapted using open source R packages. Future studies could use this case series design to evaluate interventions aiming to improve speech and language outcomes for minimally verbal autistic children, and other heterogeneous and hard to reach populations.},
  langid = {english},
  keywords = {autism,intervention,minimally verbal,Parent-mediated,Randomization,Single case design,Speech},
  file = {/Users/dorothybishop/Zotero/storage/LVSWD8YD/Saul and Norbury - 2021 - A Randomized Case Series Approach to Testing Effic.pdf}
}

@article{savenkov2015,
  title = {Testing for {{Efficacy}} in {{Single-Subject Trials}} with {{Intervention Analysis}}},
  author = {Savenkov, A. and Wu, S. and Neal, D.},
  year = {2015},
  month = jun,
  journal = {arXiv:1403.4309 [stat]},
  eprint = {1403.4309},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Single subject or n-of-1 research designs have been widely used to evaluate treatment interventions. Many statistical procedures such as split-middle trend lines, regression trend line, Shewart-chart trend line, binomial tests, randomization tests and Tryon C-statistics have been used to analyze single-subject data, but they fail to control Type I error due to serially-dependent time-series observations. The interrupted time series analysis maintains Type I error but assumes that the intervention effect to be a linear trend change from baseline. In this paper, we consider an improved intervention analysis model (Box and Tiao, 1975) for dynamic characteristics of an intervention effect in a short series of single-subject data. The maximum likelihood estimates are derived and a hypothesis testing procedure is proposed. The method is illustrated with a real clinical trial on constraint induced language therapy for aphasia patients.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/Users/dorothybishop/Zotero/storage/2IRS7R54/1403.html}
}

@article{schmitt2017,
  title = {Establishing {{Language Benchmarks}} for {{Children With Typically Developing Language}} and {{Children With Language Impairment}}},
  author = {Schmitt, Mary Beth and Logan, Jessica A. R. and Tambyraja, Sherine R. and Farquharson, Kelly and Justice, Laura M.},
  year = {2017},
  month = feb,
  journal = {Journal of speech, language, and hearing research: JSLHR},
  volume = {60},
  number = {2},
  pages = {364--378},
  issn = {1558-9102},
  doi = {10.1044/2016_JSLHR-L-15-0273},
  abstract = {Purpose: Practitioners, researchers, and policymakers (i.e., stakeholders) have vested interests in children's language growth yet currently do not have empirically driven methods for measuring such outcomes. The present study established language benchmarks for children with typically developing language (TDL) and children with language impairment (LI) from 3 to 9 years of age. Method: Effect sizes for grammar, vocabulary, and overall language were calculated for children with TDL (n = 20,018) using raw score means and standard deviations from 8 norm-referenced measures of language. Effect sizes for children with LI were calculated using fall and spring norm-referenced language measures for 497 children with LI receiving business-as-usual therapy in the public schools. Results: Considerable variability was found in expected change across both samples of children over time, with preschoolers exhibiting larger effect sizes (d = 0.82 and 0.70, respectively) compared with school-age children (d = 0.49 and 0.55, respectively). Conclusions: This study provides a first step toward establishing empirically based language benchmarks for children. These data offer stakeholders an initial tool for setting goals based on expected growth (practitioners), making informed decisions on language-based curricula (policymakers), and measuring effectiveness of intervention research (researchers).},
  langid = {english},
  pmid = {28124066},
  keywords = {Child,Child Language,Child; Preschool,Cohort Studies,Humans,Language Development Disorders,Language Tests,Language Therapy,Linguistics,Preschool,Reference Values}
}

@techreport{schonbrodt2016,
  title = {P-Hacker: {{Train}} Your p-Hacking Skills!},
  author = {Sch{\"o}nbrodt, F D},
  year = {2016}
}

@article{schulz2007,
  title = {Blinding Is Better than Masking},
  author = {Schulz, Kenneth F and Altman, Douglas G and Moher, David},
  year = {2007},
  month = may,
  journal = {BMJ : British Medical Journal},
  volume = {334},
  number = {7600},
  pages = {918},
  issn = {0959-8138},
  doi = {10.1136/bmj.39199.461644.3A},
  pmcid = {PMC1865441},
  pmid = {null},
  file = {/Users/dorothybishop/Zotero/storage/WFDYRC8N/Schulz et al. - 2007 - Blinding is better than masking.pdf}
}

@article{scott2002,
  title = {The Method of Minimization for Allocation to Clinical Trials. a Review},
  author = {Scott, Neil W. and McPherson, Gladys C. and Ramsay, Craig R. and Campbell, Marion K.},
  year = {2002},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {23},
  number = {6},
  pages = {662--674},
  issn = {0197-2456},
  doi = {10.1016/s0197-2456(02)00242-8},
  abstract = {Minimization is a largely nonrandom method of treatment allocation for clinical trials. We conducted a systematic literature search to determine its advantages and disadvantages compared with other allocation methods. Minimization was originally proposed by Taves and by Pocock and Simon. The latter paper introduces a family of allocation methods of which Taves' method is the simplest example. Minimization aims to ensure treatment arms are balanced with respect to predefined patient factors as well as for the number of patients in each group. Further extensions of the method have also been proposed by other authors. Simulation studies show that minimization provides better balanced treatment groups when compared with restricted or unrestricted randomization and that it can incorporate more prognostic factors than stratified randomization methods such as permuted blocks within strata. Some more computationally complex methods may give an even better performance. Concerns over the use of minimization have centered on the fact that treatment assignments may be predicted with certainty in some situations and on the implications for the analysis methods used. It has been suggested that adjustment should always be made for minimization factors when analyzing trials where minimization is the allocation method used. The use of minimization may sometimes result in added organizational complexity compared with other methods. Minimization has been recommended by many commentators for use in clinical trials. Despite this it is still rarely used in practice. From the evidence presented in this review, we believe minimization to be a highly effective allocation method and recommend its wider adoption in the conduct of randomized controlled trials.},
  langid = {english},
  pmid = {12505244},
  keywords = {Clinical Trials as Topic,Humans,Random Allocation,Research Design}
}

@article{scruggs1998,
  title = {Summarizing Single-Subject Research. {{Issues}} and Applications},
  author = {Scruggs, T. E. and Mastropieri, M. A.},
  year = {1998},
  month = jul,
  journal = {Behavior Modification},
  volume = {22},
  number = {3},
  pages = {221--242},
  issn = {0145-4455},
  doi = {10.1177/01454455980223001},
  abstract = {In this article, literature concerning the quantitative synthesis (meta-analysis) of single subject research literature is reviewed. First, the general rationale for such an approach is discussed. Next, procedures for synthesizing single-subject literature are described, followed by comments and critiques of those procedures. Finally, a review is presented of the results of applications of those procedures. The authors suggest that procedures based on percentage of nonoverlapping data (PND) between baseline and treatment are justifiable, meaningful, and--across nine applications--have produced results that are highly meaningful and faithful to the original research reports.},
  langid = {english},
  pmid = {9722473},
  keywords = {Behavior Therapy,Health Care,Humans,Meta-Analysis as Topic,Outcome Assessment,Outcome Assessment; Health Care,Patient Selection,Research Design}
}

@article{senn2004,
  title = {Controversies Concerning Randomization and Additivity in Clinical Trials},
  author = {Senn, Stephen},
  year = {2004},
  journal = {Statistics in Medicine},
  volume = {23},
  number = {24},
  pages = {3729--3753},
  issn = {1097-0258},
  doi = {10.1002/sim.2074},
  abstract = {`As ye randomise so shall ye analyse', is one way of describing Fisher's defence of randomization. Yet, when it comes to clinical trials we nearly always randomize but we rarely analyse the way we randomize and Fisher himself was no exception. Two controversies involving Fisher in the 1930s are discussed: one with Neyman concerning additivity and the other with Student concerning randomization. Their relevance today is considered, as is whether randomization inference in clinical trials is dead and whether modelling rules the day, whether minimization is an acceptable procedure and to what extent trialists confuse experiments with surveys. It will be maintained that a number of different possible purposes of clinical trials have been confused because in the case of the general linear model, under strong additivity, they can all be satisfied by a single analysis. More generally, however, this is not the case. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {additivity,clinical trails,history of statistics,randomisation,unit-treatment interaction},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2074},
  file = {/Users/dorothybishop/Zotero/storage/27L2V8FP/Senn - 2004 - Controversies concerning randomization and additiv.pdf;/Users/dorothybishop/Zotero/storage/FU7GI8UA/sim.html}
}

@book{senn2004a,
  title = {Statistical {{Issues}} in {{Drug Development}}},
  author = {Senn, S},
  year = {2004},
  publisher = {{John Wiley and Sons}},
  address = {{West Sussex}}
}

@article{senn2018,
  title = {Statistical Pitfalls of Personalized Medicine},
  author = {Senn, Stephen},
  year = {2018},
  month = nov,
  journal = {Nature},
  volume = {563},
  number = {7733},
  pages = {619--621},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-018-07535-2},
  abstract = {Misleading terminology and arbitrary divisions stymie drug trials and can give false hope about the potential of tailoring drugs to individuals, warns Stephen Senn.},
  copyright = {2021 Nature},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Medical research, Personalized medicine},
  file = {/Users/dorothybishop/Zotero/storage/GVVUPHS5/Senn - 2018 - Statistical pitfalls of personalized medicine.pdf;/Users/dorothybishop/Zotero/storage/8ECCZ2DK/d41586-018-07535-2.html}
}

@article{shadish2008,
  title = {The State of the Science in the Meta-Analysis of Single-Case Experimental Designs},
  author = {Shadish, William R. and Rindskopf, David M. and Hedges, Larry V.},
  year = {2008},
  month = sep,
  journal = {Evidence-Based Communication Assessment and Intervention},
  volume = {2},
  number = {3},
  pages = {188--196},
  publisher = {{Taylor \& Francis}},
  issn = {1748-9539},
  doi = {10.1080/17489530802581603},
  abstract = {The articles in the previous, special issue of Evidence-Based Communication Assessment and Intervention provided an excellent review of the meta-analysis of single-case designs. This article weaves commentary about those articles into a larger narrative about two major lines of attack on this problem: the use of parametric approaches like regression and multilevel modeling, and the development of parametric and nonparametric effect-size estimators. On each of these two topics, we describe an agenda of research topics that need to be addressed; and we also introduce a new effect-size estimator that may prove to be comparable to the usual standardized mean difference statistics (d) widely used in between-groups analysis. The article ends with observations about ways in which developments in the meta-analysis of single-case designs may have far wider implications than previously appreciated. Source of funding: U.S. Department of Education, Institute of Education Science, Grant \# H324U050001-06},
  keywords = {effect size,Meta-analysis,multilevel models,single-case design,standardized mean difference statistic},
  annotation = {\_eprint: https://doi.org/10.1080/17489530802581603},
  file = {/Users/dorothybishop/Zotero/storage/46HCP3LW/Shadish et al. - 2008 - The state of the science in the meta-analysis of s.pdf}
}

@article{sibbald1998,
  title = {Understanding Controlled Trials {{Crossover}} Trials},
  author = {Sibbald, Bonnie and Roberts, Chris},
  year = {1998},
  month = jun,
  journal = {BMJ : British Medical Journal},
  volume = {316},
  number = {7146},
  pages = {1719--1720},
  issn = {0959-8138},
  pmcid = {PMC1113275},
  pmid = {9614025},
  file = {/Users/dorothybishop/Zotero/storage/9P5C9G3Z/Sibbald and Roberts - 1998 - Understanding controlled trials Crossover trials.pdf}
}

@article{simmons2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {disclosure,methodology,motivated reasoning,publication},
  file = {/Users/dorothybishop/Zotero/storage/WAHA29FP/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{simonsohn2014,
  title = {P-{{Curve}} and {{Effect Size}}: {{Correcting}} for {{Publication Bias Using Only Significant Results}}},
  shorttitle = {P-{{Curve}} and {{Effect Size}}},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {666--681},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691614553988},
  abstract = {Journals tend to publish only statistically significant evidence, creating a scientific record that markedly overstates the size of effects. We provide a new tool that corrects for this bias without requiring access to nonsignificant results. It capitalizes on the fact that the distribution of significant p values, p-curve, is a function of the true underlying effect. Researchers armed only with sample sizes and test results of the published findings can correct for publication bias. We validate the technique with simulations and by reanalyzing data from the Many-Labs Replication project. We demonstrate that p-curve can arrive at conclusions opposite that of existing tools by reanalyzing the meta-analysis of the ``choice overload'' literature.},
  langid = {english},
  keywords = {p-curve,p-hacking,publication bias},
  file = {/Users/dorothybishop/Zotero/storage/F7EZW93E/Simonsohn et al. - 2014 - p-Curve and Effect Size Correcting for Publicatio.pdf}
}

@article{simonsohn2014a,
  title = {P-Curve: A Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  month = apr,
  journal = {Journal of Experimental Psychology. General},
  volume = {143},
  number = {2},
  pages = {534--547},
  issn = {1939-2222},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that "work," readers must ask, "Are these effects true, or do they merely reflect selective reporting?" We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps {$<$} .05). Because only true effects are expected to generate right-skewed p-curves-containing more low (.01s) than high (.04s) significant p values--only right-skewed p--curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses.},
  langid = {english},
  pmid = {23855496},
  keywords = {Data Interpretation,Data Interpretation; Statistical,Experimental,Humans,Models,Models; Statistical,Psychology,Psychology; Experimental,Publication Bias,Reproducibility of Results,Statistical,Statistics as Topic}
}

@article{singal2014,
  title = {A {{Primer}} on {{Effectiveness}} and {{Efficacy Trials}}},
  author = {Singal, Amit G and Higgins, Peter D R and Waljee, Akbar K},
  year = {2014},
  month = jan,
  journal = {Clinical and Translational Gastroenterology},
  volume = {5},
  number = {1},
  pages = {e45},
  issn = {2155-384X},
  doi = {10.1038/ctg.2013.13},
  abstract = {Although efficacy and effectiveness studies are both important when evaluating interventions, they serve distinct purposes and have different study designs. Unfortunately, the distinction between these two types of trials is often poorly understood. In this primer, we highlight several differences between these two types of trials including study design, patient populations, intervention design, data analysis, and result reporting.},
  pmcid = {PMC3912314},
  pmid = {24384867},
  file = {/Users/dorothybishop/Zotero/storage/AS2YSSWM/Singal et al. - 2014 - A Primer on Effectiveness and Efficacy Trials.pdf}
}

@article{snowling2022,
  title = {Delivering Language Intervention at Scale: Promises and Pitfalls},
  shorttitle = {Delivering Language Intervention at Scale},
  author = {Snowling, Margaret J. and West, Gillian and Fricke, Silke and {Bowyer-Crane}, Claudine and Dilnot, Julia and Cripps, Denise and Nash, Marysia and Hulme, Charles},
  year = {2022},
  journal = {Journal of Research in Reading},
  volume = {45},
  number = {3},
  pages = {342--366},
  issn = {1467-9817},
  doi = {10.1111/1467-9817.12391},
  abstract = {Background There is now substantial evidence that language interventions delivered to small groups can be effective for improving language skills and hence strengthening the foundation for formal schooling. However, there are remaining challenges when delivering such interventions in naturalistic environments at scale. Method We reflect on three randomised trials designed to evaluate the impact of an early years language programme, prior to the implementation of a large effectiveness trial, delivered in partnership with speech and language professionals. We consider findings within a framework from implementation science. Results We found that, in contrast to policy-led interventions for reading and mathematics, language interventions are not prioritised in mainstream settings. Aside from this, other obstacles to delivery were the time taken to prepare and to timetable sessions, lack of communication about the requirements of delivery and the need for language screening. Crucial to success was the support from the class teacher of teaching assistants delivering the intervention. However, feedback was largely positive from most stakeholders, and the intervention was found to have a positive impact on children's language with preliminary evidence for effects on behaviour and on reading comprehension. Conclusions While many educators recognise the importance of language for communication, the benefits of oral language interventions are only recently becoming prioritised by policy-makers. We propose that challenges to successful delivery and adoption of evidence-based language interventions in mainstream settings can be remedied through better communication with stakeholders and collaboration between researchers and professional colleagues including senior leaders, teachers, teaching assistants, speech and language therapists and psychologists. It is imperative to take account of issues of implementation when designing an intervention and to do this successfully is a multidisciplinary enterprise.},
  langid = {english},
  keywords = {Consolidated Framework for Implementation Research (CFIR),implementation,language intervention,professional partnerships,randomised controlled trials},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9817.12391},
  file = {/Users/dorothybishop/Zotero/storage/XMJNW2BI/Snowling et al. - 2022 - Delivering language intervention at scale promise.pdf}
}

@article{steegen2016,
  title = {Increasing {{Transparency Through}} a {{Multiverse Analysis}}},
  author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
  year = {2016},
  month = sep,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {11},
  number = {5},
  pages = {702--712},
  issn = {1745-6924},
  doi = {10.1177/1745691616658637},
  abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
  langid = {english},
  pmid = {27694465},
  keywords = {arbitrary choices,Attitude,Choice Behavior,Data Interpretation,Data Interpretation; Statistical,data processing,Female,Fertility,good research practices,Humans,Information Management,Marital Status,multiverse analysis,Politics,Religion,Research Design,selective reporting,Statistical,transparency},
  file = {/Users/dorothybishop/Zotero/storage/T5ZF74B6/Steegen et al. - 2016 - Increasing Transparency Through a Multiverse Analy.pdf}
}

@article{steiner2012,
  title = {Issues and {{Theoretical Constructs Regarding Parent Education}} for {{Autism Spectrum Disorders}}},
  author = {Steiner, Amanda M. and Koegel, Lynn K. and Koegel, Robert L. and Ence, Whitney A.},
  year = {2012},
  month = jun,
  journal = {Journal of autism and developmental disorders},
  volume = {42},
  number = {6},
  pages = {10.1007/s10803-011-1194-0},
  issn = {0162-3257},
  doi = {10.1007/s10803-011-1194-0},
  abstract = {Participation of parents of children with autism is commonplace in most comprehensive intervention programs, yet, there is limited research relating to the best practices in this area. This article provides an overview of parent education programs for young children with autism and details data-driven procedures which are associated with improved parent and child outcomes. In addition, we provide a troubleshooting guide based on the literature for professionals regarding a variety of complex issues which may arise during parent education.},
  pmcid = {PMC3810158},
  pmid = {21336525},
  file = {/Users/dorothybishop/Zotero/storage/2DTFM2EX/Steiner et al. - 2012 - Issues and Theoretical Constructs Regarding Parent.pdf}
}

@article{strong2011,
  title = {A Systematic Meta-Analytic Review of Evidence for the Effectiveness of the `{{Fast ForWord}}' Language Intervention Program},
  author = {Strong, Gemma K and Torgerson, Carole J and Torgerson, David and Hulme, Charles},
  year = {2011},
  month = mar,
  journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},
  volume = {52},
  number = {3},
  pages = {224--235},
  issn = {0021-9630},
  doi = {10.1111/j.1469-7610.2010.02329.x},
  abstract = {Background Fast ForWord is a suite of computer-based language intervention programs designed to improve children's reading and oral language skills. The programs are based on the hypothesis that oral language difficulties often arise from a rapid auditory temporal processing deficit that compromises the development of phonological representations. Methods A systematic review was designed, undertaken and reported using items from the PRISMA statement. A literature search was conducted using the terms `Fast ForWord' `Fast For Word' `Fastforword' with no restriction on dates of publication. Following screening of (a) titles and abstracts and (b) full papers, using pre-established inclusion and exclusion criteria, six papers were identified as meeting the criteria for inclusion (randomised controlled trial (RCT) or matched group comparison studies with baseline equivalence published in refereed journals). Data extraction and analyses were carried out on reading and language outcome measures comparing the Fast ForWord intervention groups to both active and untreated control groups. Results Meta-analyses indicated that there was no significant effect of Fast ForWord on any outcome measure in comparison to active or untreated control groups. Conclusions There is no evidence from the analysis carried out that Fast ForWord is effective as a treatment for children's oral language or reading difficulties.},
  pmcid = {PMC3061204},
  pmid = {20950285},
  file = {/Users/dorothybishop/Zotero/storage/AH6SDE3D/Strong et al. - 2011 - A systematic meta-analytic review of evidence for .pdf}
}

@article{swain2020,
  title = {Speech and Language Therapy for Adolescents in Youth Justice: {{A}} Series of Empirical Single-Case Studies},
  shorttitle = {Speech and Language Therapy for Adolescents in Youth Justice},
  author = {Swain, Nathaniel R. and Eadie, Patricia A. and Snow, Pamela C.},
  year = {2020},
  month = jul,
  journal = {International Journal of Language \& Communication Disorders},
  volume = {55},
  number = {4},
  pages = {458--479},
  issn = {1460-6984},
  doi = {10.1111/1460-6984.12529},
  abstract = {BACKGROUND: Adolescents in contact with youth justice are a vulnerable and marginalized group at high risk of developmental language disorder (DLD) and other communication difficulties. Though preliminary studies have demonstrated the benefits of speech and language therapy (SLT) services in youth justice, limited research has empirically tested the efficacy of intervention in these settings. AIMS: To evaluate the extent to which intensive, one-to-one language intervention improved the communication skills of incarcerated adolescents with below-average ({$>$} 1 SD below the mean) language and/or literacy skills. METHODS \& PROCEDURES: A series of four empirical single case studies was conducted, using multiple baseline intervention design. Individualized intervention programmes were administered, and progress on outcome measures (probes) was evaluated throughout the baseline, intervention and maintenance phases using Tau-U, a non-parametric distribution-free statistic. Additional measures were used as secondary outcomes of the intervention, including standardized language subtests, subjective rating tools by participants and their teachers collected pre- and post-intervention, and a brief structured participant interview, independently administered by youth justice staff. OUTCOMES \& RESULTS: Medium-to-large effect sizes, the majority of which were statistically significant, were detected on the primary outcome measure across the four cases, indicating improvements in the targeted communication skills. Positive results were also evident in comparisons of pre- and post-measures on standardized language subtests, subjective self- and teacher ratings of communication, and the participants' impressions of the interventions. For those participants who could be followed up, gains in language skills were generally maintained at 1 month post-intervention. CONCLUSIONS \& IMPLICATIONS: This study provides further evidence of the efficacy of one-to-one SLT intervention for adolescents in youth justice in order to address language and literacy difficulties. These findings inform future SLT service provision for adolescents in these settings, with clear policy and practice implications. Future research should investigate the wider benefits to individuals' engagement in youth justice intervention and recidivism, as well as assessing maintenance of gains over a longer period. What this paper adds What is already known on this subject The high rates of DLD in youth justice is well known, with difficulties spanning multiple areas of language and literacy. SLTs are increasingly working in community and custodial youth justice settings, and a few preliminary studies have demonstrated the effectiveness of such work. What this paper adds to existing knowledge This study extends the evidence base of the efficacy of SLT for language and literacy difficulties in youth justice, using a series of four empirical single case studies. It is also argued that SLT should be more actively considered in planning multidisciplinary interventions for young people in custody. What are the potential or actual clinical implications of this work? The results of this research support current moves to include SLT services in youth justice systems, and illustrate for clinicians currently working in this sector a way of structuring and measuring the impact of intervention services.},
  langid = {english},
  pmid = {32196891},
  keywords = {adolescents,developmental language disorder,intervention,language,literacy,service delivery,speech and language therapy},
  file = {/Users/dorothybishop/Zotero/storage/HR5VHWEX/Swain et al. - 2020 - Speech and language therapy for adolescents in you.pdf}
}

@article{tate2016,
  title = {The {{Single-Case Reporting Guideline In Behavioural Interventions}} ({{SCRIBE}}) 2016 {{Statement}}},
  author = {Tate, Robyn L. and Perdices, Michael and Rosenkoetter, Ulrike and Shadish, William and Vohra, Sunita and Barlow, David H. and Horner, Robert and Kazdin, Alan and Kratochwill, Thomas and McDonald, Skye and Sampson, Margaret and Shamseer, Larissa and Togher, Leanne and Albin, Richard and Backman, Catherine and Douglas, Jacinta and Evans, Jonathan J. and Gast, David and Manolov, Rumen and Mitchell, Geoffrey and Nickels, Lyndsey and Nikles, Jane and Ownsworth, Tamara and Rose, Miranda and Schmid, Christopher H. and Wilson, Barbara},
  year = {2016},
  month = jul,
  journal = {Physical Therapy},
  volume = {96},
  number = {7},
  pages = {e1-e10},
  issn = {1538-6724},
  doi = {10.2522/ptj.2016.96.7.e1},
  abstract = {We developed a reporting guideline to provide authors with guidance about what should be reported when writing a paper for publication in a scientific journal using a particular type of research design: the single-case experimental design. This report describes the methods used to develop the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016. As a result of 2 online surveys and a 2-day meeting of experts, the SCRIBE 2016 checklist was developed, which is a set of 26 items that authors need to address when writing about single-case research. This article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated. We recommend that the SCRIBE 2016 is used by authors preparing manuscripts describing single-case research for publication, as well as journal reviewers and editors who are evaluating such manuscripts. SCIENTIFIC ABSTRACT: Reporting guidelines, such as the Consolidated Standards of Reporting Trials (CONSORT) Statement, improve the reporting of research in the medical literature (Turner et al., 2012). Many such guidelines exist and the CONSORT Extension to Nonpharmacological Trials (Boutron et al., 2008) provides suitable guidance for reporting between-groups intervention studies in the behavioral sciences. The CONSORT Extension for N-of-1 Trials (CENT 2015) was developed for multiple crossover trials with single individuals in the medical sciences (Shamseer et al., 2015; Vohra et al., 2015), but there is no reporting guideline in the CONSORT tradition for single-case research used in the behavioral sciences. We developed the Single-Case Reporting guideline In BEhavioural interventions (SCRIBE) 2016 to meet this need. This Statement article describes the methodology of the development of the SCRIBE 2016, along with the outcome of 2 Delphi surveys and a consensus meeting of experts. We present the resulting 26-item SCRIBE 2016 checklist. The article complements the more detailed SCRIBE 2016 Explanation and Elaboration article (Tate et al., 2016) that provides a rationale for each of the items and examples of adequate reporting from the literature. Both these resources will assist authors to prepare reports of single-case research with clarity, completeness, accuracy, and transparency. They will also provide journal reviewers and editors with a practical checklist against which such reports may be critically evaluated.},
  langid = {english},
  pmid = {27371692},
  keywords = {Behavior Therapy,Checklist,Delphi Technique,Guidelines as Topic,Humans,methodology,Peer Review,Peer Review; Research,publication standardsSupplemental materials: http://dx.doi.org/10.1037/arc0000026.supp.,reporting guidelines,Research,Research Design,Research Report,single-case design},
  file = {/Users/dorothybishop/Zotero/storage/4ZSVV4LM/Tate et al. - 2016 - The Single-Case Reporting Guideline In BEhavioural.pdf;/Users/dorothybishop/Zotero/storage/LNIKGYTV/Tate et al. - 2016 - The Single-Case Reporting Guideline In BEhavioural.pdf}
}

@article{tate2016a,
  title = {The {{Single-Case Reporting Guideline In BEhavioural Interventions}} ({{SCRIBE}}) 2016: {{Explanation}} and Elaboration.},
  shorttitle = {The {{Single-Case Reporting Guideline In BEhavioural Interventions}} ({{SCRIBE}}) 2016},
  author = {Tate, Robyn L. and Perdices, Michael and Rosenkoetter, Ulrike and McDonald, Skye and Togher, Leanne and Shadish, William and Horner, Robert and Kratochwill, Thomas and Barlow, David H. and Kazdin, Alan and Sampson, Margaret and Shamseer, Larissa and Vohra, Sunita},
  year = {2016},
  journal = {Archives of Scientific Psychology},
  volume = {4},
  number = {1},
  pages = {10},
  publisher = {{US: American Psychological Association}},
  issn = {2169-3269},
  doi = {10.1037/arc0000027},
  file = {/Users/dorothybishop/Zotero/storage/ZGK539TK/Tate et al. - The Single-Case Reporting Guideline In BEhavioural.pdf;/Users/dorothybishop/Zotero/storage/LNLWA6K9/2016-17384-001.html}
}

@article{thorlund2018,
  title = {Key Design Considerations for Adaptive Clinical Trials: A Primer for Clinicians},
  shorttitle = {Key Design Considerations for Adaptive Clinical Trials},
  author = {Thorlund, Kristian and Haggstrom, Jonas and Park, Jay JH and Mills, Edward J.},
  year = {2018},
  month = mar,
  journal = {BMJ},
  volume = {360},
  pages = {k698},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.k698},
  abstract = {{$<$}p{$>$}This article reviews important considerations for researchers who are designing adaptive clinical trials. These differ from conventional clinical trials because they allow and even enforce continual modifications to key components of trial design while data are being collected. This innovative approach has the potential to reduce resource use, decrease time to trial completion, limit allocation of participants to inferior interventions, and improve the likelihood that trial results will be scientifically or clinically relevant. Adaptive designs have mostly been used in trials evaluating drugs, but their use is spreading. The US Food and Drug Administration recently issued guidance on adaptive trial designs, which highlighted general principles and different types of adaptive clinical trials but did not provide concrete guidance about important considerations in designing such trials. Decisions to adapt a trial are not arbitrary; they are based on decision rules that have been rigorously examined via statistical simulations before the first trial participant is enrolled. The authors review important characteristics of adaptive trials and common types of study modifications and provide a practical guide, illustrated with a case study, to aid investigators who are planning an adaptive clinical trial{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  pmid = {29519932},
  file = {/Users/dorothybishop/Zotero/storage/NQ8YUESV/Thorlund et al. - 2018 - Key design considerations for adaptive clinical tr.pdf;/Users/dorothybishop/Zotero/storage/ZWTMQRBK/bmj.html}
}

@book{torgesen1999,
  title = {Test of {{Word Reading Efficiency}}},
  author = {Torgesen, Joseph K and Wagner, Richard K and Rashotte, Carole A.},
  year = {1999},
  publisher = {{Pro-Ed}},
  address = {{Austin, TX}},
  file = {/Users/dorothybishop/Zotero/storage/9BIVV5PR/TestofWordReadingEfficiencySecondEdition.html}
}

@article{treweek2009,
  title = {Making Trials Matter: Pragmatic and Explanatory Trials and the Problem of Applicability},
  shorttitle = {Making Trials Matter},
  author = {Treweek, Shaun and Zwarenstein, Merrick},
  year = {2009},
  month = jun,
  journal = {Trials},
  volume = {10},
  number = {1},
  pages = {37},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-10-37},
  abstract = {Randomised controlled trials are the best research design for decisions about the effect of different interventions but randomisation does not, of itself, promote the applicability of a trial's results to situations other than the precise one in which the trial was done. While methodologists and trialists have rightly paid great attention to internal validity, much less has been given to applicability.},
  keywords = {Chronic Obstructive Pulmonary Disease,Directly Observe Treatment,Explanatory Trial,Pragmatic Trial,Rofecoxib},
  file = {/Users/dorothybishop/Zotero/storage/TVJQI7CF/Treweek and Zwarenstein - 2009 - Making trials matter pragmatic and explanatory tr.pdf;/Users/dorothybishop/Zotero/storage/TH787I75/1745-6215-10-37.html}
}

@article{varley2016,
  title = {Self-{{Administered Computer Therapy}} for {{Apraxia}} of {{Speech}}},
  author = {Varley, R and Cowell, P and Dyson, L and Inglis, L and Roper, A and Whiteside, S. P.},
  year = {2016},
  journal = {Stroke},
  volume = {47},
  number = {3},
  pages = {822--828},
  doi = {10.1161/STROKEAHA.115.011939},
  file = {/Users/dorothybishop/Zotero/storage/U6UDUNLJ/Self-Administered Computer Therapy for Apraxia of .pdf;/Users/dorothybishop/Zotero/storage/S9VFEEAG/STROKEAHA.115.html}
}

@article{vorland2021,
  title = {Errors in the Implementation, Analysis, and Reporting of Randomization within Obesity and Nutrition Research: A Guide to Their Avoidance},
  shorttitle = {Errors in the Implementation, Analysis, and Reporting of Randomization within Obesity and Nutrition Research},
  author = {Vorland, Colby J. and Brown, Andrew W. and Dawson, John A. and Dickinson, Stephanie L. and {Golzarri-Arroyo}, Lilian and Hannon, Bridget A. and Heo, Moonseong and Heymsfield, Steven B. and Jayawardene, Wasantha P. and Kahathuduwa, Chanaka N. and Keith, Scott W. and Oakes, J. Michael and Tekwe, Carmen D. and Thabane, Lehana and Allison, David B.},
  year = {2021},
  month = jul,
  journal = {International Journal of Obesity},
  pages = {1--12},
  publisher = {{Nature Publishing Group}},
  issn = {1476-5497},
  doi = {10.1038/s41366-021-00909-z},
  abstract = {Randomization is an important tool used to establish causal inferences in studies designed to further our understanding of questions related to obesity and nutrition. To take advantage of the inferences afforded by randomization, scientific standards must be upheld during the planning, execution, analysis, and reporting of such studies. We discuss ten errors in randomized experiments from real-world examples from the literature and outline best practices for their avoidance. These ten errors include: representing nonrandom allocation as random, failing to adequately conceal allocation, not accounting for changing allocation ratios, replacing subjects in nonrandom ways, failing to account for non-independence, drawing inferences by comparing statistical significance from within-group comparisons instead of between-groups, pooling data and breaking the randomized design, failing to account for missing data, failing to report sufficient information to understand study methods, and failing to frame the causal question as testing the randomized assignment per se. We hope that these examples will aid researchers, reviewers, journal editors, and other readers to endeavor to a high standard of scientific rigor in randomized experiments within obesity and nutrition research.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Diseases;Nutrition disorders Subject\_term\_id: diseases;nutrition-disorders},
  file = {/Users/dorothybishop/Zotero/storage/XMT44WSJ/Vorland et al. - 2021 - Errors in the implementation, analysis, and report.pdf;/Users/dorothybishop/Zotero/storage/TDK3M92G/s41366-021-00909-z.html}
}

@article{voudris2018,
  title = {Home {{Hospitalization}} for {{Acute Decompensated Heart Failure}}: {{Opportunities}} and {{Strategies}} for {{Improved Health Outcomes}}},
  shorttitle = {Home {{Hospitalization}} for {{Acute Decompensated Heart Failure}}},
  author = {Voudris, Konstantinos V. and Silver, Marc A.},
  year = {2018},
  month = jun,
  journal = {Healthcare},
  volume = {6},
  number = {2},
  pages = {31},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2227-9032},
  doi = {10.3390/healthcare6020031},
  abstract = {Importance: Heart failure (HF) is the leading cause of hospitalization among patients over the age of 65 in the United States and developed countries, posing a significant economic burden to the health care systems. More than half of the patients with HF will be readmitted to the hospital within 6 months from discharge, leading not only to increased health care related expenses but also functional decline, iatrogenic injuries and in-hospital infections. With the increasing prevalence of HF, there is a substantial need for innovative delivery care models that can provide hospital level of care at a patient's home. Observations: Home hospitalization was originally used to safely manage chronically ill patients with general medical (stroke, chronic obstructive pulmonary disease, deep vein thrombosis, community acquired pneumonia) and surgical conditions and was associated with improved patient satisfaction and improvement in activity of daily living status. This had no clear effect on readmission or cost. When hospital at home care model was applied to HF patients it demonstrated increased time to readmission, reduced index costs and improved health related quality of life, with no significant differences in adverse events. Eligible patients should be selected based on multiple factors taking into consideration applicable limitations and comorbidities. Conclusions and Relevance: Providing in-hospital level care to the patient's house presents a reliable alternative, yielding multiple benefits both for the patient, as well as the health care system. Formulating a well-defined model is necessary before wide implementation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Acute Decompensated Heart Failure,health outcomes,home hospitalization,hospital at home},
  file = {/Users/dorothybishop/Zotero/storage/UL3T56A7/Voudris and Silver - 2018 - Home Hospitalization for Acute Decompensated Heart.pdf;/Users/dorothybishop/Zotero/storage/F6AIPVQA/31.html}
}

@article{vries2018,
  title = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression},
  shorttitle = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments},
  author = {de Vries, Y. A. and Roest, A. M. and de Jonge, P. and Cuijpers, P. and Munaf{\`o}, M. R. and Bastiaansen, J. A.},
  year = {2018},
  month = nov,
  journal = {Psychological Medicine},
  volume = {48},
  number = {15},
  pages = {2453--2455},
  publisher = {{Cambridge University Press}},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291718001873},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0033291718001873/resource/name/firstPage-S0033291718001873a.jpg},
  langid = {english},
  keywords = {Antidepressants,Antidepressive Agents,bias,citation bias,Clinical Trials as Topic,depression,Depressive Disorder,Humans,psychotherapy,Publication Bias,reporting bias,Statistics as Topic},
  file = {/Users/dorothybishop/Zotero/storage/2B3CYSSY/de Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf;/Users/dorothybishop/Zotero/storage/6FX77EJD/de Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf;/Users/dorothybishop/Zotero/storage/Y2MYQHIY/Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf}
}

@article{weiss1991,
  title = {Stressors Experienced by Family Caregivers of Children with Pervasive Developmental Disorders | {{SpringerLink}}},
  author = {Weiss, S},
  year = {1991},
  journal = {Child Psychiatry and Human Development},
  volume = {21},
  pages = {203--216},
  file = {/Users/dorothybishop/Zotero/storage/7P9BPDTX/BF00705906.html}
}

@misc{westrick,
  title = {{{LibGuides}}: {{Systematic Reviews}}: {{Systematic Review}} or {{Literature Review}}?},
  shorttitle = {{{LibGuides}}},
  author = {Westrick, Jennifer},
  abstract = {LibGuides: Systematic Reviews: Systematic Review or Literature Review?},
  copyright = {Copyright Rush University Medical Center 2021},
  howpublished = {https://rushu.libguides.com/c.php?g=595495\&p=4585490},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/DRJR49EB/c.html}
}

@book{wiig1992,
  title = {Celf - {{Preschool}}},
  author = {Wiig, Elisabeth and Secord, Wayne and Semel, Eleanor},
  year = {1992},
  month = apr,
  publisher = {{Psychological Corporation}},
  isbn = {978-0-15-803380-8},
  langid = {english}
}

@book{wiig2006,
  title = {{{CELF-Preschool}} 2 {{UK}}},
  author = {Wiig, Elisabeth H and Secord, Wayne and Semel, Eleanor},
  year = {2006},
  publisher = {{Pearson Assessment}},
  address = {{London}},
  file = {/Users/dorothybishop/Zotero/storage/5QTE3E8E/CELF-Preschool2UK.html}
}

@article{wilcox2020,
  title = {Preschoolers with Developmental Speech and/or Language Impairment: {{Efficacy}} of the {{Teaching Early Literacy}} and {{Language}} ({{TELL}}) Curriculum},
  shorttitle = {Preschoolers with Developmental Speech and/or Language Impairment},
  author = {Wilcox, M. Jeanne and Gray, Shelley and Reiser, Mark},
  year = {2020},
  month = apr,
  journal = {Early Childhood Research Quarterly},
  volume = {51},
  pages = {124--143},
  issn = {0885-2006},
  doi = {10.1016/j.ecresq.2019.10.005},
  abstract = {Young children with developmental speech and/or language impairment (DSLI) often fail to develop oral language and early literacy skills that are foundational for subsequent schooling and reading success. The purpose of this investigation was to examine the efficacy of the Teaching Early Literacy and Language (TELL) curriculum and associated evidence-based teaching practices. Participants included 91 preschool classroom teachers and 202 male and 87 female preschoolers with DSLI who were enrolled in their classes. Children ranged in age from 43 to 63 months. In this cluster RCT, classroom teachers were randomly assigned to implement the TELL curriculum or to continue with their business-as-usual (BAU) curriculum. Proximal outcomes were assessed with investigator developed curriculum-based measures (CBMs) administered six times over the school year. Distal tests (pre-post) of oral language and early literacy skills included an investigator-developed pre-post expressive and receptive vocabulary test, two additional standardized measures (Clinical Evaluation of Language Fundamentals-Preschool 2nd Edition, the Test of Preschool Early Literacy). A benchmarked early literacy assessment, the Phonological Awareness and Literacy Screening PreK, was also administered. Results indicated a significant TELL effect for all CBMs at later measurement points with Cohen's ds in the medium (0.43) to very large (1.25) range. TELL effects were also noted for the distal vocabulary measure with small to medium between-group effect sizes (Cohen's f\^2 range from 0.02 to 0.44). There were no significant TELL effects for the standardized distal measures. Based on progress measures, the TELL curriculum was effective for improving the oral language and early literacy skills of young children with DSLI.},
  langid = {english},
  keywords = {Early literacy skills,Oral language,Preschool curriculum efficacy,Speech \& language impairment},
  file = {/Users/dorothybishop/Zotero/storage/IZUNRSBP/Wilcox et al. - 2020 - Preschoolers with developmental speech andor lang.pdf;/Users/dorothybishop/Zotero/storage/AND34VZP/S0885200619301292.html}
}

@article{wood2021,
  title = {Is Speech and Language Therapy Effective at Improving the Communication of Adults with Intellectual Disabilities?: {{A}} Systematic Review},
  shorttitle = {Is Speech and Language Therapy Effective at Improving the Communication of Adults with Intellectual Disabilities?},
  author = {Wood, Sia{\fontencoding{LECO}\selectfont\char177}n and Standen, Penny},
  year = {2021},
  journal = {International Journal of Language \& Communication Disorders},
  volume = {56},
  number = {2},
  pages = {435--450},
  issn = {1460-6984},
  doi = {10.1111/1460-6984.12601},
  abstract = {Background A significant proportion of adults with intellectual disabilities (ID) experience speech, language and communication difficulties which are associated with poor physical and mental health outcomes. Speech and language therapy (SLT) interventions are an important way to address these communication difficulties, yet there is limited available evidence to provide information about the effectiveness of the different approaches used for this heterogeneous group. Aims To review the evidence available for the effectiveness of SLT interventions aimed at improving communication for adults with ID. Methods \& Procedures A systematic search across relevant databases was performed. Information on methodological details of each relevant study, along with descriptions of the SLT interventions employed, were extracted and the Crowe Critical Appraisal Tool (CCAT) was used to assess quality. Findings were discussed in a narrative synthesis grouped by target communication skill. Outcomes \& Results A total of 10 relevant studies met the inclusion criteria. These were predominantly interventions aimed directly at adults with ID to improve speech, increase augmentative and alternative communication (AAC) use and develop interaction skills, with one study addressing work with carers. The included studies were all rated as low quality. There is weak preliminary evidence that SLT input can improve the communication skills of adults with ID. Conclusions \& Implications There is insufficient evidence to draw strong conclusions about the effectiveness of SLT in this population. Further high-level evidence across speech, language and communication domains is urgently needed. What this paper adds What is already known on the subject There is limited evidence for community health interventions used with adults with ID. Previous reviews of SLT interventions found a lack of evidence base for this population. Some areas of SLT practice such as AAC have demonstrated potential benefits and other areas including speech work, social communication skills and training for communication partners have some evidence base for children with ID but there is currently insufficient evidence for adults with ID. What this paper adds to existing knowledge The study systematically reviews the current evidence base available when considering the effectiveness of SLT intervention for adults with ID. It provides weak evidence to suggest SLT intervention can improve communication in this population and highlights the need for clinically relevant, robustly designed studies to be undertaken in this field. What are the potential or actual clinical implications of this work? The lack of high-quality studies with sufficient power to draw conclusions about effectiveness means SLTs are not able to base their intervention choices on firm evidence. There is an urgent need to conduct robust research into the effectiveness of SLT interventions for adults with ID.},
  langid = {english},
  keywords = {intellectual disabilities,speech and language therapy,systematic review},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1460-6984.12601},
  file = {/Users/dorothybishop/Zotero/storage/WTGJ46B8/Wood and Standen - 2021 - Is speech and language therapy effective at improv.pdf;/Users/dorothybishop/Zotero/storage/JURA8JYB/1460-6984.html}
}

@article{zhang2003,
  title = {Explaining and Controlling Regression to the Mean in Longitudinal Research Designs},
  author = {Zhang, Xuyang and Tomblin, J. Bruce},
  year = {2003},
  month = dec,
  journal = {Journal of speech, language, and hearing research: JSLHR},
  volume = {46},
  number = {6},
  pages = {1340--1351},
  issn = {1092-4388},
  doi = {10.1044/1092-4388(2003/104)},
  abstract = {This tutorial is concerned with examining how regression to the mean influences research findings in longitudinal studies of clinical populations. In such studies participants are often obtained because of performance that deviates systematically from the population mean and are then subsequently studied with respect to change in the trait used for this selection. It is shown that in such research there is a potential for the estimates of change to be erroneous due to the effect of regression to the mean. The source of the regression effect is shown to arise from measurement error and a sampling bias of this measurement error in the process of selecting on extreme scores. It is also shown that regression effects are greater with measures that are less reliable and with samples that are selected with more extreme scores. Furthermore, it is shown that regression effects are particularly prominent when measures of change are based on changes in dichotomous states formed from quantitative, normally distributed traits. In addition to a formal analysis of the regression to the mean, the features of regression to the mean are demonstrated via a simulation.},
  langid = {english},
  pmid = {14700359},
  keywords = {Humans,Language Disorders,Longitudinal Studies,Normal Distribution,Regression Analysis,Reproducibility of Results,Research Design}
}

@misc{zotero-1164,
  title = {{{https://academic.oup.com/psychsocgerontology/article/75/1/45/5033832}} - {{Google Search}}},
  howpublished = {https://www.google.com/search?q=https\%3A\%2F\%2Facademic.oup.com\%2Fpsychsocgerontology\%2Farticle\%2F75\%2F1\%2F45\%2F5033832\&client=firefox-b-d\&ei=z7cDYcvcMcrS1fAP7N29qAw\&oq=https\%3A\%2F\%2Facademic.oup.com\%2Fpsychsocgerontology\%2Farticle\%2F75\%2F1\%2F45\%2F5033832\&gs\_lcp=Cgdnd3Mtd2l6EAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsAMyBwgAEEcQsANKBAhBGABQmfYRWJn2EWD4gBJoAXACeACAATSIATSSAQExmAEAoAECoAEByAEIwAEB\&sclient=gws-wiz\&ved=0ahUKEwiLgPvbr4ryAhVKaRUIHexuD8UQ4dUDCA4\&uact=5},
  file = {/Users/dorothybishop/Zotero/storage/GFDT9EN3/search.html}
}

@misc{zotero-1230,
  title = {The History and Development of {{N}} of 1 Trials.},
  journal = {The James Lind Library},
  abstract = {Introduction `Trials of therapy', in which physicians `try out' treatments and assess patients' responses, are long-established, common elements of routine medical practice. Because `trials of therapy' are usually informal, they ...},
  chapter = {N-of-1 crossover},
  howpublished = {https://www.jameslindlibrary.org/articles/history-development-n-1-trials/},
  langid = {british},
  file = {/Users/dorothybishop/Zotero/storage/HJHDUY9H/history-development-n-1-trials.html}
}

@misc{zotero-1331,
  title = {Telerehabilitation for People with Aphasia: {{A}} Systematic Review and Meta-Analysis | {{Elsevier Enhanced Reader}}},
  shorttitle = {Telerehabilitation for People with Aphasia},
  doi = {10.1016/j.jcomdis.2021.106111},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0021992421000344?token=3099C6FCDCBACB8B331BF416E1EB56E1B38D0ABDE2EB9CFC553672622DFC385DE73824840E2A805A4BF2B6E4BD4A00CA\&originRegion=eu-west-1\&originCreation=20210806123944},
  langid = {english},
  file = {/Users/dorothybishop/Zotero/storage/A7B4MCAL/S0021992421000344.html}
}

@misc{zotero-1598,
  title = {The {{Single-Case Reporting Guideline In BEhavioural Interventions}} ({{SCRIBE}}) 2016: {{Explanation}} and Elaboration.},
  howpublished = {https://psycnet.apa.org/fulltext/2016-17384-001.html},
  file = {/Users/dorothybishop/Zotero/storage/8EF92LND/2016-17384-001.html}
}

@misc{zotero-986,
  title = {Clinical {{Evaluation}} of {{Language Fundamentals}} - {{Fifth Edition}} ({{CELF-5 UK}}) | {{Pearson Assessment}}},
  howpublished = {https://www.pearsonclinical.co.uk/Psychology/ChildCognitionNeuropsychologyandLanguage/ChildLanguage/celf-5/clinical-evaluation-of-language-fundamentals-fifth-edition-celf-5.aspx?gclid=CjwKCAjwoZWHBhBgEiwAiMN66ZCgI2D45ubBirWCEKjtU6y87WyBi\_4zgheGsgkBAQYfUkduPLMUdBoC2W8QAvD\_BwE},
  file = {/Users/dorothybishop/Zotero/storage/JE369QRV/clinical-evaluation-of-language-fundamentals-fifth-edition-celf-5.html}
}
